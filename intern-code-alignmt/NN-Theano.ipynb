{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import numbers\n",
    "\n",
    "import theano.tensor as T\n",
    "from theano import function, printing\n",
    "import theano\n",
    "\n",
    "from theano import config\n",
    "# config.device = 'cpu'\n",
    "# config.gcc.cxxflags = \"-D_hypot=hypot\"\n",
    "config.compute_test_value = 'off'\n",
    "#import os\n",
    "#os.environ[\"THEANO_FLAGS\"] = \"exception_verbosity=high,on_opt_error=optimizer_excluding=ShapeOpt:local_lift_transpose_through_dot:scan_opt\"\n",
    "from theano.compile.nanguardmode import NanGuardMode\n",
    "# config.NanGuardMode.action == 'pdb'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_AER(S, P, A):\n",
    "    S, P, A = np.array(S), np.array(P), np.array(A)\n",
    "    s_a, p_a, len_s, len_a = 0, 0, 0, 0\n",
    "    for s, p, a in zip(S, P, A):\n",
    "        s_a += len(list(set(s).intersection(a)))\n",
    "        p_a += len(list(set(p).intersection(a)))\n",
    "        len_s += len(s[s != \"\"])\n",
    "        len_a += len(a[a != \"\"])\n",
    "    print (\"s_a\", s_a)\n",
    "    p_a += s_a\n",
    "    print (\"p_a\", p_a)\n",
    "    aer = (s_a + p_a) / (len_s + len_a)\n",
    "    print (\"aer\", 1.-aer)\n",
    "    \n",
    "    return 1. - aer \n",
    "\n",
    "def calculate_one_AER(S, P, A):\n",
    "    S, P, A = np.array(S), np.array(P), np.array(A)\n",
    "    s_a = len(list(set(S).intersection(A)))\n",
    "    print (\"s_a\", s_a)\n",
    "    p_a = len(list(set(P).intersection(A))) + s_a\n",
    "    print (\"p_a\", p_a)\n",
    "    aer = (s_a + p_a) / (len(S[S != \"\"]) + len(A[A != \"\"]))\n",
    "    print (\"aer\", 1.-aer)\n",
    "    \n",
    "    return 1. - aer \n",
    "    \n",
    "def write_file(strs, file_name):\n",
    "    alignment_test = open(file_name,\"w\", encoding='utf8') \n",
    "    for s in strs:\n",
    "        alignment_test.write(s + \"\\n\") \n",
    "    alignment_test.close()\n",
    "    \n",
    "def load_params(path):\n",
    "    f = open(path, 'r')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "\n",
    "def save_params(obj, path):\n",
    "    f = open(path, 'wb')\n",
    "    pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5)\n",
      "(12, 5)\n"
     ]
    }
   ],
   "source": [
    "class EmissionModel:\n",
    "    \"\"\" Simple emission model without CNN\n",
    "    word embedding layer -> ReLU layer -> softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def save_obj(self, obj, path):\n",
    "        print(\"Saving file ... \" + path)\n",
    "        f = open(path, 'wb')\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        print(\"File Saved !\")\n",
    "        \n",
    "    def load_obj(self, path):\n",
    "        f = open(path, 'rb')\n",
    "        obj = pickle.load(f)\n",
    "        f.close()\n",
    "        return obj\n",
    "    \n",
    "    def save_align(self, alignments, path):\n",
    "        print(\"Saving alignment to ... \" + path)\n",
    "        f = open(path, 'w', encoding='utf8')\n",
    "        for s in alignments:\n",
    "            for ss in s:\n",
    "                f.write(ss + \" \") \n",
    "            f.write(\"\\n\") \n",
    "        f.close()\n",
    "        print(\"File Saved !\")\n",
    "    \n",
    "    def save_params(self, params_path_prefix):\n",
    "        w_saved = []\n",
    "        b_saved = []\n",
    "        for w in self.w:\n",
    "            w_saved.append(w.get_value())\n",
    "        for b in self.b:\n",
    "            b_saved.append(b.get_value())\n",
    "        self.save_obj(w_saved, params_path_prefix + \"_w.pickle\")\n",
    "        self.save_obj(b_saved, params_path_prefix + \"_b.pickle\")\n",
    "        \n",
    "    def load_params(self, params_path_prefix):\n",
    "        f_w = open(params_path_prefix+ \"_w.pickle\", 'rb')\n",
    "        w_saved = pickle.load(f_w)\n",
    "        f_w.close()\n",
    "        f_b = open(params_path_prefix+ \"_b.pickle\", 'rb')\n",
    "        b_saved = pickle.load(f_b)\n",
    "        f_b.close()\n",
    "        # TODO: check w and b sizes\n",
    "        w = []\n",
    "        b = []\n",
    "        for ww in w_saved:\n",
    "            w.append(theano.shared(\n",
    "                    value=np.asarray(ww, dtype=theano.config.floatX), \n",
    "                    borrow=True\n",
    "            ))\n",
    "        for bb in b_saved:\n",
    "            b.append(theano.shared(\n",
    "                    value=np.asarray(bb, dtype=theano.config.floatX), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "            ))\n",
    "        return w, b\n",
    "    \n",
    "    def init_weights_bias(self, vocab_input_size, layer_size, vocab_output_size, seed=1402):\n",
    "        random_state = np.random.RandomState(seed)\n",
    "        \n",
    "        size_list = np.concatenate(([vocab_input_size], layer_size, [vocab_output_size]), axis=0)\n",
    "        w = []\n",
    "        b = []\n",
    "        \n",
    "        for i in range(len(size_list) - 1):\n",
    "            w.append(theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], size_list[i])), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), borrow=True\n",
    "            ))\n",
    "            b.append(theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], 1)), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "            ))\n",
    "        \n",
    "        return w, b\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = T.transpose(x)\n",
    "        e_x = T.exp(x - x.max(axis=1, keepdims=True)) \n",
    "        out = e_x / e_x.sum(axis=1, keepdims=True)\n",
    "        return T.transpose(out)\n",
    "    \n",
    "    #[7,512]\n",
    "    def __init__(self, vocab_input_size, layer_size, vocab_output_size, baum_welch_model, \n",
    "                 epoch=1, batch=1, learning_rate = .01, seed=1412, \n",
    "                 params_path_prefix=None, out_prefix=None, target_AER=None):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch = batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.emission_posteriors = []\n",
    "        self.transition_posteriors = []\n",
    "        self.baum_welch_model = baum_welch_model\n",
    "        \n",
    "        self.vocab_input_size = vocab_input_size\n",
    "        self.d_embedding_size = layer_size[0]\n",
    "        \n",
    "        self.params_path_prefix = params_path_prefix\n",
    "        self.out_prefix = out_prefix\n",
    "        self.target_AER = target_AER\n",
    "        \n",
    "        x_training_input = T.matrix().astype(config.floatX)\n",
    "        \n",
    "        if (self.params_path_prefix == None):\n",
    "            self.w, self.b = self.init_weights_bias(vocab_input_size, layer_size, vocab_output_size, seed)\n",
    "        else:\n",
    "            self.w, self.b = self.load_params(params_path_prefix)\n",
    "            \n",
    "        # Word embedding layer\n",
    "        word_embedding_layer = T.dot(self.w[0], x_training_input) # [7, 10] * [10, 5] = [7, 5]\n",
    "        \n",
    "        # ReLU layer\n",
    "        z_relu_layer = T.dot(self.w[1], word_embedding_layer) + self.b[1] # [512, 7] * [7, 5] = [512, 5]\n",
    "        z_relu_layer_shape = T.shape(z_relu_layer)\n",
    "        relu_layer = T.nnet.relu(T.flatten(z_relu_layer))\n",
    "        relu_layer_reshaped = T.reshape(relu_layer, z_relu_layer_shape) # [512, 5]\n",
    "        \n",
    "        # Softmax layer\n",
    "        z_softmax_layer = T.dot(self.w[2], relu_layer_reshaped) + self.b[2] # [12, 512] * [512, 5] = [12, 5]\n",
    "#         softmax_layer = T.transpose(T.nnet.softmax(T.transpose(z_softmax_layer))) # Output: [12, 5]\n",
    "        softmax_layer = T.nnet.softmax(z_softmax_layer) # Output: [12, 5]\n",
    "        softmax_layer_clipped = T.clip(softmax_layer, 1e-35, 1.0 - 1e-35)\n",
    "        \n",
    "        # Calculate new gradient\n",
    "        posteriors = T.matrix().astype(config.floatX)\n",
    "        \n",
    "        cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer_clipped))\n",
    "#         cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer))\n",
    "        # TODO: use dw[] and db[] abstractly \n",
    "        dw0,dw1,dw2,db1,db2 = T.grad(\n",
    "            cost=cost, wrt=[self.w[0], self.w[1], self.w[2], self.b[1], self.b[2]]\n",
    "        )\n",
    "\n",
    "        # Update w and b\n",
    "        updates = [\n",
    "            (self.w[0], self.w[0] - self.learning_rate * dw0), \n",
    "            (self.w[1], self.w[1] - self.learning_rate * dw1), \n",
    "            (self.b[1], self.b[1] - self.learning_rate * db1),\n",
    "            (self.w[2], self.w[2] - self.learning_rate * dw2), \n",
    "            (self.b[2], self.b[2] - self.learning_rate * db2)\n",
    "        ]\n",
    "        \n",
    "        # Compile model\n",
    "        self.test = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=[word_embedding_layer, softmax_layer]\n",
    "        ) \n",
    "        self.train_mini_batch_function = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=softmax_layer, \n",
    "            updates=updates,\n",
    "            mode=NanGuardMode(nan_is_error=True, inf_is_error=True, big_is_error=False)\n",
    "        )\n",
    "        self.test_values = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=softmax_layer,\n",
    "            mode=NanGuardMode(nan_is_error=True, inf_is_error=True, big_is_error=False)\n",
    "        )\n",
    "        \n",
    "    def train_mini_batch(self, testing_target, testing_source):\n",
    "        one_hot_input = np.eye(self.vocab_input_size)[testing_target].T\n",
    "        one_hot_input = np.asarray(one_hot_input).astype(config.floatX)\n",
    "        print(\"one_hot_input\", one_hot_input, np.shape(one_hot_input))\n",
    "        softmax_matrix = self.test_values(one_hot_input)\n",
    "#        print(\"softmax_matrix\", softmax_matrix, np.shape(softmax_matrix))\n",
    "        \n",
    "        emission_posterior_vout = np.zeros_like(softmax_matrix.T) # [V_f_size, e_size]\n",
    "        emission_matrix = [] # [f_size, e_size]\n",
    "        for indice in testing_source:\n",
    "            emission_matrix.append(softmax_matrix[indice])\n",
    "        print(\"emission_matrix\", emission_matrix, np.shape(emission_matrix))\n",
    "        # Normalize emission_matrix\n",
    "#         emission_matrix = self.baum_welch_model.normalize_matrix(emission_matrix, axis=0)\n",
    "#        print(\"emission_matrix nomalized\", emission_matrix, np.shape(emission_matrix))\n",
    "        emission_posterior, transition_posterior = \\\n",
    "            self.baum_welch_model.calculate_baum_welch_posteriors(len(testing_target), np.transpose(emission_matrix))\n",
    "        print(\"emission_posterior\", emission_posterior, np.shape(emission_posterior))\n",
    "        \n",
    "        # transform emission size to [target_size, v_out]\n",
    "        for i, indice in enumerate(testing_source):\n",
    "            emission_posterior_vout[:, indice] = np.maximum(emission_posterior_vout[:, indice], emission_posterior[:, i])\n",
    "#         print(\"emission_posterior_vout\", emission_posterior_vout, np.shape(emission_posterior_vout))\n",
    "        self.train_mini_batch_function(one_hot_input, np.asarray(emission_posterior_vout).astype(config.floatX))\n",
    "        \n",
    "        return emission_posterior, transition_posterior\n",
    "    \n",
    "    def test_mini_batch(self, testing_target, testing_source):\n",
    "        one_hot_input = np.eye(self.vocab_input_size)[testing_target].T\n",
    "        one_hot_input = np.asarray(one_hot_input).astype(config.floatX)\n",
    "        softmax_matrix = self.test_values(one_hot_input)\n",
    "        \n",
    "        emission_matrix = [] # [f_size, e_size]\n",
    "        for indice in testing_source:\n",
    "            emission_matrix.append(softmax_matrix[indice])\n",
    "        return emission_matrix\n",
    "        \n",
    "    def train_model_epoch(self, target_inputs, source_inputs, \n",
    "                          aer_target_inputs=None, aer_source_inputs=None, \n",
    "                          input_indice_shift=0, align_indice_sift=0):\n",
    "        if (aer_target_inputs==None):\n",
    "            aer_target_inputs=target_inputs\n",
    "        if (aer_source_inputs==None):\n",
    "            aer_source_inputs=source_inputs\n",
    "        # TODO: add epoch functionality\n",
    "        for epoch in range(self.epoch):\n",
    "            print(\"******** Epoch \", epoch, \" ***********\")\n",
    "            self.emission_posteriors = []\n",
    "            self.transition_posteriors = []\n",
    "        #         for target_inputs_batch, source_inputs_batch in zip(np.split(target_inputs, self.batch), np.split(source_inputs, self.batch)):\n",
    "        #             for x_target, x_source in zip(target_inputs_batch, source_inputs_batch):\n",
    "            for i, x_target, x_source in zip(range(len(target_inputs)), target_inputs, source_inputs):\n",
    "                print(\"\\n+++++++++ The sentence \", i)\n",
    "                if (len(x_target) <= 1 or len(x_source) <= 1):\n",
    "                    self.emission_posteriors.append(np.zeros((len(x_target), len(x_source))))\n",
    "                    self.transition_posteriors.append(np.zeros((1, len(x_source), len(x_source))))\n",
    "                    continue\n",
    "                \n",
    "                xx_target = [int(x)+input_indice_shift for x in x_target]\n",
    "                xx_source = [int(x)+input_indice_shift for x in x_source]\n",
    "                \n",
    "                print(\"xx_source: \", len(xx_source), \" => \", xx_source)\n",
    "                print(\"xx_target: \", len(xx_target), \" => \", xx_target)\n",
    "                emis_posterior, trans_posterior = self.train_mini_batch(xx_target, xx_source)\n",
    "                self.emission_posteriors.append(emis_posterior)\n",
    "                self.transition_posteriors.append(trans_posterior)\n",
    "            \n",
    "            # Update Non-negative set of BW model\n",
    "            self.baum_welch_model.update_non_negative_transition_set(self.emission_posteriors, self.transition_posteriors)\n",
    "            print(\"New non_negative_set\", self.baum_welch_model.non_negative_set)\n",
    "            \n",
    "            if self.out_prefix != None:\n",
    "                align = self.get_alignment(target_inputs=aer_target_inputs, \n",
    "                                  source_inputs=aer_source_inputs, \n",
    "                                  input_indice_shift=input_indice_shift)\n",
    "                self.save_params(self.out_prefix + \"_params_epoch_\" + str(epoch))\n",
    "                self.save_obj(align, self.out_prefix + \"_alignment_epoch_\" + str(epoch))\n",
    "            if self.target_AER != None:\n",
    "                if self.out_prefix == None:\n",
    "                    align = self.get_alignment(target_inputs=aer_target_inputs, \n",
    "                                  source_inputs=aer_source_inputs, \n",
    "                                  input_indice_shift=input_indice_shift,\n",
    "                                  align_indice_sift=align_indice_sift)\n",
    "                # TODO: calculate AER\n",
    "                print(\"Epoch \", epoch, \"Alignment score:\", self.calculate_AER_score(result=align, align_indice_sift=align_indice_sift))\n",
    "                \n",
    "    def train_model(self, target_inputs, source_inputs, input_indice_shift=0):\n",
    "        self.emission_posteriors = []\n",
    "        self.transition_posteriors = []\n",
    "#         for target_inputs_batch, source_inputs_batch in zip(np.split(target_inputs, self.batch), np.split(source_inputs, self.batch)):\n",
    "#             for x_target, x_source in zip(target_inputs_batch, source_inputs_batch):\n",
    "        for i, x_target, x_source in zip(range(len(target_inputs)), target_inputs, source_inputs):\n",
    "            if (len(x_target) == 1 or len(x_source) == 1):\n",
    "                self.emission_posteriors.append(np.zeros((len(x_target), len(x_source))))\n",
    "                self.transition_posteriors.append(np.zeros((len(1), len(x_source), len(x_source))))\n",
    "                continue\n",
    "            xx_target = [int(x)+input_indice_shift for x in x_target]\n",
    "            xx_source = [int(x)+input_indice_shift for x in x_source]\n",
    "            print(\"\\n+++++++++ The sentence \", i)\n",
    "            print(\"xx_source: \", len(xx_source), \" => \", xx_source)\n",
    "            print(\"xx_target: \", len(xx_target), \" => \", xx_target)\n",
    "            emis_posterior, trans_posterior = self.train_mini_batch(xx_target, xx_source)\n",
    "            self.emission_posteriors.append(emis_posterior)\n",
    "            self.transition_posteriors.append(trans_posterior)\n",
    "            \n",
    "        return self.transition_posteriors\n",
    "    \n",
    "    def get_alignment(self, target_inputs, source_inputs, \n",
    "                      input_indice_shift=0, align_indice_sift=0):\n",
    "        print(\"Calculating alignments ...\")\n",
    "        alignments = []\n",
    "        for i, x_target, x_source in zip(range(len(target_inputs)), target_inputs, source_inputs):\n",
    "            align = [0]\n",
    "            if (len(x_target) == 1 or len(x_source) == 1):\n",
    "                alignments.append(align)\n",
    "                continue\n",
    "            xx_target = [int(x)+input_indice_shift for x in x_target]\n",
    "            xx_source = [int(x)+input_indice_shift for x in x_source]\n",
    "#            print(\"\\n+++++++++ The sentence \", i)\n",
    "#            print(\"xx_source: \", len(xx_source), \" => \", xx_source)\n",
    "#            print(\"xx_target: \", len(xx_target), \" => \", xx_target)\n",
    "            emis_matrix = self.test_mini_batch(xx_target, xx_source)\n",
    "            trans_matrix = self.baum_welch_model.generate_transition_distant_matrix(len(xx_target))\n",
    "            \n",
    "            # Calculate aligment [VITERBI]\n",
    "            for ind, t in enumerate(range(1, len(x_source))):\n",
    "                mul = np.array([emis_matrix[ind], emis_matrix[ind]]).flatten() * trans_matrix[align[-1]]\n",
    "#                print(\"max\", np.argmax(mul), \":\", mul)\n",
    "                align.append(np.argmax(mul))\n",
    "            \n",
    "            exporting_align = []\n",
    "            for ia, a in enumerate(align):\n",
    "                if (a < len(xx_target) or a==0 or len(align)==1):\n",
    "                    exporting_align.append(str(ia+align_indice_sift) + \"-\" + str(a+align_indice_sift)) # indice starts from 0\n",
    "            \n",
    "            assert(align_indice_sift >= 0)\n",
    "            \n",
    "            if(len(exporting_align) < 1):\n",
    "                exporting_align.append(align_indice_sift + \"-\" + align_indice_sift)\n",
    "#            print(\"Align \", i, \" : \", exporting_align, len(exporting_align))\n",
    "            alignments.append(exporting_align)\n",
    "            \n",
    "        return alignments\n",
    "    \n",
    "    def calculate_AER(self, S, P, A):\n",
    "        S, P, A = np.array(S), np.array(P), np.array(A)\n",
    "        s_a, p_a, len_s, len_a = 0, 0, 0, 0\n",
    "        t = 0.2\n",
    "        for s, p, a in zip(S, P, A):\n",
    "            s, p, a = np.array(s), np.array(p), np.array(a)\n",
    "            s_a += len(list(set(s).intersection(a)))\n",
    "            p_a += len(list(set(p).intersection(a)))\n",
    "            len_s += len(s[s != \"\"])\n",
    "            len_a += len(a[a != \"\"])\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a += s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len_s + len_a)\n",
    "        print (\"aer\", 1.-aer-t)\n",
    "        \n",
    "        return 1. - aer - t\n",
    "    \n",
    "    def calculate_AER_score(self, result, align_indice_sift=0):\n",
    "        target_file = open(self.target_AER) # Index starts from 1\n",
    "\n",
    "        target_lines = target_file.readlines()\n",
    "        target_lines = [str(line[:-1]) for line in target_lines]\n",
    "        target_lines = np.reshape(target_lines, (2501, 2))\n",
    "        \n",
    "        sure = target_lines[:,0]\n",
    "        possible = target_lines[:,1]\n",
    "        \n",
    "        S, P, A = [], [], []\n",
    "        \n",
    "        # Split and plus 1 to result indexes\n",
    "        for a, s_, p_ in zip(result, sure, possible):\n",
    "            for i, number in enumerate(a):\n",
    "                if(isinstance(number, numbers.Number)):\n",
    "                    a[i] = str(align_indice_sift) + \"-\" + str(align_indice_sift)\n",
    "                else:\n",
    "                    n1 = int(number.split(\"-\")[0]) + align_indice_sift\n",
    "                    n2 = int(number.split(\"-\")[1]) + align_indice_sift\n",
    "                    number = str(n1) + \"-\" + str(n2)\n",
    "                    a[i] = number\n",
    "            S.append(s_.strip().split(\" \"))\n",
    "            P.append(p_.strip().split(\" \"))\n",
    "            A.append(a)\n",
    "        return self.calculate_AER(S, P, A)\n",
    "\n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "    ]).astype(config.floatX)\n",
    "\n",
    "posteriors = np.asarray([\n",
    "    [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62, 0.24, 0.71, 0.81],\n",
    "    [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55, 0.11, 0.37, 0.18],\n",
    "    [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05, 0.62, 0.17, 0.29],\n",
    "    [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51, 0.19, 0.67, 0.52],\n",
    "    [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11, 0.44, 0.37, 0.14]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "vocab_input_size = np.shape(x)[0]\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = 12\n",
    "\n",
    "model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                      vocab_output_size=vocab_output_size, baum_welch_model=None)\n",
    "\n",
    "result = model.test(x)\n",
    "print(np.shape(result[0]))\n",
    "print(np.shape(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00170076e-02   1.20083610e-10   1.39718434e-15   4.68368556e-17\n",
      "    7.48935809e-12]\n",
      " [  1.22934855e-12   7.03421055e-09   1.19096011e-09   2.06476147e-09\n",
      "    2.72379169e-04]\n",
      " [  2.44000323e-11   1.05879911e-13   3.30180978e-12   3.29048344e-05\n",
      "    7.82270106e-08]\n",
      " [  1.74506021e-10   5.58641628e-16   5.26185981e-14   5.10033259e-13\n",
      "    8.75874785e-13]\n",
      " [  1.14724586e-04   1.34182380e-06   9.58455260e-11   9.28782995e-10\n",
      "    1.30416092e-05]\n",
      " [  8.76298368e-01   6.43898308e-01   2.79844013e-11   4.83316711e-08\n",
      "    3.32501895e-06]\n",
      " [  3.90381893e-10   8.50837387e-06   5.96516020e-06   3.58708603e-05\n",
      "    2.44137453e-04]\n",
      " [  3.38336337e-09   8.17109505e-11   1.03117272e-11   8.37082140e-11\n",
      "    1.07731218e-07]\n",
      " [  1.48342166e-03   1.33874192e-08   1.30569804e-13   1.14827958e-09\n",
      "    1.09271004e-05]\n",
      " [  6.92231323e-16   6.27401890e-20   1.05678456e-15   1.24847452e-10\n",
      "    1.28488304e-08]\n",
      " [  1.40317807e-05   7.53151819e-10   2.50952876e-08   1.97411268e-10\n",
      "    1.23392374e-05]\n",
      " [  1.12072520e-01   3.56091917e-01   9.99994040e-01   9.99931216e-01\n",
      "    9.99443591e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Emission model variables\n",
    "vocab_input_size = 10\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = 12\n",
    "\n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "]).astype(config.floatX)\n",
    "posteriors = np.asarray([\n",
    "    [  1.00, 0.00, 4.86, 0.00,  0.00, 0.00, 0.00, 8.30,  0.00, 0.00, 7.54, 0.00],\n",
    "    [  1.95, 0.00, 2.71, 0.00,  0.00, 0.00, 0.00, 1.03,  0.00, 0.00, 1.83, 0.00],\n",
    "    [  1.65, 0.00, 6.90, 0.00,  0.00, 0.00, 0.00, 8.50,  0.00, 0.00, 1.12, 0.00],\n",
    "    [  8.81, 0.00, 9.93, 0.00,  0.00, 0.00, 0.00, 1.47,  0.00, 0.00, 7.40, 0.00],\n",
    "    [  1.30, 0.00, 6.29, 0.00,  0.00, 0.00, 0.00, 9.91,  0.00, 0.00, 2.44, 0.00],\n",
    "]).astype(config.floatX)\n",
    "\n",
    "testing_target = [6, 8, 2, 1, 3]\n",
    "testing_source = [2, 7, 10, 0]\n",
    "\n",
    "model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                      vocab_output_size=vocab_output_size, baum_welch_model=None)\n",
    "result = model.train_mini_batch_function(x, posteriors)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaumWelchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.  69.  76.  58.   0.   0.   0.   0.]\n",
      " [ 87.  63.  69.  76.   0.   0.   0.   0.]\n",
      " [ 93.  87.  63.  69.   0.   0.   0.   0.]\n",
      " [ 53.  93.  87.  63.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ],\n",
       "       [ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix and \"Baum Welch Algorithm\"\n",
    "\n",
    "Compute forward messages: alpha <br>\n",
    "Compute backward messages: beta <br>\n",
    "Compute posteriors: <br>\n",
    "    p(z|x) = alpha * beta <br>\n",
    "    p(z_i, z_i+1 | x) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaumWelchModel:\n",
    "    \n",
    "    def add_matrix(self, a, b):\n",
    "        # compatible with two matrices different shape\n",
    "        c = np.zeros(np.max([np.shape(a), np.shape(b)], axis=0))\n",
    "        c[:np.shape(a)[0], :np.shape(a)[1]] += a\n",
    "        c[:np.shape(b)[0], :np.shape(b)[1]] += b\n",
    "        return c\n",
    "    \n",
    "    def add_vector(self, a, b):\n",
    "        # compatible with two vectors different shape\n",
    "        c = np.zeros(np.max([np.shape(a), np.shape(b)]))\n",
    "        c[:len(a)] += a\n",
    "        c[:len(b)] += b\n",
    "        return c\n",
    "    \n",
    "    def add_matrix_list(self, a):\n",
    "        sum_matrix = [[0]]\n",
    "        for aa in a:\n",
    "            sum_matrix = self.add_matrix(aa, sum_matrix)\n",
    "            \n",
    "        return sum_matrix\n",
    "    \n",
    "    def normalize_matrix(self, x, axis=1, whole_matrix=False):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\n",
    "            axis=1: row\n",
    "            axis=0: column \n",
    "        Input\n",
    "        -----\n",
    "        \n",
    "        Output\n",
    "        ------\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) == 1 or whole_matrix:\n",
    "#             e_x = np.exp(x - np.max(x))\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x)\n",
    "        if axis == 0:\n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[None, :]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[None, :]\n",
    "        else: \n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[:, None]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[:, None]\n",
    "        \n",
    "    def generate_transition_distant_matrix(self, sentence_length, \n",
    "                                           po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance \n",
    "        in the latent sentence.\n",
    "        We extend the latent sentence for 2*length in which each word has \n",
    "        an empty word to represent no-alignment state.\n",
    "        where [sentence_length:end] elements are empty words considered as \n",
    "        latent words having no direct aligment.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_distant_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_distant_matrix = np.zeros((2*sentence_length, 2*sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = j - i + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_distant_matrix[i][j] = p_\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            trans_distant_matrix[i+sentence_length][i+sentence_length] = po\n",
    "            trans_distant_matrix[i+sentence_length][i] = po\n",
    "\n",
    "            sum_d = np.sum(trans_distant_matrix[i, :sentence_length])\n",
    "            trans_distant_matrix[i, :sentence_length] = \\\n",
    "                    np.divide(\n",
    "                        trans_distant_matrix[i, :sentence_length], \n",
    "                        sum_d\n",
    "                    )\n",
    "            trans_distant_matrix[i, sentence_length:] = \\\n",
    "                    np.copy(trans_distant_matrix[i, :sentence_length])\n",
    "\n",
    "        return trans_distant_matrix\n",
    "    \n",
    "    def generate_transition_matrix(self, sentence_length, po=0., \n",
    "                                   nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_matrix = np.zeros((sentence_length, sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = j - i + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_matrix[i][j] = p_\n",
    "        if nomalized:\n",
    "            return self.normalize_matrix(trans_matrix, axis=1)\n",
    "        return trans_matrix\n",
    "        \n",
    "    def __init__(self, max_distance, po=0.3, seed=1402):\n",
    "        np.random.seed(seed)\n",
    "        self.max_distance = max_distance\n",
    "        self.non_negative_set = np.random.randint(\n",
    "                                    low=1, high=100, \n",
    "                                    size=[max_distance + max_distance + 3]\n",
    "        )\n",
    "        self.po = po\n",
    "        \n",
    "    def calc_forward_messages(self, unary_matrix, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the forward messages ~ alpha values.\n",
    "        \n",
    "        \n",
    "        Input\n",
    "        -----\n",
    "        unary_matrix: emission posteriors - marginal probabilities ~ initial matrix.\n",
    "                      size ~ [1, target_len]\n",
    "        transition_matrix: size ~ [target_len, target_len]\n",
    "        emission_matrix: size ~ [target_len, source_len]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        alpha = np.zeros(np.shape(emission_matrix))\n",
    "#        print(\"emission_matrix[:,0]\", emission_matrix[:, 0])\n",
    "#        print(\"unary_matrix\", unary_matrix)\n",
    "        alpha.T[0] = np.multiply(emission_matrix[:,0], unary_matrix)\n",
    "#        print(\"alpha.T[0]\", alpha.T[0])\n",
    "        \n",
    "        for t in np.arange(1, source_len):\n",
    "            for i in range(target_len):\n",
    "                sum_al = 0.0\n",
    "                for j in range(target_len):\n",
    "                    sum_al += alpha[j][t-1] * transition_matrix[j][i]\n",
    "\n",
    "                alpha[i][t] = emission_matrix[i][t] * sum_al\n",
    "        \n",
    "        norm_alpha = np.sum(alpha, axis=0)\n",
    "        return np.divide(alpha, norm_alpha), norm_alpha\n",
    "    \n",
    "    \n",
    "    def calc_backward_messages(self, transition_matrix, emission_matrix, norm_alpha):\n",
    "        \"\"\"Calcualte the backward messages ~ beta values.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        beta\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "        assert(len(norm_alpha) == source_len) # = t_size\n",
    "\n",
    "        beta = np.zeros(np.shape(emission_matrix))\n",
    "        beta[:,-1] = [1]*target_len\n",
    "\n",
    "        for t in reversed(range(source_len-1)):\n",
    "            for i in range(target_len):\n",
    "                for j in range(target_len):\n",
    "                    beta[i][t] += beta[j][t+1] * transition_matrix[i][j] * emission_matrix[j][t+1]\n",
    "                    \n",
    "        return np.divide(beta, norm_alpha)\n",
    "\n",
    "    def calc_posterior_matrix(self, alpha, beta, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the gama and epsilon values in order to reproduce \n",
    "        better transition and emission matrix.\n",
    "        \n",
    "        gamma: P(e_aj|f_j)\n",
    "        epsilon: P(e_aj,e_a(j+1)|f_j)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        unary_matrix, posterior_gamma, posterior_epsilon\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(alpha)[1]\n",
    "        target_len = np.shape(alpha)[0]\n",
    "\n",
    "        gamma = np.multiply(alpha, beta)\n",
    "        epsilon = np.zeros((source_len-1, target_len, target_len))\n",
    "\n",
    "        # Normalization on columns\n",
    "        gamma = self.normalize_matrix(gamma, axis=0)\n",
    "\n",
    "        for t in range(source_len-1):   \n",
    "            for i in range(target_len):\n",
    "                for j in range(target_len):\n",
    "                    epsilon[t][i][j] = alpha[i][t] * transition_matrix[i][j] * \\\n",
    "                                        beta[j][t+1] * emission_matrix[j][t+1]\n",
    "            # Normalization\n",
    "            epsilon[t] = self.normalize_matrix(epsilon[t], whole_matrix=True)\n",
    "\n",
    "        # Update unary matrix\n",
    "        # Normalization unary\n",
    "        new_unary_matrix = np.copy(gamma[:,0])\n",
    "        #self.normalize_matrix(np.copy(gamma[:,0]), axis=1)\n",
    "\n",
    "        return new_unary_matrix, gamma, epsilon\n",
    "\n",
    "\n",
    "    def calculate_baum_welch_posteriors(self, sentence_length, emission_matrix, unary_matrix=None):\n",
    "        if unary_matrix == None:\n",
    "            unary_matrix = [0.02]*sentence_length\n",
    "            unary_matrix[0] = 1 - np.sum(unary_matrix) + 0.02\n",
    "        transition_matrix = self.generate_transition_matrix(sentence_length)\n",
    "#         emission_matrix = self.normalize_matrix(emission_matrix, axis=0)\n",
    "        \n",
    "        alpha, norm_alpha = self.calc_forward_messages(unary_matrix, \n",
    "                                           transition_matrix, emission_matrix)\n",
    "        print(\"norm_alpha\", norm_alpha, len(norm_alpha))\n",
    "        print(\"alpha\", alpha, np.sum(alpha, axis=0))\n",
    "        beta = self.calc_backward_messages(transition_matrix, emission_matrix, norm_alpha)\n",
    "        print(\"beta\", beta, np.sum(beta, axis=0))\n",
    "        \n",
    "        new_unary_matrix, emission_posterior, transition_posterior \\\n",
    "            = self.calc_posterior_matrix(alpha, beta, \n",
    "                                         transition_matrix, emission_matrix)\n",
    "        return emission_posterior, transition_posterior # gamma, epsilon\n",
    "    \n",
    "    def update_non_negative_transition_set(self, emission_posteriors, \n",
    "                                           transition_posteriors):\n",
    "        # 1: update non-negative set: s[-1] = \n",
    "        # 1.1: calculate new transition matrix\n",
    "        sum_ep = [[0]]\n",
    "        sum_gamma = [0]\n",
    "        for gamma, epsilon in zip(emission_posteriors, transition_posteriors):\n",
    "            if (np.shape(gamma)[0] <= 1 or np.shape(gamma)[1] <= 1):\n",
    "                print(\"passed \", np.shape(gamma))\n",
    "                continue\n",
    "            sum_ep = self.add_matrix(self.add_matrix_list(epsilon), sum_ep)\n",
    "            print(\"self.add_matrix_list(epsilon)\", self.add_matrix_list(epsilon))\n",
    "            print(\"sum_ep\", sum_ep)\n",
    "            sum_gamma = self.add_vector(np.sum(gamma, axis=1), sum_gamma)\n",
    "            print (\"len 2\", np.shape(sum_ep), len(sum_gamma))\n",
    "            assert(len(sum_ep) == len(sum_gamma))\n",
    "        \n",
    "        # 1.2: update\n",
    "        new_non_negative_set = np.zeros(self.max_distance + self.max_distance + 3)\n",
    "        new_non_negative_set_gamma = np.zeros(self.max_distance + self.max_distance + 3)\n",
    "        \n",
    "        print (\"len\", np.shape(sum_ep), np.shape(sum_gamma))\n",
    "        assert(len(sum_ep) == len(sum_gamma))\n",
    "            \n",
    "        \n",
    "        for i in range(len(sum_ep)):\n",
    "            for j in range(len(sum_ep)):\n",
    "                indice = j - i + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    new_non_negative_set[0] += sum_ep[i][j]\n",
    "                    new_non_negative_set_gamma[0] += sum_gamma[i]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    new_non_negative_set[-1] += sum_ep[i][j]\n",
    "                    new_non_negative_set_gamma[-1] += sum_gamma[i]\n",
    "                else:\n",
    "                    new_non_negative_set[indice] += sum_ep[i][j]\n",
    "                    new_non_negative_set_gamma[indice] += sum_gamma[i]\n",
    "        \n",
    "        self.old_non_negative_set = np.copy(self.non_negative_set)\n",
    "        self.non_negative_set = np.array(np.divide(new_non_negative_set, new_non_negative_set_gamma))\n",
    "        for i, old_n, new_n in zip(range(len(self.non_negative_set)), self.old_non_negative_set, self.non_negative_set):\n",
    "            if (np.isnan(new_n)):\n",
    "                self.non_negative_set[i] = np.copy(old_n)\n",
    "        \n",
    "        print (\"self.non_negative_set\", self.non_negative_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with Unsupervised neural hidden markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Read training file\n",
    "# Read vocab en - source\n",
    "#\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.en\"\n",
    "#\"E:/Working/Intership2017/data/en-cz/formatted/testing/testing.en-cz.en\"\n",
    "en = []\n",
    "en_tokenizer = Tokenizer(lower=False, filters='\\t\\n')\n",
    "with open(\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.en\", encoding=\"utf8\") as en_file:\n",
    "    for line in en_file:\n",
    "        en.append(line.strip())\n",
    "        en_tokenizer.fit_on_texts([line.strip()])\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en)\n",
    "en_source_indices = en_tokenizer.word_index\n",
    "\n",
    "# Read vocab cz - target\n",
    "#\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.cz\"\n",
    "#\"E:/Working/Intership2017/data/en-cz/formatted/testing/testing.en-cz.cz\"\n",
    "cz = []\n",
    "cz_tokenizer = Tokenizer(lower=False, filters='\\t\\n')\n",
    "with open(\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.cz\", encoding='utf8') as cz_file:\n",
    "    for line in cz_file:\n",
    "        cz.append(line.strip())\n",
    "        cz_tokenizer.fit_on_texts([line.strip()])\n",
    "cz_sequences = cz_tokenizer.texts_to_sequences(cz)\n",
    "cz_target_indices = cz_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_sequences = en_sequences[:2]\n",
    "cz_sequences = cz_sequences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9048\n",
      "15002\n",
      "[[30, 713, 5, 2094, 1404, 3], [30, 1010, 100, 2095, 3], [30, 111, 25, 26, 142, 397, 69, 2095, 26, 62, 3]]\n",
      "[[2978, 50, 5, 1157, 5, 2979, 2], [5159, 50, 27, 2980, 2], [2981, 50, 183, 355, 2008, 1468, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(len(en_source_indices))\n",
    "print(len(cz_target_indices))\n",
    "print(en_sequences[:3])\n",
    "print(cz_sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_negative_set [29 56 82 13 35 53 25 23 21 12 15  9 13 87  9 63 62 52 43 77 95 79 77  5 78\n",
      " 41 10 10 88 19  1 37  9 70 22 82 46 51 97 46 12 32 56 30 86 45 99 89  1 88\n",
      " 94 48 26 65 53 54 76 48 98 63 67 74 40]\n",
      "******** Epoch  0  ***********\n",
      "\n",
      "+++++++++ The sentence  0\n",
      "xx_source:  6  =>  [29, 712, 4, 2093, 1403, 2]\n",
      "xx_target:  7  =>  [2977, 49, 4, 1156, 4, 2978, 1]\n",
      "one_hot_input [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] (15002, 7)\n",
      "emission_matrix [array([  4.59788949e-21,   2.90619664e-22,   3.22282087e-16,\n",
      "         2.60080953e-21,   3.22283331e-16,   1.00000000e+00,\n",
      "         3.33859704e-16], dtype=float32), array([  7.91874826e-02,   3.63307237e-07,   2.25980051e-11,\n",
      "         9.20812130e-01,   2.25973146e-11,   2.73342783e-28,\n",
      "         2.09339004e-10], dtype=float32), array([  1.73523020e-22,   6.55232356e-19,   7.75779703e-30,\n",
      "         3.65769171e-09,   7.75785646e-30,   1.00000000e+00,\n",
      "         5.33339475e-12], dtype=float32), array([  1.81968486e-23,   9.99855280e-01,   7.23630146e-05,\n",
      "         5.09992602e-13,   7.23625999e-05,   1.29258344e-20,\n",
      "         1.55979721e-18], dtype=float32), array([  2.00700815e-04,   7.70382835e-18,   9.66927573e-29,\n",
      "         9.63482400e-19,   9.66912767e-29,   2.17137051e-21,\n",
      "         9.99799311e-01], dtype=float32), array([  3.44659763e-32,   6.46849908e-21,   1.55636585e-20,\n",
      "         9.07954353e-23,   1.55633031e-20,   1.00000000e+00,\n",
      "         2.26948546e-23], dtype=float32)] (6, 7)\n",
      "norm_alpha [  2.00000000e-02   2.10198953e-03   5.85435049e-04   3.36624922e-05\n",
      "   5.80205144e-06   2.81688930e-08] 6\n",
      "alpha [[  2.02307138e-19   4.33018783e-02   2.16370556e-22   1.81877674e-23\n",
      "    4.41739112e-06   1.41292212e-30]\n",
      " [  2.90619664e-22   1.98666320e-07   1.76728631e-19   9.99356291e-01\n",
      "    6.19011827e-18   6.46766174e-20]\n",
      " [  3.22282087e-16   1.08743416e-10   3.74665111e-31   6.36476727e-04\n",
      "    1.89531320e-29   1.55619163e-19]\n",
      " [  2.60080953e-21   9.56697923e-01   1.92919991e-09   9.68502359e-13\n",
      "    1.46472204e-18   7.98894911e-21]\n",
      " [  3.22283331e-16   1.23568288e-12   1.28693599e-30   7.23264872e-06\n",
      "    4.62985894e-29   2.95667379e-19]\n",
      " [  1.00000000e+00   5.53043809e-28   9.99999998e-01   4.78017195e-20\n",
      "    3.86727015e-21   1.00000000e+00]\n",
      " [  3.33859704e-16   1.03025057e-10   1.77179854e-12   1.40311690e-18\n",
      "    9.99995583e-01   8.39602073e-22]] [ 1.  1.  1.  1.  1.  1.]\n",
      "beta [[  4.69747327e-05   3.32168987e-03   4.06417522e-02   2.32968304e+01\n",
      "    2.50101902e+04   3.55001526e+07]\n",
      " [  1.65368927e-04   7.03012593e-03   1.97945381e-01   2.48429158e+01\n",
      "    5.29323305e+04   3.55001526e+07]\n",
      " [  2.58274090e-05   2.09832112e-03   6.03511313e-03   4.93293491e+01\n",
      "    1.57990096e+04   3.55001526e+07]\n",
      " [  1.04700917e-04   6.51363540e-03   1.10320166e-01   1.32034696e+01\n",
      "    4.90434889e+04   3.55001526e+07]\n",
      " [  3.87539581e-06   8.80414454e-04   5.37168465e-01   4.31671788e+01\n",
      "    6.62895509e+03   3.55001526e+07]\n",
      " [  7.04222326e-05   4.86757877e-03   8.21887435e-02   7.50721119e+00\n",
      "    3.66497402e+04   3.55001526e+07]\n",
      " [  2.74786126e-04   1.11120319e-04   6.93595371e-02   2.60682635e+01\n",
      "    8.36664235e+02   3.55001526e+07]] [  6.91955740e-04   2.48228859e-02   1.04365916e+00   1.87415218e+02\n",
      "   1.86900379e+05   2.48501068e+08]\n",
      "emission_posterior [[  1.34947777e-19   2.25609371e-02   1.06993709e-22   1.70450774e-23\n",
      "    1.32031093e-04   1.41292212e-30]\n",
      " [  6.82447294e-22   2.19067862e-07   4.25637561e-19   9.98724419e-01\n",
      "    3.91573536e-16   6.46766174e-20]\n",
      " [  1.18197208e-16   3.57903744e-11   2.75116303e-32   1.26302127e-03\n",
      "    3.57852637e-28   1.55619163e-19]\n",
      " [  3.86677804e-21   9.77438844e-01   2.58952315e-09   5.14412488e-13\n",
      "    8.58479533e-17   7.98894911e-21]\n",
      " [  1.77355279e-17   1.70641835e-13   8.41114490e-30   1.25595416e-05\n",
      "    3.66780473e-28   2.95667379e-19]\n",
      " [  1.00000000e+00   4.22244344e-28   9.99999997e-01   1.44359345e-20\n",
      "    1.69382595e-19   1.00000000e+00]\n",
      " [  1.30271381e-15   1.79567468e-12   1.49523063e-12   1.47139308e-18\n",
      "    9.99867969e-01   8.39602073e-22]] (7, 6)\n",
      "\n",
      "+++++++++ The sentence  1\n",
      "xx_source:  5  =>  [29, 1009, 99, 2094, 2]\n",
      "xx_target:  5  =>  [5158, 49, 26, 2979, 1]\n",
      "one_hot_input [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] (15002, 5)\n",
      "emission_matrix [array([  1.76407229e-02,   7.95063182e-10,   6.32125591e-13,\n",
      "         9.81469393e-01,   8.89859919e-04], dtype=float32), array([  4.44018890e-18,   9.51344967e-01,   1.25126662e-33,\n",
      "         1.31155801e-23,   4.86550257e-02], dtype=float32), array([  6.68356657e-01,   1.49794196e-12,   3.41689455e-09,\n",
      "         4.25252974e-25,   3.31643343e-01], dtype=float32), array([  4.81641990e-19,   6.02503689e-40,   1.53064437e-13,\n",
      "         1.00000000e+00,   7.47327063e-24], dtype=float32), array([  3.27221533e-14,   1.92131832e-22,   3.80872923e-42,\n",
      "         1.00000000e+00,   7.05304462e-25], dtype=float32)] (5, 5)\n",
      "norm_alpha [  3.58766502e-02   3.29556069e-03   2.11599194e-04   4.10316120e-06\n",
      "   9.85824444e-07] 5\n",
      "alpha [[  4.52368463e-01   1.87917274e-17   1.38319667e-01   1.95861799e-18\n",
      "    7.78256619e-14]\n",
      " [  4.43220411e-10   8.93691950e-01   6.95797905e-12   1.53761209e-38\n",
      "    9.86622920e-23]\n",
      " [  3.52388302e-13   2.00987416e-33   3.77285557e-09   1.18115314e-12\n",
      "    1.02938628e-43]\n",
      " [  5.47135470e-01   2.52286222e-23   2.98534879e-24   1.00000000e+00\n",
      "    1.00000000e+00]\n",
      " [  4.96066336e-04   1.06308050e-01   8.61680330e-01   9.91417019e-23\n",
      "    1.71560545e-25]] [ 1.  1.  1.  1.  1.]\n",
      "beta [[  7.99289862e-04   8.77622229e-01   1.13544733e+02   2.43714529e+04\n",
      "    1.01437939e+06]\n",
      " [  1.63844775e-03   5.97433664e-02   5.71808009e+02   1.22733936e+05\n",
      "    1.01437939e+06]\n",
      " [  8.47203390e-04   7.61017600e-01   7.51398970e+01   1.61281674e+04\n",
      "    1.01437939e+06]\n",
      " [  7.35972372e-04   2.79345395e+00   2.72802281e+02   5.85547895e+04\n",
      "    1.01437939e+06]\n",
      " [  3.33923328e-03   3.51596571e-01   7.32546666e+00   1.57235180e+03\n",
      "    1.01437939e+06]] [  7.36014666e-03   4.84343372e+00   1.04062039e+03   2.23360697e+05\n",
      "   5.07189696e+06]\n",
      "emission_posterior [[  4.72085666e-01   1.81690978e-16   7.13311726e-01   8.15208570e-19\n",
      "    7.78256619e-14]\n",
      " [  9.48148882e-10   5.88215659e-01   1.80701512e-10   3.22291626e-38\n",
      "    9.86622920e-23]\n",
      " [  3.89792390e-13   1.68508811e-32   1.28756514e-08   3.25333517e-13\n",
      "    1.02938628e-43]\n",
      " [  5.25751561e-01   7.76415318e-22   3.69889081e-23   1.00000000e+00\n",
      "    1.00000000e+00]\n",
      " [  2.16277183e-03   4.11784341e-01   2.86688261e-01   2.66221832e-24\n",
      "    1.71560545e-25]] (5, 5)\n",
      "self.add_matrix_list(epsilon) [[  9.27121085e-21   7.07272787e-21   3.24632515e-24   5.29750830e-11\n",
      "    3.78529298e-24   2.26929681e-02   1.12581506e-13]\n",
      " [  1.30266661e-04   3.91983938e-16   1.48773582e-23   1.00398843e-15\n",
      "    3.18233841e-23   2.19067861e-07   9.98594153e-01]\n",
      " [  1.75367487e-06   7.50331759e-21   3.51712530e-27   1.08261124e-16\n",
      "    2.79181169e-27   3.57903743e-11   1.26126759e-03]\n",
      " [  1.20579216e-14   2.58934932e-09   1.95848102e-14   2.53654715e-09\n",
      "    1.54243783e-13   9.77438841e-01   1.88500313e-12]\n",
      " [  1.07573601e-08   8.15240651e-21   1.85186756e-27   1.23285177e-17\n",
      "    1.51307361e-27   1.70641835e-13   1.25487843e-05]\n",
      " [  2.25609371e-02   9.98724636e-01   1.26302130e-03   9.77438844e-01\n",
      "    1.25595416e-05   1.83818530e-19   1.79567616e-12]\n",
      " [  2.60944900e-17   1.49465877e-12   2.14950711e-16   1.27741278e-15\n",
      "    3.57423542e-16   9.99867969e-01   1.46281242e-18]]\n",
      "sum_ep [[  9.27121085e-21   7.07272787e-21   3.24632515e-24   5.29750830e-11\n",
      "    3.78529298e-24   2.26929681e-02   1.12581506e-13]\n",
      " [  1.30266661e-04   3.91983938e-16   1.48773582e-23   1.00398843e-15\n",
      "    3.18233841e-23   2.19067861e-07   9.98594153e-01]\n",
      " [  1.75367487e-06   7.50331759e-21   3.51712530e-27   1.08261124e-16\n",
      "    2.79181169e-27   3.57903743e-11   1.26126759e-03]\n",
      " [  1.20579216e-14   2.58934932e-09   1.95848102e-14   2.53654715e-09\n",
      "    1.54243783e-13   9.77438841e-01   1.88500313e-12]\n",
      " [  1.07573601e-08   8.15240651e-21   1.85186756e-27   1.23285177e-17\n",
      "    1.51307361e-27   1.70641835e-13   1.25487843e-05]\n",
      " [  2.25609371e-02   9.98724636e-01   1.26302130e-03   9.77438844e-01\n",
      "    1.25595416e-05   1.83818530e-19   1.79567616e-12]\n",
      " [  2.60944900e-17   1.49465877e-12   2.14950711e-16   1.27741278e-15\n",
      "    3.57423542e-16   9.99867969e-01   1.46281242e-18]]\n",
      "len 2 (7, 7) 7\n",
      "self.add_matrix_list(epsilon) [[  2.05451497e-16   1.26148323e-01   9.56867093e-14   7.13311726e-01\n",
      "    3.45937343e-01]\n",
      " [  3.45137428e-01   9.48355211e-10   1.05090018e-08   1.80701512e-10\n",
      "    2.43078220e-01]\n",
      " [  2.79461816e-26   1.76624891e-14   2.23164609e-21   1.28759768e-08\n",
      "    3.72129901e-13]\n",
      " [  7.79717820e-14   4.60147517e-01   3.01579691e-32   1.00000000e+00\n",
      "    6.56040442e-02]\n",
      " [  3.68174297e-01   1.91981805e-03   2.36687927e-09   2.86688261e-01\n",
      "    4.38529952e-02]]\n",
      "sum_ep [[  2.05460769e-16   1.26148323e-01   9.56867093e-14   7.13311726e-01\n",
      "    3.45937343e-01   2.26929681e-02   1.12581506e-13]\n",
      " [  3.45267695e-01   9.48355603e-10   1.05090018e-08   1.80702516e-10\n",
      "    2.43078220e-01   2.19067861e-07   9.98594153e-01]\n",
      " [  1.75367487e-06   1.76624966e-14   2.23164961e-21   1.28759769e-08\n",
      "    3.72129901e-13   3.57903743e-11   1.26126759e-03]\n",
      " [  9.00297036e-14   4.60147520e-01   1.95848102e-14   1.00000000e+00\n",
      "    6.56040442e-02   9.77438841e-01   1.88500313e-12]\n",
      " [  3.68174308e-01   1.91981805e-03   2.36687927e-09   2.86688261e-01\n",
      "    4.38529952e-02   1.70641835e-13   1.25487843e-05]\n",
      " [  2.25609371e-02   9.98724636e-01   1.26302130e-03   9.77438844e-01\n",
      "    1.25595416e-05   1.83818530e-19   1.79567616e-12]\n",
      " [  2.60944900e-17   1.49465877e-12   2.14950711e-16   1.27741278e-15\n",
      "    3.57423542e-16   9.99867969e-01   1.46281242e-18]]\n",
      "len 2 (7, 7) 7\n",
      "len (7, 7) (7,)\n",
      "self.non_negative_set [  2.90000000e+01   5.60000000e+01   8.20000000e+01   1.30000000e+01\n",
      "   3.50000000e+01   5.30000000e+01   2.50000000e+01   2.30000000e+01\n",
      "   2.10000000e+01   1.20000000e+01   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   8.70000000e+01   9.00000000e+00   6.30000000e+01\n",
      "   6.20000000e+01   5.20000000e+01   4.30000000e+01   7.70000000e+01\n",
      "   9.50000000e+01   7.90000000e+01   7.70000000e+01   5.00000000e+00\n",
      "   7.80000000e+01   2.60979357e-17   5.64042046e-03   2.90797643e-01\n",
      "   3.87975780e-04   1.75209444e-01   1.66651506e-01   9.48957272e-02\n",
      "   1.91749859e-02   1.39633279e-01   1.51820360e-01   1.24163935e-01\n",
      "   3.65393889e-01   9.31896399e-14   9.70000000e+01   4.60000000e+01\n",
      "   1.20000000e+01   3.20000000e+01   5.60000000e+01   3.00000000e+01\n",
      "   8.60000000e+01   4.50000000e+01   9.90000000e+01   8.90000000e+01\n",
      "   1.00000000e+00   8.80000000e+01   9.40000000e+01   4.80000000e+01\n",
      "   2.60000000e+01   6.50000000e+01   5.30000000e+01   5.40000000e+01\n",
      "   7.60000000e+01   4.80000000e+01   9.80000000e+01   6.30000000e+01\n",
      "   6.70000000e+01   7.40000000e+01   4.00000000e+01]\n",
      "New non_negative_set [  2.90000000e+01   5.60000000e+01   8.20000000e+01   1.30000000e+01\n",
      "   3.50000000e+01   5.30000000e+01   2.50000000e+01   2.30000000e+01\n",
      "   2.10000000e+01   1.20000000e+01   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   8.70000000e+01   9.00000000e+00   6.30000000e+01\n",
      "   6.20000000e+01   5.20000000e+01   4.30000000e+01   7.70000000e+01\n",
      "   9.50000000e+01   7.90000000e+01   7.70000000e+01   5.00000000e+00\n",
      "   7.80000000e+01   2.60979357e-17   5.64042046e-03   2.90797643e-01\n",
      "   3.87975780e-04   1.75209444e-01   1.66651506e-01   9.48957272e-02\n",
      "   1.91749859e-02   1.39633279e-01   1.51820360e-01   1.24163935e-01\n",
      "   3.65393889e-01   9.31896399e-14   9.70000000e+01   4.60000000e+01\n",
      "   1.20000000e+01   3.20000000e+01   5.60000000e+01   3.00000000e+01\n",
      "   8.60000000e+01   4.50000000e+01   9.90000000e+01   8.90000000e+01\n",
      "   1.00000000e+00   8.80000000e+01   9.40000000e+01   4.80000000e+01\n",
      "   2.60000000e+01   6.50000000e+01   5.30000000e+01   5.40000000e+01\n",
      "   7.60000000e+01   4.80000000e+01   9.80000000e+01   6.30000000e+01\n",
      "   6.70000000e+01   7.40000000e+01   4.00000000e+01]\n",
      "******** Epoch  1  ***********\n",
      "\n",
      "+++++++++ The sentence  0\n",
      "xx_source:  6  =>  [29, 712, 4, 2093, 1403, 2]\n",
      "xx_target:  7  =>  [2977, 49, 4, 1156, 4, 2978, 1]\n",
      "one_hot_input [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] (15002, 7)\n",
      "emission_matrix [array([  2.00544377e-23,   1.17533822e-23,   3.17223537e-18,\n",
      "         1.52879123e-21,   3.17228376e-18,   1.00000000e+00,\n",
      "         1.44177945e-17], dtype=float32), array([  9.99892712e-01,   2.00851300e-08,   6.47101513e-13,\n",
      "         1.07259926e-04,   6.47103952e-13,   7.84840150e-30,\n",
      "         1.21499338e-10], dtype=float32), array([  1.00703629e-22,   5.52512912e-19,   5.49631931e-30,\n",
      "         6.05304296e-09,   5.49636144e-30,   1.00000000e+00,\n",
      "         2.45361760e-12], dtype=float32), array([  8.34520017e-24,   9.99884009e-01,   5.79980151e-05,\n",
      "         5.89972407e-13,   5.79985681e-05,   1.12119477e-20,\n",
      "         9.44260480e-19], dtype=float32), array([  4.68536155e-06,   1.75987651e-19,   4.28388284e-30,\n",
      "         6.36714328e-20,   4.28394829e-30,   8.75936066e-23,\n",
      "         9.99995291e-01], dtype=float32), array([  4.12635957e-32,   6.92514155e-21,   1.46328121e-20,\n",
      "         6.00977967e-23,   1.46324777e-20,   1.00000000e+00,\n",
      "         1.50280464e-23], dtype=float32)] (6, 7)\n",
      "norm_alpha [  2.00000000e-02   1.50343572e-04   6.12631933e-05   2.36646125e-05\n",
      "   8.14396904e-06   1.85010767e-06] 6\n",
      "alpha [[  8.82395259e-22   9.96677845e-01   2.61141959e-23   1.61880025e-25\n",
      "    2.13705536e-06   4.11533980e-38]\n",
      " [  1.17533822e-23   1.03218003e-06   3.00056440e-20   9.99966682e-01\n",
      "    4.57048422e-20   2.34386077e-22]\n",
      " [  3.17223537e-18   4.43677038e-14   2.10717490e-30   7.73861503e-08\n",
      "    2.24886805e-31   2.55333697e-20]\n",
      " [  1.52879123e-21   3.32112276e-03   2.51746038e-09   3.55495606e-13\n",
      "    2.43325382e-20   1.40007019e-25]\n",
      " [  3.17228376e-18   1.90578198e-11   1.86601779e-30   3.32408054e-05\n",
      "    1.77999301e-30   1.53838491e-20]\n",
      " [  1.00000000e+00   1.31618759e-28   9.99999997e-01   3.65908710e-21\n",
      "    2.97648580e-23   1.00000000e+00]\n",
      " [  1.44177945e-17   4.11717084e-10   4.06224288e-15   6.22690012e-20\n",
      "    9.99997863e-01   8.55732990e-24]] [ 1.  1.  1.  1.  1.  1.]\n",
      "beta [[  6.53487062e-02   8.19994388e+01   2.73450137e+01   8.56891591e-03\n",
      "    5.01259084e+04   5.40509082e+05]\n",
      " [  9.67454737e-02   2.34905067e+01   1.14053091e+02   3.30374152e+03\n",
      "    1.43596469e+04   5.40509082e+05]\n",
      " [  1.23904498e-01   3.49905364e+01   2.43999176e+02   1.36762620e+03\n",
      "    2.13895662e+04   5.40509082e+05]\n",
      " [  3.23620662e-04   3.75086563e+01   2.98988305e+02   1.94903122e+03\n",
      "    2.29288821e+04   5.40509082e+05]\n",
      " [  2.02125345e-01   4.34356279e+00   5.67966534e-01   1.51165884e+03\n",
      "    2.65520139e+03   5.40509082e+05]\n",
      " [  4.62526919e-03   2.53223474e+01   4.92944181e+02   2.44534213e+02\n",
      "    1.54794433e+04   5.40509082e+05]\n",
      " [  1.60275552e-08   4.56323317e+01   9.83133749e+00   1.24181321e+03\n",
      "    2.78948505e+04   5.40509082e+05]] [  4.93072929e-01   2.53287380e+02   1.18772907e+03   9.61841377e+03\n",
      "   1.54833499e+05   3.78356357e+06]\n",
      "emission_posterior [[  1.24670341e-20   9.98477793e-01   1.44862861e-24   4.19875885e-31\n",
      "    3.84019477e-06   4.11533980e-38]\n",
      " [  2.45842238e-22   2.96224219e-07   6.94244217e-21   9.99984758e-01\n",
      "    2.35277934e-20   2.34386077e-22]\n",
      " [  8.49797526e-17   1.89666346e-14   1.04301655e-30   3.20355691e-08\n",
      "    1.72441255e-31   2.55333697e-20]\n",
      " [  1.06966408e-22   1.52191067e-03   1.52692991e-09   2.09727230e-13\n",
      "    2.00007148e-20   1.40007019e-25]\n",
      " [  1.38629543e-16   1.01132804e-12   2.15001150e-33   1.52099265e-05\n",
      "    1.69430264e-31   1.53838491e-20]\n",
      " [  1.00000000e+00   4.07187541e-29   9.99999998e-01   2.70840850e-22\n",
      "    1.65171219e-23   1.00000000e+00]\n",
      " [  4.99607671e-23   2.29532547e-10   8.10178562e-17   2.34061490e-20\n",
      "    9.99996160e-01   8.55732990e-24]] (7, 6)\n",
      "\n",
      "+++++++++ The sentence  1\n",
      "xx_source:  5  =>  [29, 1009, 99, 2094, 2]\n",
      "xx_target:  5  =>  [5158, 49, 26, 2979, 1]\n",
      "one_hot_input [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] (15002, 5)\n",
      "emission_matrix [array([  9.95527551e-39,   9.25446605e-28,   1.60047068e-32,\n",
      "         1.00000000e+00,   1.13538203e-21], dtype=float32), array([  1.61038814e-29,   1.00000000e+00,   0.00000000e+00,\n",
      "         2.54463550e-36,   2.01566128e-29], dtype=float32), array([  4.45027050e-04,   2.23544924e-12,   1.13570160e-10,\n",
      "         7.51306072e-26,   9.99554992e-01], dtype=float32), array([  9.98522271e-21,   4.51638495e-42,   1.45484516e-14,\n",
      "         1.00000000e+00,   1.99459378e-24], dtype=float32), array([  1.80650042e-17,   2.39232246e-23,   3.12489558e-43,\n",
      "         1.00000000e+00,   5.19071647e-26], dtype=float32)] (5, 5)\n",
      "norm_alpha [  2.00000000e-02   7.67924189e-03   2.03768817e-03   4.66555663e-04\n",
      "   9.70243993e-05] 5\n",
      "alpha [[  4.57942673e-37   3.56596984e-32   4.88479953e-04   1.74168224e-20\n",
      "    7.38577415e-20]\n",
      " [  9.25446605e-28   1.00000000e+00   1.39721283e-12   1.08568230e-44\n",
      "    4.41703227e-23]\n",
      " [  1.60047068e-32   0.00000000e+00   1.43433430e-11   1.52943771e-14\n",
      "    5.48779769e-43]\n",
      " [  1.00000000e+00   1.37820788e-36   6.90966401e-26   1.00000000e+00\n",
      "    1.00000000e+00]\n",
      " [  1.13538203e-21   2.20594710e-30   9.99511520e-01   1.13607686e-24\n",
      "    1.04885560e-26]] [ 1.  1.  1.  1.  1.]\n",
      "beta [[  2.28690067e-02   1.45324289e+00   2.92515862e+01   6.14336307e+02\n",
      "    1.03066858e+04]\n",
      " [  1.04773089e-01   1.64529685e+00   2.49057246e+01   5.23065339e+02\n",
      "    1.03066858e+04]\n",
      " [  1.76771699e-01   1.45392363e+00   3.28583462e+00   6.90084802e+01\n",
      "    1.03066858e+04]\n",
      " [  2.42560998e-01   2.60404761e-01   2.12235130e+01   4.45732225e+02\n",
      "    1.03066858e+04]\n",
      " [  3.36697801e-04   8.09224519e-01   2.33642617e+01   4.90691825e+02\n",
      "    1.03066858e+04]] [  5.47311491e-01   5.62209266e+00   1.02030920e+02   2.14283418e+03\n",
      "   5.15334290e+04]\n",
      "emission_posterior [[  4.31755069e-38   3.14971752e-32   6.11491820e-04   2.40049648e-20\n",
      "    7.38577415e-20]\n",
      " [  3.99742333e-28   1.00000000e+00   1.48921099e-12   1.27404471e-44\n",
      "    4.41703227e-23]\n",
      " [  1.16637845e-32   0.00000000e+00   2.01692882e-12   2.36788291e-15\n",
      "    5.48779769e-43]\n",
      " [  1.00000000e+00   2.18132000e-37   6.27579411e-26   1.00000000e+00\n",
      "    1.00000000e+00]\n",
      " [  1.57601855e-24   1.08497532e-30   9.99388508e-01   1.25066934e-24\n",
      "    1.04885560e-26]] (5, 5)\n",
      "self.add_matrix_list(epsilon) [[  1.24675040e-20   6.69973932e-21   2.17280459e-26   1.52313137e-09\n",
      "    1.93432474e-26   9.98481632e-01   1.24613904e-26]\n",
      " [  3.83992770e-06   3.04699283e-20   3.36827233e-26   1.22307008e-15\n",
      "    2.94775041e-25   2.96224218e-07   9.99980918e-01]\n",
      " [  3.80688410e-13   4.69019159e-25   1.27104667e-29   4.65142245e-22\n",
      "    2.32169287e-30   1.89666346e-14   3.20351885e-08]\n",
      " [  4.61815153e-21   1.52689060e-09   3.48726145e-14   3.79731910e-12\n",
      "    4.43507368e-15   1.52191066e-03   2.09808201e-13]\n",
      " [  2.66688964e-10   4.89288741e-27   2.30659078e-29   3.61689848e-20\n",
      "    1.55081592e-30   1.01132800e-12   1.52096599e-05]\n",
      " [  9.98477793e-01   9.99985053e-01   3.20355532e-08   1.52191067e-03\n",
      "    1.52099275e-05   2.87358052e-22   2.29532547e-10]\n",
      " [  6.83499240e-32   8.08513436e-17   1.25623245e-19   2.01186699e-21\n",
      "    8.20410800e-20   9.99996160e-01   2.34211191e-20]]\n",
      "sum_ep [[  1.24675040e-20   6.69973932e-21   2.17280459e-26   1.52313137e-09\n",
      "    1.93432474e-26   9.98481632e-01   1.24613904e-26]\n",
      " [  3.83992770e-06   3.04699283e-20   3.36827233e-26   1.22307008e-15\n",
      "    2.94775041e-25   2.96224218e-07   9.99980918e-01]\n",
      " [  3.80688410e-13   4.69019159e-25   1.27104667e-29   4.65142245e-22\n",
      "    2.32169287e-30   1.89666346e-14   3.20351885e-08]\n",
      " [  4.61815153e-21   1.52689060e-09   3.48726145e-14   3.79731910e-12\n",
      "    4.43507368e-15   1.52191066e-03   2.09808201e-13]\n",
      " [  2.66688964e-10   4.89288741e-27   2.30659078e-29   3.61689848e-20\n",
      "    1.55081592e-30   1.01132800e-12   1.52096599e-05]\n",
      " [  9.98477793e-01   9.99985053e-01   3.20355532e-08   1.52191067e-03\n",
      "    1.52099275e-05   2.87358052e-22   2.29532547e-10]\n",
      " [  6.83499240e-32   8.08513436e-17   1.25623245e-19   2.01186699e-21\n",
      "    8.20410800e-20   9.99996160e-01   2.34211191e-20]]\n",
      "len 2 (7, 7) 7\n",
      "self.add_matrix_list(epsilon) [[  5.26013872e-24   4.31755915e-38   1.26676126e-18   6.11491820e-04\n",
      "    1.09814024e-27]\n",
      " [  6.11491820e-04   1.48921099e-12   2.01692882e-12   1.48921099e-12\n",
      "    9.99388508e-01]\n",
      " [  6.44490922e-31   1.16642768e-32   2.24826438e-26   2.01929671e-12\n",
      "    3.22511911e-35]\n",
      " [  7.38577415e-20   1.00000000e+00   2.48791503e-40   1.00000000e+00\n",
      "    1.04896410e-26]\n",
      " [  2.39997046e-20   1.57601855e-24   2.36661615e-15   9.99388508e-01\n",
      "    1.24957231e-24]]\n",
      "sum_ep [[  1.24727642e-20   6.69973932e-21   1.26676128e-18   6.11493343e-04\n",
      "    2.04413876e-26   9.98481632e-01   1.24613904e-26]\n",
      " [  6.15331748e-04   1.48921102e-12   2.01692882e-12   1.49043406e-12\n",
      "    9.99388508e-01   2.96224218e-07   9.99980918e-01]\n",
      " [  3.80688410e-13   4.69019171e-25   2.24953543e-26   2.01929671e-12\n",
      "    2.32172512e-30   1.89666346e-14   3.20351885e-08]\n",
      " [  7.84758931e-20   1.00000000e+00   3.48726145e-14   1.00000000e+00\n",
      "    4.43507368e-15   1.52191066e-03   2.09808201e-13]\n",
      " [  2.66688964e-10   1.58091144e-24   2.36661615e-15   9.99388508e-01\n",
      "    1.24957386e-24   1.01132800e-12   1.52096599e-05]\n",
      " [  9.98477793e-01   9.99985053e-01   3.20355532e-08   1.52191067e-03\n",
      "    1.52099275e-05   2.87358052e-22   2.29532547e-10]\n",
      " [  6.83499240e-32   8.08513436e-17   1.25623245e-19   2.01186699e-21\n",
      "    8.20410800e-20   9.99996160e-01   2.34211191e-20]]\n",
      "len 2 (7, 7) 7\n",
      "len (7, 7) (7,)\n",
      "self.non_negative_set [  2.90000000e+01   5.60000000e+01   8.20000000e+01   1.30000000e+01\n",
      "   3.50000000e+01   5.30000000e+01   2.50000000e+01   2.30000000e+01\n",
      "   2.10000000e+01   1.20000000e+01   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   8.70000000e+01   9.00000000e+00   6.30000000e+01\n",
      "   6.20000000e+01   5.20000000e+01   4.30000000e+01   7.70000000e+01\n",
      "   9.50000000e+01   7.90000000e+01   7.70000000e+01   5.00000000e+00\n",
      "   7.80000000e+01   6.83501865e-32   2.49619688e-01   2.00021018e-01\n",
      "   4.00398279e-09   1.25175815e-01   1.99983385e-01   9.09090909e-02\n",
      "   2.34584445e-11   2.19588497e-04   1.66649998e-01   1.09453433e-07\n",
      "   6.66358938e-01   1.24727016e-26   9.70000000e+01   4.60000000e+01\n",
      "   1.20000000e+01   3.20000000e+01   5.60000000e+01   3.00000000e+01\n",
      "   8.60000000e+01   4.50000000e+01   9.90000000e+01   8.90000000e+01\n",
      "   1.00000000e+00   8.80000000e+01   9.40000000e+01   4.80000000e+01\n",
      "   2.60000000e+01   6.50000000e+01   5.30000000e+01   5.40000000e+01\n",
      "   7.60000000e+01   4.80000000e+01   9.80000000e+01   6.30000000e+01\n",
      "   6.70000000e+01   7.40000000e+01   4.00000000e+01]\n",
      "New non_negative_set [  2.90000000e+01   5.60000000e+01   8.20000000e+01   1.30000000e+01\n",
      "   3.50000000e+01   5.30000000e+01   2.50000000e+01   2.30000000e+01\n",
      "   2.10000000e+01   1.20000000e+01   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   8.70000000e+01   9.00000000e+00   6.30000000e+01\n",
      "   6.20000000e+01   5.20000000e+01   4.30000000e+01   7.70000000e+01\n",
      "   9.50000000e+01   7.90000000e+01   7.70000000e+01   5.00000000e+00\n",
      "   7.80000000e+01   6.83501865e-32   2.49619688e-01   2.00021018e-01\n",
      "   4.00398279e-09   1.25175815e-01   1.99983385e-01   9.09090909e-02\n",
      "   2.34584445e-11   2.19588497e-04   1.66649998e-01   1.09453433e-07\n",
      "   6.66358938e-01   1.24727016e-26   9.70000000e+01   4.60000000e+01\n",
      "   1.20000000e+01   3.20000000e+01   5.60000000e+01   3.00000000e+01\n",
      "   8.60000000e+01   4.50000000e+01   9.90000000e+01   8.90000000e+01\n",
      "   1.00000000e+00   8.80000000e+01   9.40000000e+01   4.80000000e+01\n",
      "   2.60000000e+01   6.50000000e+01   5.30000000e+01   5.40000000e+01\n",
      "   7.60000000e+01   4.80000000e+01   9.80000000e+01   6.30000000e+01\n",
      "   6.70000000e+01   7.40000000e+01   4.00000000e+01]\n",
      "******** Epoch  2  ***********\n",
      "\n",
      "+++++++++ The sentence  0\n",
      "xx_source:  6  =>  [29, 712, 4, 2093, 1403, 2]\n",
      "xx_target:  7  =>  [2977, 49, 4, 1156, 4, 2978, 1]\n",
      "one_hot_input [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] (15002, 7)\n",
      "emission_matrix [array([  1.97877065e-23,   1.17512752e-23,   3.17200542e-18,\n",
      "         1.53284393e-21,   3.17199322e-18,   1.00000000e+00,\n",
      "         1.44193347e-17], dtype=float32), array([  9.99919236e-01,   1.72282153e-08,   5.57925613e-13,\n",
      "         8.07699835e-05,   5.57927727e-13,   6.79048818e-30,\n",
      "         1.05227931e-10], dtype=float32), array([  1.00310259e-22,   5.52666768e-19,   5.49724203e-30,\n",
      "         6.12484996e-09,   5.49736766e-30,   1.00000000e+00,\n",
      "         2.45563097e-12], dtype=float32), array([  8.26868697e-24,   9.99881864e-01,   5.90587151e-05,\n",
      "         5.98704300e-13,   5.90596173e-05,   1.13158631e-20,\n",
      "         9.51511959e-19], dtype=float32), array([  4.59943794e-06,   1.76143484e-19,   4.29045771e-30,\n",
      "         6.38226970e-20,   4.29042498e-30,   8.77039525e-23,\n",
      "         9.99995410e-01], dtype=float32), array([  4.15957962e-32,   6.92540570e-21,   1.46248877e-20,\n",
      "         5.92419520e-23,   1.46247762e-20,   1.00000000e+00,\n",
      "         1.50128624e-23], dtype=float32)] (6, 7)\n",
      "norm_alpha [ 0.02        0.00576659  0.00415789  0.00096062  0.0005694   0.00013154] 6\n",
      "alpha [[  8.70659085e-22   9.99959481e-01   1.36849886e-23   1.03196606e-23\n",
      "    1.38050701e-06   2.44531842e-38]\n",
      " [  1.17512752e-23   1.38055772e-08   6.66766295e-24   9.99940948e-01\n",
      "    2.40307347e-20   8.64426892e-21]\n",
      " [  3.17200542e-18   8.94967255e-21   1.91747566e-33   1.31662983e-12\n",
      "    8.68146172e-35   1.46275768e-20]\n",
      " [  1.53284393e-21   4.05050401e-05   1.53182003e-09   3.74699260e-13\n",
      "    2.30950500e-23   6.50291196e-29]\n",
      " [  3.17199322e-18   4.47003068e-13   9.18580367e-37   5.90519947e-05\n",
      "    1.07306072e-30   9.15406244e-21]\n",
      " [  1.00000000e+00   2.47313006e-30   9.99999998e-01   5.14333755e-21\n",
      "    1.44059995e-29   1.00000000e+00]\n",
      " [  1.44193347e-17   9.88937644e-21   3.94646346e-17   1.91514488e-27\n",
      "    9.99998619e-01   6.82456590e-24]] [ 1.  1.  1.  1.  1.  1.]\n",
      "beta [[  1.12188023e-01   3.95568317e+00   8.40305997e-10   3.39621761e-04\n",
      "    1.26634498e+03   7.60251438e+03]\n",
      " [  2.02888139e-01   5.34169784e-07   2.66309436e+00   1.42549463e+02\n",
      "    1.71000267e-04   7.60251438e+03]\n",
      " [  2.44891616e-01   1.56831367e+00   1.12970194e+01   7.86501301e-04\n",
      "    5.02069062e+02   7.60251438e+03]\n",
      " [  1.53496057e-08   2.06651377e-03   7.07115676e+00   6.87466637e+01\n",
      "    6.61557826e-01   7.60251438e+03]\n",
      " [  3.70128952e-01   1.03417525e-08   3.90822626e-07   8.68005299e-02\n",
      "    6.68469005e-08   7.60251438e+03]\n",
      " [  3.28838576e-01   5.76083394e-01   7.60844720e+00   9.95484815e-04\n",
      "    1.84423341e+02   7.60251438e+03]\n",
      " [  4.09692060e-12   1.26727817e+00   9.49509307e+00   2.52524764e+01\n",
      "    4.05697645e+02   7.60251438e+03]] [  1.25893532e+00   7.36942546e+00   3.81348112e+01   2.36637526e+02\n",
      "   2.35919675e+03   5.32176007e+04]\n",
      "emission_posterior [[  2.97037905e-22   9.99999979e-01   1.51142246e-33   2.45878735e-29\n",
      "    4.30910315e-06   2.44531842e-38]\n",
      " [  7.25034871e-24   1.86436085e-15   2.33380283e-24   9.99999964e-01\n",
      "    1.01288486e-26   8.64426892e-21]\n",
      " [  2.36224577e-18   3.54842934e-21   2.84706710e-33   7.26479191e-18\n",
      "    1.07436675e-34   1.46275768e-20]\n",
      " [  7.15504558e-29   2.11613543e-08   1.42364655e-09   1.80715127e-13\n",
      "    3.76602293e-26   6.50291196e-29]\n",
      " [  3.57028224e-18   1.16869378e-21   4.71846596e-44   3.59597773e-08\n",
      "    1.76807960e-40   9.15406244e-21]\n",
      " [  1.00000000e+00   3.60187304e-31   9.99999999e-01   3.59202799e-26\n",
      "    6.54870643e-30   1.00000000e+00]\n",
      " [  1.79647017e-28   3.16837772e-21   4.92505723e-17   3.39285775e-28\n",
      "    9.99995691e-01   6.82456590e-24]] (7, 6)\n",
      "\n",
      "+++++++++ The sentence  1\n",
      "xx_source:  5  =>  [29, 1009, 99, 2094, 2]\n",
      "xx_target:  5  =>  [5158, 49, 26, 2979, 1]\n",
      "one_hot_input [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] (15002, 5)\n",
      "emission_matrix [array([  9.95694586e-39,   9.24804404e-28,   1.60053166e-32,\n",
      "         1.00000000e+00,   1.13553793e-21], dtype=float32), array([  1.60949153e-29,   1.00000000e+00,   0.00000000e+00,\n",
      "         2.54479084e-36,   2.01629187e-29], dtype=float32), array([  4.29402047e-04,   2.19977292e-12,   1.11392888e-10,\n",
      "         7.36794421e-26,   9.99570608e-01], dtype=float32), array([  9.97087307e-21,   4.51638495e-42,   1.45486447e-14,\n",
      "         1.00000000e+00,   1.99826455e-24], dtype=float32), array([  1.80671400e-17,   2.39538150e-23,   3.12489558e-43,\n",
      "         1.00000000e+00,   5.18604555e-26], dtype=float32)] (5, 5)\n",
      "norm_alpha [ 0.02        0.00601708  0.00219073  0.00071148  0.00015545] 5\n",
      "alpha [[  4.58019509e-37   5.14825997e-37   5.15246924e-04   9.96809954e-21\n",
      "    7.95745919e-25]\n",
      " [  9.24804404e-28   1.00000000e+00   1.19989308e-12   9.03363710e-50\n",
      "    3.29828216e-23]\n",
      " [  1.60053166e-32   0.00000000e+00   1.56788446e-20   9.09712840e-15\n",
      "    6.87419914e-43]\n",
      " [  1.00000000e+00   1.84815750e-36   9.70763183e-29   1.00000000e+00\n",
      "    1.00000000e+00]\n",
      " [  1.13553793e-21   3.77861097e-39   9.99484753e-01   9.07445848e-25\n",
      "    1.33822229e-35]] [ 1.  1.  1.  1.  1.]\n",
      "beta [[  1.17554600e-10   3.56000000e-03   6.44780965e+01   9.08651689e+02\n",
      "    6.43274905e+03]\n",
      " [  2.56540012e-01   4.29370091e+00   4.78436008e-02   6.74231578e-01\n",
      "    6.43274905e+03]\n",
      " [  6.20565665e-01   9.24607130e-03   5.62030022e-09   7.92035754e-08\n",
      "    6.43274905e+03]\n",
      " [  3.88636333e-01   7.61305488e-10   2.17919835e+01   3.07101538e+02\n",
      "    6.43274905e+03]\n",
      " [  8.39529809e-09   1.74181097e+00   3.23745924e+01   4.56235989e+02\n",
      "    6.43274905e+03]] [  1.26574202e+00   6.04831795e+00   1.18692516e+02   1.67266345e+03\n",
      "   3.21637452e+04]\n",
      "emission_posterior [[  1.38541603e-46   4.26853334e-40   1.02565539e-03   2.94936018e-20\n",
      "    7.95745919e-25]\n",
      " [  6.10466169e-28   1.00000000e+00   1.77231233e-15   1.98330605e-52\n",
      "    3.29828216e-23]\n",
      " [  2.55569258e-32   0.00000000e+00   2.72049182e-30   2.34621129e-24\n",
      "    6.87419914e-43]\n",
      " [  1.00000000e+00   3.27692235e-46   6.53106356e-29   1.00000000e+00\n",
      "    1.00000000e+00]\n",
      " [  2.45298204e-29   1.53285620e-39   9.98974345e-01   1.34811911e-24\n",
      "    1.33822229e-35]] (5, 5)\n",
      "self.add_matrix_list(epsilon) [[  2.97037907e-22   6.81146907e-30   2.07700304e-29   1.42359668e-09\n",
      "    1.03516091e-32   1.00000429e+00   5.73612481e-38]\n",
      " [  4.30863896e-06   2.34423106e-24   1.87116413e-40   2.12905901e-20\n",
      "    1.53872447e-31   1.86430477e-15   9.99995655e-01]\n",
      " [  9.20997689e-18   1.16278773e-32   3.79520090e-31   1.86840849e-35\n",
      "    6.15748779e-42   3.54842934e-21   4.17060784e-19]\n",
      " [  6.23723371e-26   1.42364655e-09   7.41216071e-19   4.98689304e-14\n",
      "    9.59576689e-27   2.11613043e-08   1.80764343e-13]\n",
      " [  4.64190991e-10   4.89983575e-38   4.94384128e-31   1.14525008e-21\n",
      "    2.36714931e-39   2.35970294e-23   3.54955863e-08]\n",
      " [  9.99999979e-01   9.99999963e-01   6.52712425e-18   2.11615350e-08\n",
      "    3.59597773e-08   3.59275490e-26   3.16837756e-21]\n",
      " [  3.94826306e-48   4.92592156e-17   1.46275896e-20   1.19587304e-30\n",
      "    9.15495073e-21   9.99995691e-01   6.82508224e-24]]\n",
      "sum_ep [[  2.97037907e-22   6.81146907e-30   2.07700304e-29   1.42359668e-09\n",
      "    1.03516091e-32   1.00000429e+00   5.73612481e-38]\n",
      " [  4.30863896e-06   2.34423106e-24   1.87116413e-40   2.12905901e-20\n",
      "    1.53872447e-31   1.86430477e-15   9.99995655e-01]\n",
      " [  9.20997689e-18   1.16278773e-32   3.79520090e-31   1.86840849e-35\n",
      "    6.15748779e-42   3.54842934e-21   4.17060784e-19]\n",
      " [  6.23723371e-26   1.42364655e-09   7.41216071e-19   4.98689304e-14\n",
      "    9.59576689e-27   2.11613043e-08   1.80764343e-13]\n",
      " [  4.64190991e-10   4.89983575e-38   4.94384128e-31   1.14525008e-21\n",
      "    2.36714931e-39   2.35970294e-23   3.54955863e-08]\n",
      " [  9.99999979e-01   9.99999963e-01   6.52712425e-18   2.11615350e-08\n",
      "    3.59597773e-08   3.59275490e-26   3.16837756e-21]\n",
      " [  3.94826306e-48   4.92592156e-17   1.46275896e-20   1.19587304e-30\n",
      "    9.15495073e-21   9.99995691e-01   6.82508224e-24]]\n",
      "len 2 (7, 7) 7\n",
      "self.add_matrix_list(epsilon) [[  1.65063900e-23   1.38541702e-46   5.07097036e-30   1.02565539e-03\n",
      "    1.99979774e-33]\n",
      " [  1.02565539e-03   1.77231233e-15   2.72049182e-30   1.77231233e-15\n",
      "    9.98974345e-01]\n",
      " [  2.26192242e-31   2.55574049e-32   3.95585951e-44   2.34621378e-24\n",
      "    1.13904952e-42]\n",
      " [  7.95745693e-25   1.00000000e+00   6.87419915e-43   1.00000000e+00\n",
      "    1.33837546e-35]\n",
      " [  2.94770954e-20   2.45298204e-29   2.34620622e-24   9.98974345e-01\n",
      "    1.34811911e-24]]\n",
      "sum_ep [[  3.13544297e-22   6.81146907e-30   2.58410008e-29   1.02565682e-03\n",
      "    1.23514069e-32   1.00000429e+00   5.73612481e-38]\n",
      " [  1.02996403e-03   1.77231233e-15   2.72049182e-30   1.77233362e-15\n",
      "    9.98974345e-01   1.86430477e-15   9.99995655e-01]\n",
      " [  9.20997689e-18   3.71852822e-32   3.79520090e-31   2.34621378e-24\n",
      "    7.29653731e-42   3.54842934e-21   4.17060784e-19]\n",
      " [  8.58118030e-25   1.00000000e+00   7.41216071e-19   1.00000000e+00\n",
      "    9.59576690e-27   2.11613043e-08   1.80764343e-13]\n",
      " [  4.64190991e-10   2.45298204e-29   2.34620671e-24   9.98974345e-01\n",
      "    1.34811911e-24   2.35970294e-23   3.54955863e-08]\n",
      " [  9.99999979e-01   9.99999963e-01   6.52712425e-18   2.11615350e-08\n",
      "    3.59597773e-08   3.59275490e-26   3.16837756e-21]\n",
      " [  3.94826306e-48   4.92592156e-17   1.46275896e-20   1.19587304e-30\n",
      "    9.15495073e-21   9.99995691e-01   6.82508224e-24]]\n",
      "len 2 (7, 7) 7\n",
      "len (7, 7) (7,)\n",
      "self.non_negative_set [  2.90000000e+01   5.60000000e+01   8.20000000e+01   1.30000000e+01\n",
      "   3.50000000e+01   5.30000000e+01   2.50000000e+01   2.30000000e+01\n",
      "   2.10000000e+01   1.20000000e+01   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   8.70000000e+01   9.00000000e+00   6.30000000e+01\n",
      "   6.20000000e+01   5.20000000e+01   4.30000000e+01   7.70000000e+01\n",
      "   9.50000000e+01   7.90000000e+01   7.70000000e+01   5.00000000e+00\n",
      "   7.80000000e+01   3.94828008e-48   2.50000264e-01   2.00041198e-01\n",
      "   8.15995689e-19   1.25016097e-01   2.00020605e-01   9.09090909e-02\n",
      "   3.19432903e-22   8.09383679e-09   1.66638063e-01   6.21360630e-16\n",
      "   6.66437858e-01   5.73022301e-38   9.70000000e+01   4.60000000e+01\n",
      "   1.20000000e+01   3.20000000e+01   5.60000000e+01   3.00000000e+01\n",
      "   8.60000000e+01   4.50000000e+01   9.90000000e+01   8.90000000e+01\n",
      "   1.00000000e+00   8.80000000e+01   9.40000000e+01   4.80000000e+01\n",
      "   2.60000000e+01   6.50000000e+01   5.30000000e+01   5.40000000e+01\n",
      "   7.60000000e+01   4.80000000e+01   9.80000000e+01   6.30000000e+01\n",
      "   6.70000000e+01   7.40000000e+01   4.00000000e+01]\n",
      "New non_negative_set [  2.90000000e+01   5.60000000e+01   8.20000000e+01   1.30000000e+01\n",
      "   3.50000000e+01   5.30000000e+01   2.50000000e+01   2.30000000e+01\n",
      "   2.10000000e+01   1.20000000e+01   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   8.70000000e+01   9.00000000e+00   6.30000000e+01\n",
      "   6.20000000e+01   5.20000000e+01   4.30000000e+01   7.70000000e+01\n",
      "   9.50000000e+01   7.90000000e+01   7.70000000e+01   5.00000000e+00\n",
      "   7.80000000e+01   3.94828008e-48   2.50000264e-01   2.00041198e-01\n",
      "   8.15995689e-19   1.25016097e-01   2.00020605e-01   9.09090909e-02\n",
      "   3.19432903e-22   8.09383679e-09   1.66638063e-01   6.21360630e-16\n",
      "   6.66437858e-01   5.73022301e-38   9.70000000e+01   4.60000000e+01\n",
      "   1.20000000e+01   3.20000000e+01   5.60000000e+01   3.00000000e+01\n",
      "   8.60000000e+01   4.50000000e+01   9.90000000e+01   8.90000000e+01\n",
      "   1.00000000e+00   8.80000000e+01   9.40000000e+01   4.80000000e+01\n",
      "   2.60000000e+01   6.50000000e+01   5.30000000e+01   5.40000000e+01\n",
      "   7.60000000e+01   4.80000000e+01   9.80000000e+01   6.30000000e+01\n",
      "   6.70000000e+01   7.40000000e+01   4.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "# BW model variables\n",
    "max_distance = 30\n",
    "baum_welch_model = BaumWelchModel(max_distance, seed=1111)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)\n",
    "\n",
    "# Emission model variables\n",
    "vocab_input_size = len(cz_target_indices)\n",
    "d_embedding = 128\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = len(en_source_indices)\n",
    "emission_model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                               vocab_output_size=vocab_output_size, baum_welch_model=baum_welch_model)\n",
    "emission_model.epoch = 3\n",
    "\n",
    "trans_posteriors = emission_model.train_model_epoch(target_inputs=np.array(cz_sequences), source_inputs=np.array(en_sequences), input_indice_shift=-1)\n",
    "# print(\"trans_posteriors\", np.shape(trans_posteriors), trans_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7, 7)\n",
      "(4, 5, 5)\n",
      "(10, 7, 7)\n",
      "(12, 4, 4)\n",
      "(8, 6, 6)\n",
      "(12, 8, 8)\n",
      "(6, 6, 6)\n",
      "(6, 5, 5)\n",
      "(4, 3, 3)\n",
      "(32, 22, 22)\n"
     ]
    }
   ],
   "source": [
    "for t in trans_posteriors:\n",
    "    print(np.shape(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_file(strs, file_name):\n",
    "    alignment_test = open(file_name,\"w\", encoding='utf8') \n",
    "    alignment_test.write(str(np.shape(strs)[0]) + \" \" + str(np.shape(strs)[1]) + \"\\n\")\n",
    "    for s in strs:\n",
    "        for ss in s:\n",
    "            alignment_test.write(str(ss) + \" \")\n",
    "        alignment_test.write(\"\\n\")\n",
    "    alignment_test.close()\n",
    "    \n",
    "write_file(trans_posteriors[0], \"result0.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
