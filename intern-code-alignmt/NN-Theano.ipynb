{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import theano.tensor as T\n",
    "from theano import function, printing\n",
    "import theano\n",
    "\n",
    "from theano import config\n",
    "config.gcc.cxxflags = \"-D_hypot=hypot\"\n",
    "config.compute_test_value = 'raise'\n",
    "\n",
    "# updates = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evolution:\n",
    "\n",
    "    def calculate_AER(self, S, P, A):\n",
    "        s_a, p_a, len_s, len_a = 0, 0, 0, 0\n",
    "        for s, p, a in zip(S, P, A):\n",
    "            s_a += len(list(set(s).intersection(a)))\n",
    "            p_a += len(list(set(p).intersection(a)))\n",
    "            len_s += len(s)\n",
    "            len_a += len(a)\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a += s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len_s + len_a)\n",
    "        print (\"aer\", 1.-aer)\n",
    "\n",
    "        return 1. - aer \n",
    "\n",
    "\n",
    "    def calculate_one_AER(self, S, P, A):\n",
    "        s_a = len(list(set(S).intersection(A)))\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a = len(list(set(P).intersection(A))) + s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len(S) + len(A))\n",
    "        print (\"aer\", 1.-aer)\n",
    "\n",
    "        return 1. - aer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9)\n",
    "np.split(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 512)\n",
      "(9, 512)\n",
      "(9, 5)\n"
     ]
    }
   ],
   "source": [
    "class EmissionModel:\n",
    "    \"\"\" Simple emission model without CNN\n",
    "    word embedding layer -> ReLU layer -> softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def init_weights_bias(self, input_size, layer_size, output_size, seed=1402):\n",
    "        random_state = np.random.RandomState(seed)\n",
    "        \n",
    "        size_list = np.concatenate(([input_size[0]], layer_size, [output_size]), axis=0)\n",
    "        w = []\n",
    "        b = []\n",
    "        \n",
    "        for i in range(len(size_list) - 1):\n",
    "            w.append(\n",
    "                theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], size_list[i])), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True\n",
    "                )\n",
    "            )\n",
    "            b.append(\n",
    "                theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], 1)), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return w, b\n",
    "    \n",
    "    #[7,512]\n",
    "    def __init__(self, input_size, layer_size, output_size, epoch=1, batch=1, learning_rate = .01, seed=1412):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch = batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.posteriors = []\n",
    "        \n",
    "        x_training_input = T.matrix().astype(config.floatX)\n",
    "        x_training_input.tag.test_value = np.asarray([\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  1.,  0.],\n",
    "            [ 0.,  0.,  1.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  1.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 1.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  1.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.]\n",
    "        ]).astype(x_training_input.dtype)\n",
    "        \n",
    "        self.w, self.b = self.init_weights_bias(input_size, layer_size, output_size, seed)\n",
    "        \n",
    "        # word embedding layer\n",
    "        word_embedding_layer = T.dot(self.w[0], x_training_input) # [7, 10] * [10, 5] = [7, 5]\n",
    "        \n",
    "        # ReLU layer\n",
    "        z_relu_layer = T.dot(self.w[1], word_embedding_layer) + self.b[1] # [512, 7] * [7, 5] = [512, 5]\n",
    "        z_relu_layer_shape = T.shape(z_relu_layer)\n",
    "        z_reshaped_relu_layer = T.reshape(z_relu_layer, [z_relu_layer_shape[0]*z_relu_layer_shape[1], 1])\n",
    "        relu_layer = T.nnet.relu(z_reshaped_relu_layer)\n",
    "        relu_layer_reshaped = T.reshape(relu_layer, z_relu_layer_shape) # [512, 5]\n",
    "        \n",
    "        # Softmax layer\n",
    "        z_softmax_layer = T.dot(self.w[2], relu_layer_reshaped) + self.b[2] # [9, 512] * [512, 5] = [9, 5]\n",
    "        softmax_layer = T.transpose(T.nnet.softmax(T.transpose(z_softmax_layer))) # [9, 5]\n",
    "        \n",
    "        # calculate new gradient\n",
    "        posteriors = T.matrix().astype(config.floatX)\n",
    "        posteriors.tag.test_value = np.asarray([\n",
    "            [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62],\n",
    "            [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55],\n",
    "            [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05],\n",
    "            [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51],\n",
    "            [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11]\n",
    "        ]).astype(posteriors.dtype)\n",
    "        \n",
    "        cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer))\n",
    "        # TODO: use dw[] and db[] abstractly \n",
    "        dw0,dw1,dw2,db1,db2 = T.grad(\n",
    "            cost=cost, wrt=[self.w[0],self.w[1],self.w[2],self.b[1],self.b[2]]\n",
    "        )\n",
    "\n",
    "        # Update w and b\n",
    "        updates = [\n",
    "            (self.w[0], self.w[0] - self.learning_rate * dw0), \n",
    "            (self.w[1], self.w[1] - self.learning_rate * dw1), \n",
    "            (self.b[1], self.b[1] - self.learning_rate * db1),\n",
    "            (self.w[2], self.w[2] - self.learning_rate * dw2), \n",
    "            (self.b[2], self.b[2] - self.learning_rate * db2)\n",
    "        ]\n",
    "        \n",
    "        # Compile model\n",
    "        self.test = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw1, softmax_layer]\n",
    "        ) \n",
    "        self.train_mini_batch = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw2, self.w[2], softmax_layer], \n",
    "            updates=updates\n",
    "        )\n",
    "        self.test_values = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=[softmax_layer]\n",
    "        )\n",
    "        \n",
    "    def train_model(inputs):\n",
    "        for i in range(self.epoch):\n",
    "            for x_input in np.split(inputs, self.batch):\n",
    "                self.posteriors = \n",
    "                self.train_mini_batch(x_input, posteriors)\n",
    "            # TODO: create train_batch function \n",
    "    \n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "input_size = np.shape(x)\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "output_size = 9\n",
    "\n",
    "model = EmissionModel(input_size=input_size, layer_size=layer_size, output_size=output_size)\n",
    "\n",
    "result = model.train_mini_batch(x, posteriors)\n",
    "print(np.shape(result[0]))\n",
    "print(np.shape(result[1]))\n",
    "print(np.shape(result[2]))\n",
    "# print(np.shape(model.evaluate_model(x)))\n",
    "# print(model.calculate_gradient(posteriors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.25463021e-01  -1.74731529e+00  -3.40455741e-01 ...,   8.40714455e-01\n",
      "    1.16684544e+00  -1.74363041e+00]\n",
      " [ -1.73396075e+00  -3.52505970e+00  -1.80466461e+00 ...,   1.56047273e+00\n",
      "    2.06868815e+00  -2.42936778e+00]\n",
      " [ -5.50276101e-01   5.80202416e-02  -1.51815784e+00 ...,   9.53709185e-01\n",
      "    2.68967301e-01   3.69657218e-01]\n",
      " ..., \n",
      " [ -9.68661189e-01   5.42034388e-01   1.15126371e+00 ...,  -8.52006793e-01\n",
      "   -6.70304239e-01   4.77848887e-01]\n",
      " [  3.67000699e-01   4.21549737e-01   8.97033513e-03 ...,   2.57458866e-01\n",
      "    1.96427971e-01   7.65285119e-02]\n",
      " [ -4.27518426e-05  -1.21131372e-02   3.77787501e-02 ...,  -8.82423893e-02\n",
      "    4.54101712e-03   5.87681048e-02]]\n",
      "\n",
      "[[ -1.84710197e+01  -7.45869827e+01  -6.41599178e+00  -2.60746651e+01\n",
      "   -1.36473475e+01]\n",
      " [ -3.23260193e+01  -8.30433655e+01  -8.10556221e+00  -3.15460014e+01\n",
      "   -2.47860146e+01]\n",
      " [ -4.28678932e+01  -1.31970825e+02  -5.94393015e+00  -4.57771158e+00\n",
      "   -1.53612356e+01]\n",
      " [ -3.80866966e+01  -1.26840454e+02  -1.99077091e+01  -2.30702705e+01\n",
      "   -1.82781239e+01]\n",
      " [ -7.54507494e+00  -6.50036545e+01  -6.28632164e+00  -4.02389079e-01\n",
      "   -1.08189918e-02]\n",
      " [ -2.83696628e+00   0.00000000e+00  -2.49587297e+00  -1.13631201e+00\n",
      "   -1.17951384e+01]\n",
      " [ -4.67132912e+01  -9.42796783e+01  -1.91201077e+01  -1.46994944e+01\n",
      "   -9.69219494e+00]\n",
      " [ -1.10600939e+01  -9.10002518e+01  -1.04452059e-01  -2.24838829e+01\n",
      "   -7.43489122e+00]\n",
      " [ -6.09690063e-02  -6.65967712e+01  -4.57201195e+00  -2.98846054e+01\n",
      "   -4.59525347e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(model.test(x, posteriors)[0])\n",
    "print(\"\")\n",
    "print(model.test(x, posteriors)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -4.25463021e-01,  -1.74731529e+00,  -3.40455741e-01, ...,\n",
      "          8.40714455e-01,   1.16684544e+00,  -1.74363041e+00],\n",
      "       [ -1.73396075e+00,  -3.52505970e+00,  -1.80466461e+00, ...,\n",
      "          1.56047273e+00,   2.06868815e+00,  -2.42936778e+00],\n",
      "       [ -5.50276101e-01,   5.80202416e-02,  -1.51815784e+00, ...,\n",
      "          9.53709185e-01,   2.68967301e-01,   3.69657218e-01],\n",
      "       ..., \n",
      "       [ -9.68661189e-01,   5.42034388e-01,   1.15126371e+00, ...,\n",
      "         -8.52006793e-01,  -6.70304239e-01,   4.77848887e-01],\n",
      "       [  3.67000699e-01,   4.21549737e-01,   8.97033513e-03, ...,\n",
      "          2.57458866e-01,   1.96427971e-01,   7.65285119e-02],\n",
      "       [ -4.27518426e-05,  -1.21131372e-02,   3.77787501e-02, ...,\n",
      "         -8.82423893e-02,   4.54101712e-03,   5.87681048e-02]], dtype=float32), array([[ -1.84710197e+01,  -7.45869827e+01,  -6.41599178e+00,\n",
      "         -2.60746651e+01,  -1.36473475e+01],\n",
      "       [ -3.23260193e+01,  -8.30433655e+01,  -8.10556221e+00,\n",
      "         -3.15460014e+01,  -2.47860146e+01],\n",
      "       [ -4.28678932e+01,  -1.31970825e+02,  -5.94393015e+00,\n",
      "         -4.57771158e+00,  -1.53612356e+01],\n",
      "       [ -3.80866966e+01,  -1.26840454e+02,  -1.99077091e+01,\n",
      "         -2.30702705e+01,  -1.82781239e+01],\n",
      "       [ -7.54507494e+00,  -6.50036545e+01,  -6.28632164e+00,\n",
      "         -4.02389079e-01,  -1.08189918e-02],\n",
      "       [ -2.83696628e+00,   0.00000000e+00,  -2.49587297e+00,\n",
      "         -1.13631201e+00,  -1.17951384e+01],\n",
      "       [ -4.67132912e+01,  -9.42796783e+01,  -1.91201077e+01,\n",
      "         -1.46994944e+01,  -9.69219494e+00],\n",
      "       [ -1.10600939e+01,  -9.10002518e+01,  -1.04452059e-01,\n",
      "         -2.24838829e+01,  -7.43489122e+00],\n",
      "       [ -6.09690063e-02,  -6.65967712e+01,  -4.57201195e+00,\n",
      "         -2.98846054e+01,  -4.59525347e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "print(model.test(x, posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaumWelchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.  69.  76.  58.   0.   0.   0.   0.]\n",
      " [ 87.  63.  69.  76.   0.   0.   0.   0.]\n",
      " [ 93.  87.  63.  69.   0.   0.   0.   0.]\n",
      " [ 53.  93.  87.  63.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ],\n",
       "       [ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix and \"Baum Welch Algorithm\"\n",
    "\n",
    "Compute forward messages: alpha <br>\n",
    "Compute backward messages: beta <br>\n",
    "Compute posteriors: <br>\n",
    "    p(z|x) = alpha * beta <br>\n",
    "    p(z_i, z_i+1 | x) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65  0.86  0.96  0.71  0.08  0.85  0.89  0.43  0.76]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -1.18, -0.52, -0.75, -0.44, -1.66, -0.51, -1.27, -1.69],\n",
       "       [-1.06, -0.91,  0.  ,  0.  ,  0.  ,  0.  , -0.77,  0.  , -0.84],\n",
       "       [-1.1 , -0.82, -1.9 , -0.3 , -0.04, -1.15,  0.  , -0.52, -1.18],\n",
       "       [-0.84, -0.54, -0.96, -0.69, -0.74, -1.26, -0.78, -0.48,  0.  ],\n",
       "       [-0.97,  0.  , -0.87, -1.12, -0.65, -1.4 , -1.74, -0.52, -1.03]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "])\n",
    "print(np.max(test, axis=0))\n",
    "np.subtract(test, np.max(test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,   4.],\n",
       "       [  9.,  16.,  25.],\n",
       "       [ 36.,  49.,  64.]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x2 = np.arange(9.0).reshape((3, 3))\n",
    "np.multiply(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaumWelchModel:\n",
    "    \n",
    "    def normalize_matrix(self, x, axis=1, whole_matrix=False):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\n",
    "            axis=1: row\n",
    "            axis=0: column \n",
    "        Input\n",
    "        -----\n",
    "        \n",
    "        Output\n",
    "        ------\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) == 1 or whole_matrix:\n",
    "            e_x = np.exp(x - np.max(x))\n",
    "            return e_x / np.sum(e_x)\n",
    "        if axis == 0:\n",
    "            e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[None, :]) )\n",
    "            return e_x / np.sum(e_x, axis=axis)[None, :]\n",
    "        else: \n",
    "            e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[:, None]) )\n",
    "            return e_x / np.sum(e_x, axis=axis)[:, None]\n",
    "        \n",
    "    def generate_transition_distant_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "        We extend the latent sentence for 2*length in which each word has \n",
    "        an empty word to represent no-alignment state.\n",
    "        where [sentence_length:end] elements are empty words considered as \n",
    "        latent words having no direct aligment.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_distant_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_distant_matrix = np.zeros((2*sentence_length, 2*sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_distant_matrix[i][j] = p_\n",
    "\n",
    "        print(trans_distant_matrix)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            trans_distant_matrix[i+sentence_length][i+sentence_length] = po\n",
    "            trans_distant_matrix[i][i+sentence_length] = po\n",
    "\n",
    "            sum_d = np.sum(trans_distant_matrix[:sentence_length, i])\n",
    "            trans_distant_matrix[:sentence_length, i] = \\\n",
    "                    np.divide(\n",
    "                        trans_distant_matrix[:sentence_length, i], \n",
    "                        sum_d\n",
    "                    )\n",
    "            trans_distant_matrix[sentence_length:, i] = \\\n",
    "                    np.copy(trans_distant_matrix[:sentence_length, i])\n",
    "\n",
    "        return trans_distant_matrix\n",
    "    \n",
    "    def generate_transition_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_matrix = np.zeros((sentence_length, sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_matrix[i][j] = p_\n",
    "        if nomalized:\n",
    "            return self.normalize_matrix(trans_matrix, axis=1)\n",
    "        return trans_matrix\n",
    "        \n",
    "    def __init__(self, max_distance, po=0.3, seed=1402):\n",
    "        np.random.seed(seed)\n",
    "        self.max_distance = max_distance\n",
    "        self.non_negative_set = np.random.randint(\n",
    "                                    low=1, high=100, \n",
    "                                    size=[max_distance + max_distance + 3]\n",
    "        )\n",
    "        self.po = po\n",
    "        \n",
    "    def calc_forward_messages(self, unary_matrix, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the forward messages ~ alpha values.\n",
    "        \n",
    "        \n",
    "        Input\n",
    "        -----\n",
    "        unary_matrix: emission posteriors - marginal probabilities ~ initial matrix.\n",
    "                      size ~ [1, target_len]\n",
    "        transition_matrix: size ~ [target_len, target_len]\n",
    "        emission_matrix: size ~ [target_len, source_len]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        alpha = np.zeros(np.shape(emission_matrix))\n",
    "        alpha[:,0] = np.multiply(emission_matrix[:,0], unary_matrix)\n",
    "        print(\"first alpha\", alpha)\n",
    "        \n",
    "        for t in np.arange(1, source_len):\n",
    "            for i in range(target_len):\n",
    "                sum_al = 0.0;\n",
    "#                 print(\"alpha : \", t, i, \" :: \", emission_matrix[i][t])\n",
    "                for j in range(target_len):\n",
    "                    sum_al += alpha[j][t-1] * transition_matrix[j][i]\n",
    "#                     print(\"   sum_al: \", t, i, j, alpha[j][t-1], transition_matrix[j][i])\n",
    "\n",
    "                alpha[i][t] = emission_matrix[i][t] * sum_al\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def calc_backward_messages(self, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the backward messages ~ beta values.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        beta\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        beta = np.zeros(np.shape(emission_matrix))\n",
    "        beta[:,-1] = [1]*target_len\n",
    "\n",
    "        for t in reversed(range(source_len-1)):\n",
    "            for i in range(target_len):\n",
    "    #             print(\"beta \", t, i)\n",
    "                for j in range(target_len):\n",
    "                    beta[i][t] += beta[j][t+1] * transition_matrix[i][j] * emission_matrix[j][t+1]\n",
    "    #                 print(\"    \", beta[t+1][j], transition_matrix[i][j], emission_matrix[ observation_sentence[t+1] ][j], beta[t][i])\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def calc_posterior_matrix(self, alpha, beta):\n",
    "        \"\"\"Calcualte the gama and epsilon values in order to reproduce \n",
    "        better transition and emission matrix.\n",
    "        \n",
    "        gamma: P(e_aj|f_j)\n",
    "        epsilon: P(e_aj,e_a(j+1)|f_j)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        unary_matrix, posterior_gamma, posterior_epsilon\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(alpha)[1]\n",
    "        target_len = np.shape(alpha)[0]\n",
    "\n",
    "        gamma = np.multiply(alpha, beta)\n",
    "        epsilon = np.zeros((source_len, target_len, target_len))\n",
    "\n",
    "        # Normalization on columns\n",
    "        gamma = self.normalize_matrix(gamma, axis=0)\n",
    "\n",
    "        for t in range(source_len-1):   \n",
    "            for i in range(target_len):\n",
    "                for j in range(target_len):\n",
    "                    epsilon[t][i][j] = alpha[i][t] * transition_matrix[i][j] * \\\n",
    "                                        beta[j][t+1] * emission_matrix[j][t+1]\n",
    "            # Normalization\n",
    "            epsilon[t] = self.normalize_matrix(epsilon[t], whole_matrix=True)\n",
    "\n",
    "        # Update unary matrix\n",
    "        # Normalization unary\n",
    "        new_unary_matrix = np.copy(gamma[:,0])#self.normalize_matrix(np.copy(gamma[:,0]), axis=1)\n",
    "\n",
    "#         new_transition_matrix = np.zeros( (latent_indice_len, latent_indice_len) )\n",
    "#         new_emission_matrix = np.zeros( (observation_len, latent_indice_len) )\n",
    "            \n",
    "#         # Update emission matrix\n",
    "#         sum_gamma = [np.sum(gamma.T[i]) for i in range(latent_indice_len)]   \n",
    "#         for i in range(latent_indice_len):\n",
    "#             new_emission_matrix.T[i] = np.divide(gamma.T[i], sum_gamma[i])\n",
    "\n",
    "        return new_unary_matrix, gamma, epsilon\n",
    "\n",
    "\n",
    "    def calculate_baum_welch_posteriors(self, sentence_length, unary_matrix, emission_matrix):\n",
    "        transition_matrix = self.generate_transition_matrix(sentence_length)\n",
    "        alpha = self.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "        beta = self.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "        new_unary_matrix, emission_posterior, transition_posterior = self.calc_posterior_matrix(alpha, beta)\n",
    "        return emission_posterior, transition_posterior # gamma, epsilon\n",
    "    \n",
    "    def update_non_negative_transition_set(self, emission_posteriors, transition_posteriors):\n",
    "        pass\n",
    "        # TODO 1: update non-negative set: s[-1] = \n",
    "        # TODO 1.1: calculate new transition matrix\n",
    "        transition_list = np.array([])\n",
    "        for gamma, epsilon in zip(emission_posteriors, transition_posteriors):\n",
    "            source_len = np.shape(gamma)[1]\n",
    "            target_len = np.shape(gamma)[0]\n",
    "            new_transition_matrix = np.zeros((target_len, target_len))\n",
    "\n",
    "            for i in range(target_len):\n",
    "                sum_gamma = np.sum(gamma[i][:-1])\n",
    "                for j in range(target_len):\n",
    "                    sum_ep = np.sum(epsilon[:-1][i][j])\n",
    "                    new_transition_matrix[i][j] = sum_ep/sum_gamma\n",
    "            # Normalization\n",
    "            new_transition_matrix = self.normalize_matrix(new_transition_matrix, axis=1)\n",
    "            transition_list.append(new_transition_matrix)\n",
    "            \n",
    "        # TODO 1.2: update\n",
    "        new_non_negative_set = np.zeros(max_distance)\n",
    "        \n",
    "        return new_non_negative_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_negative_set [58 48 56 10 59 11 62]\n",
      "transition_matrix [[  1.25523017e-21   1.19198156e-01   3.99865265e-05   8.80761858e-01]\n",
      " [  9.52558972e-01   4.99415778e-22   4.74251187e-02   1.59093549e-05]\n",
      " [  1.35757443e-21   9.52574127e-01   4.99423723e-22   4.74258732e-02]\n",
      " [  9.52574127e-01   6.75896510e-23   4.74258732e-02   2.48648431e-23]]\n",
      "emission_matrix [[ 0.23465716  0.28093447  0.26001113  0.18759221  0.23988929  0.33138161]\n",
      " [ 0.2866109   0.28093447  0.19262099  0.20732145  0.32381667  0.22213174]\n",
      " [ 0.19212103  0.23000969  0.28735674  0.41749413  0.19640474  0.20099311]\n",
      " [ 0.2866109   0.20812137  0.26001113  0.18759221  0.23988929  0.24549354]]\n",
      "first alpha [[ 0.11732858  0.          0.          0.          0.          0.        ]\n",
      " [ 0.02866109  0.          0.          0.          0.          0.        ]\n",
      " [ 0.04803026  0.          0.          0.          0.          0.        ]\n",
      " [ 0.04299164  0.          0.          0.          0.          0.        ]]\n",
      "alpha [[  1.17328580e-01   1.91749288e-02   9.60087690e-03   8.90759827e-04\n",
      "    4.41628736e-04   8.02438924e-05]\n",
      " [  2.86610903e-02   1.67823880e-02   5.83870381e-04   3.41631365e-04\n",
      "    6.48759328e-05   1.55039082e-05]\n",
      " [  4.80302584e-02   7.82691466e-04   5.28491490e-04   9.88592806e-05\n",
      "    1.80087562e-05   2.42670496e-06]\n",
      " [  4.29916354e-02   2.19811283e-02   4.40093099e-03   1.59099945e-03\n",
      "    1.89330514e-04   9.56994838e-05]]\n",
      "beta [[  6.22050859e-04   4.17796100e-03   1.09308675e-02   8.12633366e-02\n",
      "    2.42707075e-01   1.00000000e+00]\n",
      " [  1.15347632e-03   2.86922266e-03   1.65806935e-02   5.75412889e-02\n",
      "    3.25196557e-01   1.00000000e+00]\n",
      " [  7.96155248e-04   3.24678317e-03   1.18757006e-02   1.04009693e-01\n",
      "    2.23239693e-01   1.00000000e+00]\n",
      " [  1.15348517e-03   2.86919972e-03   1.65807856e-02   5.75409633e-02\n",
      "    3.25197826e-01   1.00000000e+00]]\n",
      "new_unary_matrix:  [ 0.25000613  0.24999615  0.24999744  0.25000028]\n",
      "gamma:  [[ 0.25000613  0.25000791  0.25001412  0.25000598  0.25001468  0.25000794]\n",
      " [ 0.24999615  0.24999992  0.2499903   0.2499928   0.24999316  0.24999176]\n",
      " [ 0.24999744  0.24998852  0.24998945  0.24999045  0.24998889  0.24998849]\n",
      " [ 0.25000028  0.25000365  0.25000613  0.25001077  0.25000328  0.25001181]]\n",
      "epsilon:  [[[ 0.06249924  0.06249995  0.06249924  0.0625031 ]\n",
      "  [ 0.06250125  0.06249924  0.06249931  0.06249924]\n",
      "  [ 0.06249924  0.06250155  0.06249924  0.06249933]\n",
      "  [ 0.06250225  0.06249924  0.06249934  0.06249924]]\n",
      "\n",
      " [[ 0.06249924  0.0624997   0.06249924  0.06250379]\n",
      "  [ 0.06250208  0.06249924  0.06249941  0.06249924]\n",
      "  [ 0.06249924  0.06249939  0.06249924  0.06249925]\n",
      "  [ 0.06250296  0.06249924  0.06249947  0.06249924]]\n",
      "\n",
      " [[ 0.06249924  0.0625001   0.06249924  0.06250495]\n",
      "  [ 0.06249977  0.06249924  0.06249932  0.06249924]\n",
      "  [ 0.06249924  0.06249962  0.06249924  0.06249926]\n",
      "  [ 0.06250324  0.06249924  0.06249981  0.06249924]]\n",
      "\n",
      " [[ 0.06249924  0.06249994  0.06249924  0.06250307]\n",
      "  [ 0.06250043  0.06249924  0.06249929  0.06249924]\n",
      "  [ 0.06249924  0.06249986  0.06249924  0.06249927]\n",
      "  [ 0.06250476  0.06249924  0.06249945  0.06249924]]\n",
      "\n",
      " [[ 0.06249924  0.06249997  0.06249924  0.06250521]\n",
      "  [ 0.06250052  0.06249924  0.06249928  0.06249924]\n",
      "  [ 0.06249924  0.06249948  0.06249924  0.06249926]\n",
      "  [ 0.06250298  0.06249924  0.06249936  0.06249924]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "target_length = 4\n",
    "max_distance = 2\n",
    "\n",
    "baum_welch_model = BaumWelchModel(max_distance)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)\n",
    "unary_matrix = [.50, .1, .25, .15]\n",
    "# transition_matrix = np.array([\n",
    "#     [.3, .7], \n",
    "#     [.1, .9]\n",
    "# ])\n",
    "transition_matrix = baum_welch_model.generate_transition_matrix(target_length, nomalized=True)\n",
    "print(\"transition_matrix\", transition_matrix)\n",
    "emission_matrix = baum_welch_model.normalize_matrix(np.array([\n",
    "    [.4, .5, .7, .1, .4, .8],\n",
    "    [.6, .5, .4, .2, .7, .4],\n",
    "    [.2, .3, .8, .9, .2, .3],\n",
    "    [.6, .2, .7, .1, .4, .5]\n",
    "]), axis=0)\n",
    "print(\"emission_matrix\", emission_matrix)\n",
    "\n",
    "alpha = baum_welch_model.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "beta = baum_welch_model.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "print(\"alpha\", alpha)\n",
    "print(\"beta\", beta)\n",
    "\n",
    "new_unary_matrix, emission_posterior, transition_posterior = baum_welch_model.calc_posterior_matrix(alpha, beta)\n",
    "print(\"new_unary_matrix: \", new_unary_matrix)\n",
    "print(\"gamma: \", emission_posterior)\n",
    "print(\"epsilon: \", transition_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first alpha [[ 0.11732858  0.          0.          0.          0.          0.        ]\n",
      " [ 0.02866109  0.          0.          0.          0.          0.        ]\n",
      " [ 0.04803026  0.          0.          0.          0.          0.        ]\n",
      " [ 0.04299164  0.          0.          0.          0.          0.        ]]\n",
      "emission_posterior [[  3.22930939e-06   8.16988927e-06   5.06318676e-06   7.16789256e-06\n",
      "    5.07044613e-06   8.25982156e-06]\n",
      " [  2.39984566e-06   2.41359773e-06   7.59571907e-07   7.78932507e-07\n",
      "    1.95912414e-06   9.21736944e-07]\n",
      " [  2.23113160e-06   3.38434358e-07   3.97050630e-07   1.34145548e-06\n",
      "    3.88866479e-07   2.49592902e-07]\n",
      " [  6.10829605e-06   3.04666133e-06   7.74877341e-06   4.68030215e-06\n",
      "    6.55014595e-06   4.53743129e-06]]\n",
      "transition_posterior [[[ 0.06249991  0.06249997  0.06249991  0.06250042]\n",
      "  [ 0.06250016  0.06249991  0.06249992  0.06249991]\n",
      "  [ 0.06249991  0.06250009  0.06249991  0.06249992]\n",
      "  [ 0.06250028  0.06249991  0.06249993  0.06249991]]\n",
      "\n",
      " [[ 0.06249988  0.06249996  0.06249988  0.06250121]\n",
      "  [ 0.06250012  0.06249988  0.0624999   0.06249988]\n",
      "  [ 0.06249988  0.06249991  0.06249988  0.06249989]\n",
      "  [ 0.06250006  0.06249988  0.0624999   0.06249988]]\n",
      "\n",
      " [[ 0.06249991  0.06249998  0.06249991  0.06250071]\n",
      "  [ 0.06249998  0.06249991  0.06249992  0.06249991]\n",
      "  [ 0.06249991  0.06249994  0.06249991  0.06249991]\n",
      "  [ 0.06250032  0.06249991  0.06249998  0.06249991]]\n",
      "\n",
      " [[ 0.06249989  0.06250001  0.06249989  0.06250101]\n",
      "  [ 0.06249997  0.06249989  0.0624999   0.06249989]\n",
      "  [ 0.06249989  0.0625      0.06249989  0.0624999 ]\n",
      "  [ 0.06250016  0.06249989  0.06249991  0.06249989]]\n",
      "\n",
      " [[ 0.0624999   0.0625      0.0624999   0.06250068]\n",
      "  [ 0.06250011  0.0624999   0.06249991  0.0624999 ]\n",
      "  [ 0.0624999   0.06249994  0.0624999   0.06249991]\n",
      "  [ 0.0625003   0.0624999   0.06249992  0.0624999 ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "emission_posterior, transition_posterior = \\\n",
    "    baum_welch_model.calculate_baum_welch_posteriors(sentence_length, unary_matrix, emission_matrix)\n",
    "print(\"emission_posterior\", emission_posterior)\n",
    "print(\"transition_posterior\", transition_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with Unsupervised neural hidden markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BW model variables\n",
    "\n",
    "# Emission model variables\n",
    "input_size = np.shape(x)\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "output_size = 9\n",
    "\n",
    "emission_model = EmissionModel(input_size=input_size, layer_size=layer_size, output_size=output_size)\n",
    "\n",
    "result = emission_model.train_mini_batch(x, posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
