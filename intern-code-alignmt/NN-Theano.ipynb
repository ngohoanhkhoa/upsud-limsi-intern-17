{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import function, printing\n",
    "import theano\n",
    "\n",
    "from theano import config\n",
    "config.compute_test_value = 'raise'\n",
    "\n",
    "# updates = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evolution:\n",
    "\n",
    "    def calculate_AER(self, S, P, A):\n",
    "        s_a, p_a, len_s, len_a = 0, 0, 0, 0\n",
    "        for s, p, a in zip(S, P, A):\n",
    "            s_a += len(list(set(s).intersection(a)))\n",
    "            p_a += len(list(set(p).intersection(a)))\n",
    "            len_s += len(s)\n",
    "            len_a += len(a)\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a += s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len_s + len_a)\n",
    "        print (\"aer\", 1.-aer)\n",
    "\n",
    "        return 1. - aer \n",
    "\n",
    "\n",
    "    def calculate_one_AER(self, S, P, A):\n",
    "        s_a = len(list(set(S).intersection(A)))\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a = len(list(set(P).intersection(A))) + s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len(S) + len(A))\n",
    "        print (\"aer\", 1.-aer)\n",
    "\n",
    "        return 1. - aer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9)\n",
    "np.split(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 512)\n",
      "(9, 512)\n",
      "(9, 5)\n"
     ]
    }
   ],
   "source": [
    "class EmissionModel:\n",
    "    \"\"\" Simple emission model without CNN\n",
    "    word embedding layer -> ReLU layer -> softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def init_weights_bias(self, input_size, layer_size, output_size, seed=1402):\n",
    "        random_state = np.random.RandomState(seed)\n",
    "        \n",
    "        size_list = np.concatenate(([input_size[0]], layer_size, [output_size]), axis=0)\n",
    "        w = []\n",
    "        b = []\n",
    "        \n",
    "        for i in range(len(size_list) - 1):\n",
    "            w.append(\n",
    "                theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], size_list[i])), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True\n",
    "                )\n",
    "            )\n",
    "            b.append(\n",
    "                theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], 1)), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return w, b\n",
    "    \n",
    "    #[7,512]\n",
    "    def __init__(self, input_size, layer_size, output_size, epoch=1, batch=1, learning_rate = .01, seed=1412):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch = batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.posteriors = []\n",
    "        \n",
    "        x_training_input = T.matrix().astype(config.floatX)\n",
    "        x_training_input.tag.test_value = np.asarray([\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  1.,  0.],\n",
    "            [ 0.,  0.,  1.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  1.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 1.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  1.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.]\n",
    "        ]).astype(x_training_input.dtype)\n",
    "        \n",
    "        self.w, self.b = self.init_weights_bias(input_size, layer_size, output_size, seed)\n",
    "        \n",
    "        # word embedding layer\n",
    "        word_embedding_layer = T.dot(self.w[0], x_training_input) # [7, 10] * [10, 5] = [7, 5]\n",
    "        \n",
    "        # ReLU layer\n",
    "        z_relu_layer = T.dot(self.w[1], word_embedding_layer) + self.b[1] # [512, 7] * [7, 5] = [512, 5]\n",
    "        z_relu_layer_shape = T.shape(z_relu_layer)\n",
    "        z_reshaped_relu_layer = T.reshape(z_relu_layer, [z_relu_layer_shape[0]*z_relu_layer_shape[1], 1])\n",
    "        relu_layer = T.nnet.relu(z_reshaped_relu_layer)\n",
    "        relu_layer_reshaped = T.reshape(relu_layer, z_relu_layer_shape) # [512, 5]\n",
    "        \n",
    "        # Softmax layer\n",
    "        z_softmax_layer = T.dot(self.w[2], relu_layer_reshaped) + self.b[2] # [9, 512] * [512, 5] = [9, 5]\n",
    "        softmax_layer = T.transpose(T.nnet.softmax(T.transpose(z_softmax_layer))) # [9, 5]\n",
    "        \n",
    "        # calculate new gradient\n",
    "        posteriors = T.matrix().astype(config.floatX)\n",
    "        posteriors.tag.test_value = np.asarray([\n",
    "            [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62],\n",
    "            [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55],\n",
    "            [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05],\n",
    "            [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51],\n",
    "            [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11]\n",
    "        ]).astype(posteriors.dtype)\n",
    "        \n",
    "        cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer))\n",
    "        # TODO: use dw[] and db[] abstractly \n",
    "        dw0,dw1,dw2,db1,db2 = T.grad(\n",
    "            cost=cost, wrt=[self.w[0],self.w[1],self.w[2],self.b[1],self.b[2]]\n",
    "        )\n",
    "\n",
    "        # Update w and b\n",
    "        updates = [\n",
    "            (self.w[0], self.w[0] - self.learning_rate * dw0), \n",
    "            (self.w[1], self.w[1] - self.learning_rate * dw1), \n",
    "            (self.b[1], self.b[1] - self.learning_rate * db1),\n",
    "            (self.w[2], self.w[2] - self.learning_rate * dw2), \n",
    "            (self.b[2], self.b[2] - self.learning_rate * db2)\n",
    "        ]\n",
    "        \n",
    "        # Compile model\n",
    "        self.test = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw1, softmax_layer]\n",
    "        ) \n",
    "        self.train_mini_batch = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw2, self.w[2], softmax_layer], \n",
    "            updates=updates\n",
    "        )\n",
    "        self.test_values = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=[softmax_layer]\n",
    "        )\n",
    "        \n",
    "    def train_model(inputs):\n",
    "        for i in range(self.epoch):\n",
    "            for x_input in np.split(inputs, self.batch):\n",
    "                self.posteriors = \n",
    "                self.train_mini_batch(x_input, posteriors)\n",
    "            # TODO: create train_batch function \n",
    "    \n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "input_size = np.shape(x)\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "output_size = 9\n",
    "\n",
    "model = EmissionModel(input_size=input_size, layer_size=layer_size, output_size=output_size)\n",
    "\n",
    "result = model.train_mini_batch(x, posteriors)\n",
    "print(np.shape(result[0]))\n",
    "print(np.shape(result[1]))\n",
    "print(np.shape(result[2]))\n",
    "# print(np.shape(model.evaluate_model(x)))\n",
    "# print(model.calculate_gradient(posteriors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.25463021e-01  -1.74731529e+00  -3.40455741e-01 ...,   8.40714455e-01\n",
      "    1.16684544e+00  -1.74363041e+00]\n",
      " [ -1.73396075e+00  -3.52505970e+00  -1.80466461e+00 ...,   1.56047273e+00\n",
      "    2.06868815e+00  -2.42936778e+00]\n",
      " [ -5.50276101e-01   5.80202416e-02  -1.51815784e+00 ...,   9.53709185e-01\n",
      "    2.68967301e-01   3.69657218e-01]\n",
      " ..., \n",
      " [ -9.68661189e-01   5.42034388e-01   1.15126371e+00 ...,  -8.52006793e-01\n",
      "   -6.70304239e-01   4.77848887e-01]\n",
      " [  3.67000699e-01   4.21549737e-01   8.97033513e-03 ...,   2.57458866e-01\n",
      "    1.96427971e-01   7.65285119e-02]\n",
      " [ -4.27518426e-05  -1.21131372e-02   3.77787501e-02 ...,  -8.82423893e-02\n",
      "    4.54101712e-03   5.87681048e-02]]\n",
      "\n",
      "[[ -1.84710197e+01  -7.45869827e+01  -6.41599178e+00  -2.60746651e+01\n",
      "   -1.36473475e+01]\n",
      " [ -3.23260193e+01  -8.30433655e+01  -8.10556221e+00  -3.15460014e+01\n",
      "   -2.47860146e+01]\n",
      " [ -4.28678932e+01  -1.31970825e+02  -5.94393015e+00  -4.57771158e+00\n",
      "   -1.53612356e+01]\n",
      " [ -3.80866966e+01  -1.26840454e+02  -1.99077091e+01  -2.30702705e+01\n",
      "   -1.82781239e+01]\n",
      " [ -7.54507494e+00  -6.50036545e+01  -6.28632164e+00  -4.02389079e-01\n",
      "   -1.08189918e-02]\n",
      " [ -2.83696628e+00   0.00000000e+00  -2.49587297e+00  -1.13631201e+00\n",
      "   -1.17951384e+01]\n",
      " [ -4.67132912e+01  -9.42796783e+01  -1.91201077e+01  -1.46994944e+01\n",
      "   -9.69219494e+00]\n",
      " [ -1.10600939e+01  -9.10002518e+01  -1.04452059e-01  -2.24838829e+01\n",
      "   -7.43489122e+00]\n",
      " [ -6.09690063e-02  -6.65967712e+01  -4.57201195e+00  -2.98846054e+01\n",
      "   -4.59525347e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(model.test(x, posteriors)[0])\n",
    "print(\"\")\n",
    "print(model.test(x, posteriors)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -4.25463021e-01,  -1.74731529e+00,  -3.40455741e-01, ...,\n",
      "          8.40714455e-01,   1.16684544e+00,  -1.74363041e+00],\n",
      "       [ -1.73396075e+00,  -3.52505970e+00,  -1.80466461e+00, ...,\n",
      "          1.56047273e+00,   2.06868815e+00,  -2.42936778e+00],\n",
      "       [ -5.50276101e-01,   5.80202416e-02,  -1.51815784e+00, ...,\n",
      "          9.53709185e-01,   2.68967301e-01,   3.69657218e-01],\n",
      "       ..., \n",
      "       [ -9.68661189e-01,   5.42034388e-01,   1.15126371e+00, ...,\n",
      "         -8.52006793e-01,  -6.70304239e-01,   4.77848887e-01],\n",
      "       [  3.67000699e-01,   4.21549737e-01,   8.97033513e-03, ...,\n",
      "          2.57458866e-01,   1.96427971e-01,   7.65285119e-02],\n",
      "       [ -4.27518426e-05,  -1.21131372e-02,   3.77787501e-02, ...,\n",
      "         -8.82423893e-02,   4.54101712e-03,   5.87681048e-02]], dtype=float32), array([[ -1.84710197e+01,  -7.45869827e+01,  -6.41599178e+00,\n",
      "         -2.60746651e+01,  -1.36473475e+01],\n",
      "       [ -3.23260193e+01,  -8.30433655e+01,  -8.10556221e+00,\n",
      "         -3.15460014e+01,  -2.47860146e+01],\n",
      "       [ -4.28678932e+01,  -1.31970825e+02,  -5.94393015e+00,\n",
      "         -4.57771158e+00,  -1.53612356e+01],\n",
      "       [ -3.80866966e+01,  -1.26840454e+02,  -1.99077091e+01,\n",
      "         -2.30702705e+01,  -1.82781239e+01],\n",
      "       [ -7.54507494e+00,  -6.50036545e+01,  -6.28632164e+00,\n",
      "         -4.02389079e-01,  -1.08189918e-02],\n",
      "       [ -2.83696628e+00,   0.00000000e+00,  -2.49587297e+00,\n",
      "         -1.13631201e+00,  -1.17951384e+01],\n",
      "       [ -4.67132912e+01,  -9.42796783e+01,  -1.91201077e+01,\n",
      "         -1.46994944e+01,  -9.69219494e+00],\n",
      "       [ -1.10600939e+01,  -9.10002518e+01,  -1.04452059e-01,\n",
      "         -2.24838829e+01,  -7.43489122e+00],\n",
      "       [ -6.09690063e-02,  -6.65967712e+01,  -4.57201195e+00,\n",
      "         -2.98846054e+01,  -4.59525347e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "print(model.test(x, posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with Unsupervised neural hidden markov model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.  69.  76.  58.   0.   0.   0.   0.]\n",
      " [ 87.  63.  69.  76.   0.   0.   0.   0.]\n",
      " [ 93.  87.  63.  69.   0.   0.   0.   0.]\n",
      " [ 53.  93.  87.  63.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ],\n",
       "       [ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initlize non negative set\n",
    "sentence_length = 4\n",
    "max_distance = 2\n",
    "non_negative_set = np.random.randint(low=1, high=100, size=[max_distance+max_distance+3])\n",
    "po = 0.3\n",
    "global_transition_matrix = generate_transition_distant_matrix(sentence_length, max_distance, non_negative_set, po)\n",
    "global_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix and \"Baum Welch Algorithm\"\n",
    "\n",
    "Compute forward messages: alpha <br>\n",
    "Compute backward messages: beta <br>\n",
    "Compute posteriors: <br>\n",
    "    p(z|x) = alpha * beta <br>\n",
    "    p(z_i, z_i+1 | x) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaumWelchModel:\n",
    "    \n",
    "    def generate_transition_distant_matrix(\n",
    "            self, sentence_length, po=0.):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "        We extend the latent sentence for 2*length in which each word has \n",
    "        an empty word to represent no-alignment state.\n",
    "        where [sentence_length:end] elements are empty words considered as \n",
    "        latent words having no direct aligment.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_distant_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_distant_matrix = np.zeros((2*sentence_length, 2*sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > self.max_distance + self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_distant_matrix[i][j] = p_\n",
    "\n",
    "        print(trans_distant_matrix)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            trans_distant_matrix[i+sentence_length][i+sentence_length] = po\n",
    "            trans_distant_matrix[i][i+sentence_length] = po\n",
    "\n",
    "            sum_d = np.sum(trans_distant_matrix[:sentence_length, i])\n",
    "            trans_distant_matrix[:sentence_length, i] = \\\n",
    "                    np.divide(\n",
    "                        trans_distant_matrix[:sentence_length, i], \n",
    "                        sum_d\n",
    "                    )\n",
    "            trans_distant_matrix[sentence_length:, i] = \\\n",
    "                    np.copy(trans_distant_matrix[:sentence_length, i])\n",
    "\n",
    "        return trans_distant_matrix \n",
    "    \n",
    "    def __init__(self, max_distance, po=0.3, seed=1402):\n",
    "        np.random.seed(seed)\n",
    "        self.max_distance = max_distance\n",
    "        self.non_negative_set = np.random.randint(\n",
    "                                    low=1, high=100, \n",
    "                                    size=[max_distance + max_distance + 3]\n",
    "        )\n",
    "        self.po = po\n",
    "        \n",
    "    def _calc_forward_messages(self, unary_matrix, transition_matrix, emission_matrix,\n",
    "                              latent_indice_len, observation_sentence):\n",
    "        \"\"\"Calcualte the forward messages ~ alpha values.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        unary_matrix: marginal probabilities ~ initial matrix.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: verify matrix length\n",
    "        observation_len = len(observation_sentence)\n",
    "\n",
    "        alpha = np.zeros( (observation_len, latent_indice_len) )\n",
    "        alpha[0] = np.multiply(emission_matrix[0], unary_matrix)\n",
    "\n",
    "        for t in np.arange(1, observation_len):\n",
    "            for j in range(latent_indice_len):\n",
    "                sum_al = 0.0;\n",
    "    #             print(\"alpha : \", t, j, \" :: \", emission_matrix[ observation_sentence[t] ][ j ])\n",
    "                for i in range(latent_indice_len):\n",
    "                    sum_al += alpha[t-1][i] * transition_matrix[i][j]\n",
    "    #                 print(\"   sum_al: \", alpha[t-1][i], transition_matrix[i][j])\n",
    "\n",
    "                alpha[t][j] = emission_matrix[ observation_sentence[t] ][ j ] * sum_al\n",
    "\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def _calc_backward_messages(self, transition_matrix, emission_matrix,\n",
    "                               latent_indice_len, observation_sentence):\n",
    "        \"\"\"Calcualte the backward messages ~ beta values.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        beta\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        observation_len = len(observation_sentence)\n",
    "\n",
    "        beta = np.zeros( (observation_len, latent_indice_len) )\n",
    "        beta[-1] = [1]*latent_indice_len\n",
    "\n",
    "        for t in reversed(range(observation_len-1)):\n",
    "            for i in range(latent_indice_len):\n",
    "    #             print(\"beta \", t, i)\n",
    "                for j in range(latent_indice_len):\n",
    "                    beta[t][i] += \\\n",
    "                            beta[t+1][j] \\\n",
    "                            * transition_matrix[i][j] \\\n",
    "                            * emission_matrix[ observation_sentence[t+1] ][j]\n",
    "    #                 print(\"    \", beta[t+1][j], transition_matrix[i][j], emission_matrix[ observation_sentence[t+1] ][j], beta[t][i])\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def _calc_posterior_matrix(self, alpha, beta, latent_indice_len, observation_sentence):\n",
    "        \"\"\"Calcualte the gama and epsilon values in order to reproduce \n",
    "        better transition and emission matrix.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        unary_matrix, posterior_gamma, posterior_epsilon\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        observation_len = len(observation_sentence)\n",
    "\n",
    "\n",
    "        gamma = np.multiply(alpha, beta)\n",
    "        epsilon = np.zeros( (observation_len-1, latent_indice_len, latent_indice_len) )\n",
    "\n",
    "        # Normalization on rows\n",
    "        for i in range(len(gamma)):\n",
    "            sum_gamma = np.sum(gamma[i])\n",
    "            gamma[i] = np.divide(gamma[i], sum_gamma)\n",
    "\n",
    "        for t in range(observation_len - 1):   \n",
    "            for i in range(latent_indice_len):\n",
    "                for j in range(latent_indice_len):\n",
    "                    epsilon[t][i][j] = \\\n",
    "                            alpha[t][i] * \\\n",
    "                            transition_matrix[j][i] * \\\n",
    "                            beta[t+1][j] * \\\n",
    "                            emission_matrix[ observation_sentence[t+1] ][j]\n",
    "            # Normalization\n",
    "            sum_ep = np.sum(epsilon[t])\n",
    "            epsilon[t] = np.divide(epsilon[t], sum_ep)\n",
    "\n",
    "        print(\"gamma: \", gamma)\n",
    "        print(\"epsilon: \", epsilon)\n",
    "\n",
    "        # Update unary matrix\n",
    "        new_unary_matrix = np.copy(gamma[0])\n",
    "        #Normalization unary\n",
    "        sum_unary = np.sum(new_unary_matrix)\n",
    "        new_unary_matrix = np.divide(new_unary_matrix, sum_unary)\n",
    "\n",
    "        new_transition_matrix = np.zeros( (latent_indice_len, latent_indice_len) )\n",
    "        new_emission_matrix = np.zeros( (observation_len, latent_indice_len) )\n",
    "\n",
    "        # Update transition matrix\n",
    "        for i in range(latent_indice_len):\n",
    "            sum_gamma = np.sum(gamma.T[i][:-1])\n",
    "            for j in range(latent_indice_len):\n",
    "                sum_ep = np.sum( epsilon.T[j][i][:-1] )\n",
    "                new_transition_matrix[i][j] = sum_ep/sum_gamma\n",
    "            # Normalization\n",
    "            sum_trans = np.sum(new_transition_matrix[i])\n",
    "            new_transition_matrix[i] = np.divide(new_transition_matrix[i], sum_trans)\n",
    "\n",
    "        # Update emission matrix\n",
    "        sum_gamma = [np.sum(gamma.T[i]) for i in range(latent_indice_len)]   \n",
    "        for i in range(latent_indice_len):\n",
    "            new_emission_matrix.T[i] = np.divide(gamma.T[i], sum_gamma[i])\n",
    "\n",
    "        return new_unary_matrix, gamma, epsilon\n",
    "\n",
    "\n",
    "    def calculate_baum_welch_posteriors(self, sentence_length, \n",
    "                                        unary_matrix, emission_matrix,\n",
    "                                        latent_indice_len, observation_sentence):\n",
    "        transition_matrix = self.generate_transition_distant_matrix(sentence_length, max_distance)\n",
    "        \n",
    "        alpha = calc_forward_messages(\n",
    "            unary_matrix, transition_matrix, emission_matrix,\n",
    "            latent_indice_len, observation_sentence\n",
    "        )\n",
    "        beta = calc_backward_messages(\n",
    "            transition_matrix, emission_matrix, latent_indice_len, observation_sentence\n",
    "        )\n",
    "\n",
    "        new_unary_matrix, emission_postorior, transition__postorior = calc_posterior_matrix(\n",
    "                                                alpha, beta, latent_indice_len, observation_sentence\n",
    "        )\n",
    "\n",
    "        return gamma, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.  56.  48.  58.   0.   0.   0.   0.]\n",
      " [ 59.  10.  56.  48.   0.   0.   0.   0.]\n",
      " [ 11.  59.  10.  56.   0.   0.   0.   0.]\n",
      " [ 62.  11.  59.  10.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07042254,  0.41176471,  0.27745665,  0.3372093 ,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.41549296,  0.07352941,  0.32369942,  0.27906977,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.07746479,  0.43382353,  0.05780347,  0.3255814 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.43661972,  0.08088235,  0.34104046,  0.05813953,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ],\n",
       "       [ 0.07042254,  0.41176471,  0.27745665,  0.3372093 ,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.41549296,  0.07352941,  0.32369942,  0.27906977,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.07746479,  0.43382353,  0.05780347,  0.3255814 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.43661972,  0.08088235,  0.34104046,  0.05813953,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initlize non negative set\n",
    "sentence_length = 4\n",
    "max_distance = 2\n",
    "\n",
    "baum_welch_model = BaumWelchModel(max_distance)\n",
    "baum_welch_model.generate_transition_distant_matrix(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unary_matrix = [.85, .15]\n",
    "transition_matrix = [\n",
    "    [.3, .7], \n",
    "    [.1, .9]\n",
    "]\n",
    "emission_matrix = [\n",
    "    [.4, .5],\n",
    "    [.6, .5]\n",
    "]\n",
    "latent_indice_len = 2\n",
    "observation_sentence = [0,1,1,0]\n",
    "\n",
    "\n",
    "baum_welch_model.calculate_baum_welch_posteriors(sentence_length, unary_matrix, emission_matrix,\n",
    "                                        latent_indice_len, observation_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
