{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import theano.tensor as T\n",
    "from theano import function, printing\n",
    "import theano\n",
    "\n",
    "from theano import config\n",
    "# config.device = 'cpu'\n",
    "config.mode = 'DebugMode'\n",
    "# config.gcc.cxxflags = \"-D_hypot=hypot\"\n",
    "config.compute_test_value = 'warn'\n",
    "\n",
    "\n",
    "# updates = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_AER(S, P, A):\n",
    "    S, P, A = np.array(S), np.array(P), np.array(A)\n",
    "    s_a, p_a, len_s, len_a = 0, 0, 0, 0\n",
    "    for s, p, a in zip(S, P, A):\n",
    "        s_a += len(list(set(s).intersection(a)))\n",
    "        p_a += len(list(set(p).intersection(a)))\n",
    "        len_s += len(s[s != \"\"])\n",
    "        len_a += len(a[a != \"\"])\n",
    "    print (\"s_a\", s_a)\n",
    "    p_a += s_a\n",
    "    print (\"p_a\", p_a)\n",
    "    aer = (s_a + p_a) / (len_s + len_a)\n",
    "    print (\"aer\", 1.-aer)\n",
    "    \n",
    "    return 1. - aer \n",
    "\n",
    "\n",
    "def calculate_one_AER(S, P, A):\n",
    "    S, P, A = np.array(S), np.array(P), np.array(A)\n",
    "    s_a = len(list(set(S).intersection(A)))\n",
    "    print (\"s_a\", s_a)\n",
    "    p_a = len(list(set(P).intersection(A))) + s_a\n",
    "    print (\"p_a\", p_a)\n",
    "    aer = (s_a + p_a) / (len(S[S != \"\"]) + len(A[A != \"\"]))\n",
    "    print (\"aer\", 1.-aer)\n",
    "    \n",
    "    return 1. - aer \n",
    "    \n",
    "def write_file(strs, file_name):\n",
    "    alignment_test = open(file_name,\"w\", encoding='utf8') \n",
    "    for s in strs:\n",
    "        alignment_test.write(s + \"\\n\") \n",
    "    alignment_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9)\n",
    "np.split(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels = 5\n",
    "target_vector = [2, 4, 0]\n",
    "np.eye(n_labels)[target_vector].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "class EmissionModel:\n",
    "    \"\"\" Simple emission model without CNN\n",
    "    word embedding layer -> ReLU layer -> softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def init_weights_bias(self, vocab_input_size, layer_size, output_size, seed=1402):\n",
    "        random_state = np.random.RandomState(seed)\n",
    "        \n",
    "        size_list = np.concatenate(([vocab_input_size], layer_size, [output_size]), axis=0)\n",
    "        w = []\n",
    "        b = []\n",
    "        \n",
    "        for i in range(len(size_list) - 1):\n",
    "            w.append(theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], size_list[i])), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), borrow=True\n",
    "            ))\n",
    "            b.append(theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], 1)), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "            ))\n",
    "        \n",
    "        return w, b\n",
    "    \n",
    "    #[7,512]\n",
    "    def __init__(self, vocab_input_size, layer_size, vocab_output_size, baum_welch_model, \n",
    "                 epoch=1, batch=1, learning_rate = .01, seed=1412):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch = batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.posteriors = []\n",
    "        self.baum_welch_model = baum_welch_model\n",
    "        \n",
    "        self.vocab_input_size = vocab_input_size\n",
    "        self.d_embedding_size = layer_size[0]\n",
    "        \n",
    "        x_training_input = T.matrix().astype(config.floatX)\n",
    "        x_training_input.tag.test_value = np.asarray([\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  1.,  0.],\n",
    "            [ 0.,  0.,  1.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  1.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 1.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  1.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.]\n",
    "        ]).astype(x_training_input.dtype)\n",
    "        \n",
    "        self.w, self.b = self.init_weights_bias(vocab_input_size, layer_size, vocab_output_size, seed)\n",
    "        \n",
    "        # Word embedding layer\n",
    "        word_embedding_layer = T.dot(self.w[0], x_training_input) # [7, 10] * [10, 5] = [7, 5]\n",
    "        \n",
    "        # ReLU layer\n",
    "        z_relu_layer = T.dot(self.w[1], word_embedding_layer) + self.b[1] # [512, 7] * [7, 5] = [512, 5]\n",
    "        z_relu_layer_shape = T.shape(z_relu_layer)\n",
    "        z_reshaped_relu_layer = T.reshape(z_relu_layer, [z_relu_layer_shape[0]*z_relu_layer_shape[1], 1])\n",
    "        relu_layer = T.nnet.relu(z_reshaped_relu_layer)\n",
    "        relu_layer_reshaped = T.reshape(relu_layer, z_relu_layer_shape) # [512, 5]\n",
    "        \n",
    "        # Softmax layer\n",
    "        z_softmax_layer = T.dot(self.w[2], relu_layer_reshaped) + self.b[2] # [12, 512] * [512, 5] = [12, 5]\n",
    "        softmax_layer = T.transpose(T.nnet.softmax(T.transpose(z_softmax_layer))) # Output: [12, 5]\n",
    "        \n",
    "        # Calculate new gradient\n",
    "        posteriors = T.matrix().astype(config.floatX)\n",
    "        posteriors.tag.test_value = np.asarray([\n",
    "            [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62, 0.24, 0.71, 0.81],\n",
    "            [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55, 0.11, 0.37, 0.18],\n",
    "            [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05, 0.62, 0.17, 0.29],\n",
    "            [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51, 0.19, 0.67, 0.52],\n",
    "            [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11, 0.44, 0.37, 0.14]\n",
    "        ]).astype(posteriors.dtype)\n",
    "        \n",
    "        cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer))\n",
    "        # TODO: use dw[] and db[] abstractly \n",
    "        dw0,dw1,dw2,db1,db2 = T.grad(\n",
    "            cost=cost, wrt=[self.w[0],self.w[1],self.w[2],self.b[1],self.b[2]]\n",
    "        )\n",
    "\n",
    "        # Update w and b\n",
    "        updates = [\n",
    "            (self.w[0], self.w[0] - self.learning_rate * dw0), \n",
    "            (self.w[1], self.w[1] - self.learning_rate * dw1), \n",
    "            (self.b[1], self.b[1] - self.learning_rate * db1),\n",
    "            (self.w[2], self.w[2] - self.learning_rate * dw2), \n",
    "            (self.b[2], self.b[2] - self.learning_rate * db2)\n",
    "        ]\n",
    "        \n",
    "        # Compile model\n",
    "        self.test = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw1, softmax_layer]\n",
    "        ) \n",
    "        self.train_mini_batch_function = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=softmax_layer, \n",
    "            updates=updates\n",
    "        )\n",
    "        self.test_values = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=softmax_layer\n",
    "        )\n",
    "    \n",
    "    def train_mini_batch(self, testing_target, testing_source):\n",
    "        one_hot_input = np.eye(self.vocab_input_size)[testing_target].T\n",
    "        one_hot_input = np.asarray(one_hot_input).astype(config.floatX)\n",
    "        print(\"one_hot_input\", one_hot_input, np.shape(one_hot_input))\n",
    "        emission_matrix = self.test_values(one_hot_input)\n",
    "        print(\"emission_matrix 1\", emission_matrix, np.shape(emission_matrix))\n",
    "        \n",
    "        emission_posterior_vout = np.zeros_like(emission_matrix.T) # [V_f_size, e_size]\n",
    "        new_emission_matrix = [] # [f_size, e_size]\n",
    "        for indice in testing_source:\n",
    "            new_emission_matrix.append(emission_matrix[indice])\n",
    "        print(\"new_emission_matrix\", new_emission_matrix, np.shape(new_emission_matrix))\n",
    "        emission_posterior, transition_posterior = \\\n",
    "            baum_welch_model.calculate_baum_welch_posteriors(len(testing_target), np.transpose(new_emission_matrix))\n",
    "        print(\"emission_posterior\", emission_posterior, np.shape(emission_posterior))\n",
    "        \n",
    "        # transform emission size to [target_size, v_out]\n",
    "        for i, indice in enumerate(testing_source):\n",
    "            emission_posterior_vout[:, indice] = emission_posterior[:, i]\n",
    "        print(\"emission_posterior_vout\", emission_posterior_vout, np.shape(emission_posterior_vout))\n",
    "        \n",
    "        return emission_posterior_vout\n",
    "#         return self.train_mini_batch_function(one_hot_input, np.asarray(emission_posterior).astype(config.floatX))\n",
    "        \n",
    "    def train_model(inputs):\n",
    "        pass\n",
    "#         for i in range(self.epoch):\n",
    "#             for x_input in np.split(inputs, self.batch):\n",
    "#                 self.posteriors = \n",
    "#                 self.train_mini_batch(x_input, posteriors)\n",
    "            # TODO: create train_batch function \n",
    "\n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "    ]).astype(config.floatX)\n",
    "\n",
    "posteriors = np.asarray([\n",
    "    [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62, 0.24, 0.71, 0.81],\n",
    "    [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55, 0.11, 0.37, 0.18],\n",
    "    [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05, 0.62, 0.17, 0.29],\n",
    "    [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51, 0.19, 0.67, 0.52],\n",
    "    [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11, 0.44, 0.37, 0.14]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "vocab_input_size = np.shape(x)[0]\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = 12\n",
    "\n",
    "model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                      vocab_output_size=vocab_output_size, baum_welch_model=None)\n",
    "\n",
    "result = model.train_mini_batch_function(x, posteriors)\n",
    "print(np.shape(result[0]))\n",
    "print(np.shape(result[1]))\n",
    "print(np.shape(result[2]))\n",
    "# print(np.shape(model.evaluate_model(x)))\n",
    "# print(model.calculate_gradient(posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emission model variables\n",
    "vocab_input_size = 10\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = 12\n",
    "\n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "]).astype(config.floatX)\n",
    "posteriors = np.asarray(result).astype(config.floatX)\n",
    "\n",
    "testing_target = [6, 8, 2, 1, 3]\n",
    "testing_source = [2, 7, 10, 0, 4, 5]\n",
    "\n",
    "model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, vocab_output_size=vocab_output_size, baum_welch_model=None)\n",
    "result = model.train_mini_batch_function(x, posteriors)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.25463021e-01  -1.74731529e+00  -3.40455741e-01 ...,   8.40714455e-01\n",
      "    1.16684544e+00  -1.74363041e+00]\n",
      " [ -1.73396075e+00  -3.52505970e+00  -1.80466461e+00 ...,   1.56047273e+00\n",
      "    2.06868815e+00  -2.42936778e+00]\n",
      " [ -5.50276101e-01   5.80202416e-02  -1.51815784e+00 ...,   9.53709185e-01\n",
      "    2.68967301e-01   3.69657218e-01]\n",
      " ..., \n",
      " [ -9.68661189e-01   5.42034388e-01   1.15126371e+00 ...,  -8.52006793e-01\n",
      "   -6.70304239e-01   4.77848887e-01]\n",
      " [  3.67000699e-01   4.21549737e-01   8.97033513e-03 ...,   2.57458866e-01\n",
      "    1.96427971e-01   7.65285119e-02]\n",
      " [ -4.27518426e-05  -1.21131372e-02   3.77787501e-02 ...,  -8.82423893e-02\n",
      "    4.54101712e-03   5.87681048e-02]]\n",
      "\n",
      "[[ -1.84710197e+01  -7.45869827e+01  -6.41599178e+00  -2.60746651e+01\n",
      "   -1.36473475e+01]\n",
      " [ -3.23260193e+01  -8.30433655e+01  -8.10556221e+00  -3.15460014e+01\n",
      "   -2.47860146e+01]\n",
      " [ -4.28678932e+01  -1.31970825e+02  -5.94393015e+00  -4.57771158e+00\n",
      "   -1.53612356e+01]\n",
      " [ -3.80866966e+01  -1.26840454e+02  -1.99077091e+01  -2.30702705e+01\n",
      "   -1.82781239e+01]\n",
      " [ -7.54507494e+00  -6.50036545e+01  -6.28632164e+00  -4.02389079e-01\n",
      "   -1.08189918e-02]\n",
      " [ -2.83696628e+00   0.00000000e+00  -2.49587297e+00  -1.13631201e+00\n",
      "   -1.17951384e+01]\n",
      " [ -4.67132912e+01  -9.42796783e+01  -1.91201077e+01  -1.46994944e+01\n",
      "   -9.69219494e+00]\n",
      " [ -1.10600939e+01  -9.10002518e+01  -1.04452059e-01  -2.24838829e+01\n",
      "   -7.43489122e+00]\n",
      " [ -6.09690063e-02  -6.65967712e+01  -4.57201195e+00  -2.98846054e+01\n",
      "   -4.59525347e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(model.test(x, posteriors)[0])\n",
    "print(\"\")\n",
    "print(model.test(x, posteriors)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -4.25463021e-01,  -1.74731529e+00,  -3.40455741e-01, ...,\n",
      "          8.40714455e-01,   1.16684544e+00,  -1.74363041e+00],\n",
      "       [ -1.73396075e+00,  -3.52505970e+00,  -1.80466461e+00, ...,\n",
      "          1.56047273e+00,   2.06868815e+00,  -2.42936778e+00],\n",
      "       [ -5.50276101e-01,   5.80202416e-02,  -1.51815784e+00, ...,\n",
      "          9.53709185e-01,   2.68967301e-01,   3.69657218e-01],\n",
      "       ..., \n",
      "       [ -9.68661189e-01,   5.42034388e-01,   1.15126371e+00, ...,\n",
      "         -8.52006793e-01,  -6.70304239e-01,   4.77848887e-01],\n",
      "       [  3.67000699e-01,   4.21549737e-01,   8.97033513e-03, ...,\n",
      "          2.57458866e-01,   1.96427971e-01,   7.65285119e-02],\n",
      "       [ -4.27518426e-05,  -1.21131372e-02,   3.77787501e-02, ...,\n",
      "         -8.82423893e-02,   4.54101712e-03,   5.87681048e-02]], dtype=float32), array([[ -1.84710197e+01,  -7.45869827e+01,  -6.41599178e+00,\n",
      "         -2.60746651e+01,  -1.36473475e+01],\n",
      "       [ -3.23260193e+01,  -8.30433655e+01,  -8.10556221e+00,\n",
      "         -3.15460014e+01,  -2.47860146e+01],\n",
      "       [ -4.28678932e+01,  -1.31970825e+02,  -5.94393015e+00,\n",
      "         -4.57771158e+00,  -1.53612356e+01],\n",
      "       [ -3.80866966e+01,  -1.26840454e+02,  -1.99077091e+01,\n",
      "         -2.30702705e+01,  -1.82781239e+01],\n",
      "       [ -7.54507494e+00,  -6.50036545e+01,  -6.28632164e+00,\n",
      "         -4.02389079e-01,  -1.08189918e-02],\n",
      "       [ -2.83696628e+00,   0.00000000e+00,  -2.49587297e+00,\n",
      "         -1.13631201e+00,  -1.17951384e+01],\n",
      "       [ -4.67132912e+01,  -9.42796783e+01,  -1.91201077e+01,\n",
      "         -1.46994944e+01,  -9.69219494e+00],\n",
      "       [ -1.10600939e+01,  -9.10002518e+01,  -1.04452059e-01,\n",
      "         -2.24838829e+01,  -7.43489122e+00],\n",
      "       [ -6.09690063e-02,  -6.65967712e+01,  -4.57201195e+00,\n",
      "         -2.98846054e+01,  -4.59525347e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "print(model.test(x, posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaumWelchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.  69.  76.  58.   0.   0.   0.   0.]\n",
      " [ 87.  63.  69.  76.   0.   0.   0.   0.]\n",
      " [ 93.  87.  63.  69.   0.   0.   0.   0.]\n",
      " [ 53.  93.  87.  63.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ],\n",
       "       [ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix and \"Baum Welch Algorithm\"\n",
    "\n",
    "Compute forward messages: alpha <br>\n",
    "Compute backward messages: beta <br>\n",
    "Compute posteriors: <br>\n",
    "    p(z|x) = alpha * beta <br>\n",
    "    p(z_i, z_i+1 | x) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaumWelchModel:\n",
    "    \n",
    "    def normalize_matrix(self, x, axis=1, whole_matrix=False):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\n",
    "            axis=1: row\n",
    "            axis=0: column \n",
    "        Input\n",
    "        -----\n",
    "        \n",
    "        Output\n",
    "        ------\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) == 1 or whole_matrix:\n",
    "#             e_x = np.exp(x - np.max(x))\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x)\n",
    "        if axis == 0:\n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[None, :]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[None, :]\n",
    "        else: \n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[:, None]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[:, None]\n",
    "        \n",
    "    def generate_transition_distant_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "        We extend the latent sentence for 2*length in which each word has \n",
    "        an empty word to represent no-alignment state.\n",
    "        where [sentence_length:end] elements are empty words considered as \n",
    "        latent words having no direct aligment.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_distant_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_distant_matrix = np.zeros((2*sentence_length, 2*sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_distant_matrix[i][j] = p_\n",
    "\n",
    "        print(trans_distant_matrix)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            trans_distant_matrix[i+sentence_length][i+sentence_length] = po\n",
    "            trans_distant_matrix[i][i+sentence_length] = po\n",
    "\n",
    "            sum_d = np.sum(trans_distant_matrix[:sentence_length, i])\n",
    "            trans_distant_matrix[:sentence_length, i] = \\\n",
    "                    np.divide(\n",
    "                        trans_distant_matrix[:sentence_length, i], \n",
    "                        sum_d\n",
    "                    )\n",
    "            trans_distant_matrix[sentence_length:, i] = \\\n",
    "                    np.copy(trans_distant_matrix[:sentence_length, i])\n",
    "\n",
    "        return trans_distant_matrix\n",
    "    \n",
    "    def generate_transition_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_matrix = np.zeros((sentence_length, sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_matrix[i][j] = p_\n",
    "        if nomalized:\n",
    "            return self.normalize_matrix(trans_matrix, axis=1)\n",
    "        return trans_matrix\n",
    "        \n",
    "    def __init__(self, max_distance, po=0.3, seed=1402):\n",
    "        np.random.seed(seed)\n",
    "        self.max_distance = max_distance\n",
    "        self.non_negative_set = np.random.randint(\n",
    "                                    low=1, high=100, \n",
    "                                    size=[max_distance + max_distance + 3]\n",
    "        )\n",
    "        self.po = po\n",
    "        \n",
    "    def calc_forward_messages(self, unary_matrix, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the forward messages ~ alpha values.\n",
    "        \n",
    "        \n",
    "        Input\n",
    "        -----\n",
    "        unary_matrix: emission posteriors - marginal probabilities ~ initial matrix.\n",
    "                      size ~ [1, target_len]\n",
    "        transition_matrix: size ~ [target_len, target_len]\n",
    "        emission_matrix: size ~ [target_len, source_len]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        alpha = np.zeros(np.shape(emission_matrix))\n",
    "        print(\"emission_matrix[:,0]\", emission_matrix[:, 0])\n",
    "        print(\"unary_matrix\", unary_matrix)\n",
    "        alpha.T[0] = np.multiply(emission_matrix[:,0], unary_matrix)\n",
    "        print(\"alpha.T[0]\", alpha.T[0])\n",
    "        \n",
    "        for t in np.arange(1, source_len):\n",
    "            for i in range(target_len):\n",
    "                sum_al = 0.0;\n",
    "#                 print(\"alpha : \", t, i, \" :: \", emission_matrix[i][t])\n",
    "                for j in range(target_len):\n",
    "                    sum_al += alpha[j][t-1] * transition_matrix[j][i]\n",
    "#                     print(\"   sum_al: \", t, i, j, alpha[j][t-1], transition_matrix[j][i])\n",
    "\n",
    "                alpha[i][t] = emission_matrix[i][t] * sum_al\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def calc_backward_messages(self, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the backward messages ~ beta values.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        beta\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        beta = np.zeros(np.shape(emission_matrix))\n",
    "        beta[:,-1] = [1]*target_len\n",
    "\n",
    "        for t in reversed(range(source_len-1)):\n",
    "            for i in range(target_len):\n",
    "    #             print(\"beta \", t, i)\n",
    "                for j in range(target_len):\n",
    "                    beta[i][t] += beta[j][t+1] * transition_matrix[i][j] * emission_matrix[j][t+1]\n",
    "    #                 print(\"    \", beta[t+1][j], transition_matrix[i][j], emission_matrix[ observation_sentence[t+1] ][j], beta[t][i])\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def calc_posterior_matrix(self, alpha, beta, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the gama and epsilon values in order to reproduce \n",
    "        better transition and emission matrix.\n",
    "        \n",
    "        gamma: P(e_aj|f_j)\n",
    "        epsilon: P(e_aj,e_a(j+1)|f_j)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        unary_matrix, posterior_gamma, posterior_epsilon\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(alpha)[1]\n",
    "        target_len = np.shape(alpha)[0]\n",
    "\n",
    "        gamma = np.multiply(alpha, beta)\n",
    "        epsilon = np.zeros((source_len, target_len, target_len))\n",
    "\n",
    "        # Normalization on columns\n",
    "        gamma = self.normalize_matrix(gamma, axis=0)\n",
    "\n",
    "        for t in range(source_len-1):   \n",
    "            for i in range(target_len):\n",
    "                for j in range(target_len):\n",
    "                    epsilon[t][i][j] = alpha[i][t] * transition_matrix[i][j] * \\\n",
    "                                        beta[j][t+1] * emission_matrix[j][t+1]\n",
    "            # Normalization\n",
    "            epsilon[t] = self.normalize_matrix(epsilon[t], whole_matrix=True)\n",
    "\n",
    "        # Update unary matrix\n",
    "        # Normalization unary\n",
    "        new_unary_matrix = np.copy(gamma[:,0])#self.normalize_matrix(np.copy(gamma[:,0]), axis=1)\n",
    "\n",
    "#         new_transition_matrix = np.zeros( (latent_indice_len, latent_indice_len) )\n",
    "#         new_emission_matrix = np.zeros( (observation_len, latent_indice_len) )\n",
    "            \n",
    "#         # Update emission matrix\n",
    "#         sum_gamma = [np.sum(gamma.T[i]) for i in range(latent_indice_len)]   \n",
    "#         for i in range(latent_indice_len):\n",
    "#             new_emission_matrix.T[i] = np.divide(gamma.T[i], sum_gamma[i])\n",
    "\n",
    "        return new_unary_matrix, gamma, epsilon\n",
    "\n",
    "\n",
    "    def calculate_baum_welch_posteriors(self, sentence_length, emission_matrix, unary_matrix=None):\n",
    "        if unary_matrix == None:\n",
    "            unary_matrix = [0.01]*sentence_length\n",
    "            unary_matrix[0] = 1 - np.sum(unary_matrix) + 0.01\n",
    "        transition_matrix = self.generate_transition_matrix(sentence_length)\n",
    "        alpha = self.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "        beta = self.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "        new_unary_matrix, emission_posterior, transition_posterior = self.calc_posterior_matrix(alpha, beta, transition_matrix, emission_matrix)\n",
    "        return emission_posterior, transition_posterior # gamma, epsilon\n",
    "    \n",
    "    def update_non_negative_transition_set(self, emission_posteriors, transition_posteriors):\n",
    "        pass\n",
    "        # TODO 1: update non-negative set: s[-1] = \n",
    "        # TODO 1.1: calculate new transition matrix\n",
    "        transition_list = np.array([])\n",
    "        for gamma, epsilon in zip(emission_posteriors, transition_posteriors):\n",
    "            source_len = np.shape(gamma)[1]\n",
    "            target_len = np.shape(gamma)[0]\n",
    "            new_transition_matrix = np.zeros((target_len, target_len))\n",
    "\n",
    "            for i in range(target_len):\n",
    "                sum_gamma = np.sum(gamma[i][:-1])\n",
    "                for j in range(target_len):\n",
    "                    sum_ep = np.sum(epsilon[:-1][i][j])\n",
    "                    new_transition_matrix[i][j] = sum_ep/sum_gamma\n",
    "            # Normalization\n",
    "            new_transition_matrix = self.normalize_matrix(new_transition_matrix, axis=1)\n",
    "            transition_list.append(new_transition_matrix)\n",
    "            \n",
    "        # TODO 1.2: update\n",
    "        new_non_negative_set = np.zeros(max_distance)\n",
    "        \n",
    "        return new_non_negative_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_negative_set [58 48 56 10 59 11 62]\n",
      "transition_matrix [[ 0.05813953  0.3255814   0.27906977  0.3372093 ]\n",
      " [ 0.34104046  0.05780347  0.32369942  0.27745665]\n",
      " [ 0.08088235  0.43382353  0.07352941  0.41176471]\n",
      " [ 0.43661972  0.07746479  0.41549296  0.07042254]]\n",
      "emission_matrix [[ 0.22222222  0.33333333  0.26923077  0.07692308  0.23529412  0.4       ]\n",
      " [ 0.33333333  0.33333333  0.15384615  0.15384615  0.41176471  0.2       ]\n",
      " [ 0.11111111  0.2         0.30769231  0.69230769  0.11764706  0.15      ]\n",
      " [ 0.33333333  0.13333333  0.26923077  0.07692308  0.23529412  0.25      ]]\n",
      "alpha [[  2.15555556e-01   5.07145654e-03   3.69359339e-03   2.08747262e-04\n",
      "    1.19557985e-04   1.18326645e-04]\n",
      " [  3.33333333e-03   2.37045996e-02   1.41985426e-03   5.33144041e-04\n",
      "    4.69317390e-04   2.29307705e-05]\n",
      " [  1.11111111e-03   1.25401425e-02   4.34674510e-03   2.34877976e-03\n",
      "    6.13782685e-05   4.61472491e-05]\n",
      " [  3.33333333e-03   9.90726028e-03   3.80919096e-03   2.84427012e-04\n",
      "    2.83644837e-04   5.39449218e-05]]\n",
      "beta [[ 0.00107805  0.00452427  0.01942338  0.06766259  0.21453488  1.        ]\n",
      " [ 0.00108279  0.00484332  0.02108511  0.0500518   0.26589595  1.        ]\n",
      " [ 0.00112636  0.00492989  0.00929125  0.07976302  0.23308824  1.        ]\n",
      " [ 0.00123278  0.00421653  0.02606408  0.04639017  0.27007042  1.        ]]\n",
      "new_unary_matrix:  [ 0.96283366  0.01495464  0.00518545  0.01702626]\n",
      "gamma:  [[ 0.96283366  0.09506816  0.29725378  0.0585225   0.10627471  0.49027076]\n",
      " [ 0.01495464  0.47569605  0.12404325  0.11056501  0.51704914  0.09501061]\n",
      " [ 0.00518545  0.25614949  0.16733684  0.77624234  0.0592773   0.19120501]\n",
      " [ 0.01702626  0.1730863   0.41136613  0.05467015  0.31739885  0.22351363]]\n",
      "epsilon:  [[[  7.83090416e-02   4.69455540e-01   2.45749714e-01   1.69319363e-01]\n",
      "  [  7.10339212e-03   1.28886761e-03   4.40800096e-03   2.15437717e-03]\n",
      "  [  5.61555135e-04   3.22438032e-03   3.33764078e-04   1.06574786e-03]\n",
      "  [  9.09417329e-03   1.72726413e-03   5.65800626e-03   5.46812281e-04]]\n",
      "\n",
      " [[  6.38862283e-03   2.21926062e-02   1.67644584e-02   4.97224747e-02]\n",
      "  [  1.75162655e-01   1.84163068e-02   9.08905178e-02   1.91226572e-01]\n",
      "  [  2.19765359e-02   7.31192276e-02   1.09221364e-02   1.50131586e-01]\n",
      "  [  9.37259617e-02   1.03151116e-02   4.87597274e-02   2.02854996e-02]]\n",
      "\n",
      " [[  4.63104877e-03   3.83679394e-02   2.35839239e-01   1.84155492e-02]\n",
      "  [  1.04425940e-02   2.61852954e-03   1.05157410e-01   5.82471858e-03]\n",
      "  [  7.58187581e-03   6.01640534e-02   7.31272857e-02   2.64636251e-02]\n",
      "  [  3.58669796e-02   9.41449213e-03   3.62118407e-01   3.96625328e-03]]\n",
      "\n",
      " [[  2.53836909e-03   3.08315001e-02   6.61894005e-03   1.85336889e-02]\n",
      "  [  3.80288242e-02   1.39801794e-02   1.96083636e-02   3.89476473e-02]\n",
      "  [  3.97336538e-02   4.62242302e-01   1.96226771e-02   2.54643708e-01]\n",
      "  [  2.59738671e-02   9.99515349e-03   1.34273211e-02   5.27380453e-03]]\n",
      "\n",
      " [[  1.15202942e-02   3.22568238e-02   2.07365296e-02   4.17610665e-02]\n",
      "  [  2.65268687e-01   2.24803972e-02   9.44176682e-02   1.34882383e-01]\n",
      "  [  8.22776430e-03   2.20653679e-02   2.80491965e-03   2.61792500e-02]\n",
      "  [  2.05254014e-01   1.82080174e-02   7.32458882e-02   2.06909289e-02]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "target_length = 4\n",
    "max_distance = 2\n",
    "\n",
    "baum_welch_model = BaumWelchModel(max_distance)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)\n",
    "unary_matrix = [0.97, .01, .01, .01]\n",
    "# transition_matrix = np.array([\n",
    "#     [.3, .7], \n",
    "#     [.1, .9]\n",
    "# ])\n",
    "transition_matrix = baum_welch_model.generate_transition_matrix(target_length, nomalized=True)\n",
    "print(\"transition_matrix\", transition_matrix)\n",
    "# print(\"unnormalized transition\", baum_welch_model.generate_transition_matrix(target_length, nomalized=False))\n",
    "emission_matrix = baum_welch_model.normalize_matrix(np.array([\n",
    "    [.4, .5, .7, .1, .4, .8],\n",
    "    [.6, .5, .4, .2, .7, .4],\n",
    "    [.2, .3, .8, .9, .2, .3],\n",
    "    [.6, .2, .7, .1, .4, .5]\n",
    "]), axis=0)\n",
    "print(\"emission_matrix\", emission_matrix)\n",
    "\n",
    "alpha = baum_welch_model.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "beta = baum_welch_model.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "print(\"alpha\", alpha)\n",
    "print(\"beta\", beta)\n",
    "\n",
    "new_unary_matrix, emission_posterior, transition_posterior = baum_welch_model.calc_posterior_matrix(alpha, beta)\n",
    "print(\"new_unary_matrix: \", new_unary_matrix)\n",
    "print(\"gamma: \", emission_posterior)\n",
    "print(\"epsilon: \", transition_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_posterior [[ 0.96283366  0.09506816  0.29725378  0.0585225   0.10627471  0.49027076]\n",
      " [ 0.01495464  0.47569605  0.12404325  0.11056501  0.51704914  0.09501061]\n",
      " [ 0.00518545  0.25614949  0.16733684  0.77624234  0.0592773   0.19120501]\n",
      " [ 0.01702626  0.1730863   0.41136613  0.05467015  0.31739885  0.22351363]]\n",
      "transition_posterior [[[  7.83090416e-02   4.69455540e-01   2.45749714e-01   1.69319363e-01]\n",
      "  [  7.10339212e-03   1.28886761e-03   4.40800096e-03   2.15437717e-03]\n",
      "  [  5.61555135e-04   3.22438032e-03   3.33764078e-04   1.06574786e-03]\n",
      "  [  9.09417329e-03   1.72726413e-03   5.65800626e-03   5.46812281e-04]]\n",
      "\n",
      " [[  6.38862283e-03   2.21926062e-02   1.67644584e-02   4.97224747e-02]\n",
      "  [  1.75162655e-01   1.84163068e-02   9.08905178e-02   1.91226572e-01]\n",
      "  [  2.19765359e-02   7.31192276e-02   1.09221364e-02   1.50131586e-01]\n",
      "  [  9.37259617e-02   1.03151116e-02   4.87597274e-02   2.02854996e-02]]\n",
      "\n",
      " [[  4.63104877e-03   3.83679394e-02   2.35839239e-01   1.84155492e-02]\n",
      "  [  1.04425940e-02   2.61852954e-03   1.05157410e-01   5.82471858e-03]\n",
      "  [  7.58187581e-03   6.01640534e-02   7.31272857e-02   2.64636251e-02]\n",
      "  [  3.58669796e-02   9.41449213e-03   3.62118407e-01   3.96625328e-03]]\n",
      "\n",
      " [[  2.53836909e-03   3.08315001e-02   6.61894005e-03   1.85336889e-02]\n",
      "  [  3.80288242e-02   1.39801794e-02   1.96083636e-02   3.89476473e-02]\n",
      "  [  3.97336538e-02   4.62242302e-01   1.96226771e-02   2.54643708e-01]\n",
      "  [  2.59738671e-02   9.99515349e-03   1.34273211e-02   5.27380453e-03]]\n",
      "\n",
      " [[  1.15202942e-02   3.22568238e-02   2.07365296e-02   4.17610665e-02]\n",
      "  [  2.65268687e-01   2.24803972e-02   9.44176682e-02   1.34882383e-01]\n",
      "  [  8.22776430e-03   2.20653679e-02   2.80491965e-03   2.61792500e-02]\n",
      "  [  2.05254014e-01   1.82080174e-02   7.32458882e-02   2.06909289e-02]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "emission_posterior, transition_posterior = \\\n",
    "    baum_welch_model.calculate_baum_welch_posteriors(target_length, emission_matrix, unary_matrix)\n",
    "print(\"emission_posterior\", emission_posterior)\n",
    "print(\"transition_posterior\", transition_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with Unsupervised neural hidden markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_negative_set [29 56 82 13 35 53 25 23 21 12 15  9 13 87  9 63 62 52 43]\n"
     ]
    }
   ],
   "source": [
    "# BW model variables\n",
    "max_distance = 8\n",
    "baum_welch_model = BaumWelchModel(max_distance, seed=1111)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "    ]).astype(config.floatX)\n",
    "\n",
    "# posteriors = np.asarray([\n",
    "#     [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "#     [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "#     [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "#     [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "#     [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "# ]).astype(config.floatX)\n",
    "\n",
    "# Emission model variables\n",
    "vocab_input_size = 10\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = 12\n",
    "emission_model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                               vocab_output_size=vocab_output_size, baum_welch_model=baum_welch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_input [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] (10, 5)\n",
      "emission_matrix 1 [[  1.00170076e-02   1.20083610e-10   1.39718434e-15   4.68368556e-17\n",
      "    7.48935809e-12]\n",
      " [  1.22934855e-12   7.03421055e-09   1.19096011e-09   2.06476147e-09\n",
      "    2.72379169e-04]\n",
      " [  2.44000323e-11   1.05879911e-13   3.30180978e-12   3.29048344e-05\n",
      "    7.82270106e-08]\n",
      " [  1.74506021e-10   5.58641628e-16   5.26185981e-14   5.10033259e-13\n",
      "    8.75874785e-13]\n",
      " [  1.14724586e-04   1.34182380e-06   9.58455260e-11   9.28782995e-10\n",
      "    1.30416092e-05]\n",
      " [  8.76298368e-01   6.43898308e-01   2.79844013e-11   4.83316711e-08\n",
      "    3.32501895e-06]\n",
      " [  3.90381893e-10   8.50837387e-06   5.96516020e-06   3.58708603e-05\n",
      "    2.44137453e-04]\n",
      " [  3.38336337e-09   8.17109505e-11   1.03117272e-11   8.37082140e-11\n",
      "    1.07731218e-07]\n",
      " [  1.48342166e-03   1.33874192e-08   1.30569804e-13   1.14827958e-09\n",
      "    1.09271004e-05]\n",
      " [  6.92231323e-16   6.27401890e-20   1.05678456e-15   1.24847452e-10\n",
      "    1.28488304e-08]\n",
      " [  1.40317807e-05   7.53151819e-10   2.50952876e-08   1.97411268e-10\n",
      "    1.23392374e-05]\n",
      " [  1.12072520e-01   3.56091917e-01   9.99994040e-01   9.99931216e-01\n",
      "    9.99443591e-01]] (12, 5)\n",
      "new_emission_matrix [array([  2.44000323e-11,   1.05879911e-13,   3.30180978e-12,\n",
      "         3.29048344e-05,   7.82270106e-08], dtype=float32), array([  1.40317807e-05,   7.53151819e-10,   2.50952876e-08,\n",
      "         1.97411268e-10,   1.23392374e-05], dtype=float32), array([  3.38336337e-09,   8.17109505e-11,   1.03117272e-11,\n",
      "         8.37082140e-11,   1.07731218e-07], dtype=float32), array([  1.00170076e-02,   1.20083610e-10,   1.39718434e-15,\n",
      "         4.68368556e-17,   7.48935809e-12], dtype=float32)] (4, 5)\n",
      "emission_matrix[:,0] [  2.44000323e-11   1.05879911e-13   3.30180978e-12   3.29048344e-05\n",
      "   7.82270106e-08]\n",
      "unary_matrix [0.95999999999999996, 0.01, 0.01, 0.01, 0.01]\n",
      "alpha.T[0] [  2.34240310e-11   1.05879911e-15   3.30180978e-14   3.29048344e-07\n",
      "   7.82270106e-10]\n",
      "emission_posterior [[  4.86867663e-05   7.54211452e-01   8.30507859e-03   9.99999998e-01]\n",
      " [  2.71174819e-09   1.83684671e-05   1.02993518e-04   1.95257710e-09]\n",
      " [  6.90604571e-08   1.12396457e-03   8.50479518e-06   1.65711797e-14]\n",
      " [  9.93653572e-01   7.39338856e-06   1.47209383e-04   8.81624366e-16]\n",
      " [  6.29766953e-03   2.44638822e-01   9.91436214e-01   1.29990227e-10]] (5, 4)\n",
      "emission_posterior_vout [[  1.00000000e+00   0.00000000e+00   4.86867648e-05   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   8.30507837e-03\n",
      "    0.00000000e+00   0.00000000e+00   7.54211426e-01   0.00000000e+00]\n",
      " [  1.95257699e-09   0.00000000e+00   2.71174816e-09   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.02993516e-04\n",
      "    0.00000000e+00   0.00000000e+00   1.83684679e-05   0.00000000e+00]\n",
      " [  1.65711798e-14   0.00000000e+00   6.90604551e-08   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   8.50479501e-06\n",
      "    0.00000000e+00   0.00000000e+00   1.12396455e-03   0.00000000e+00]\n",
      " [  8.81624343e-16   0.00000000e+00   9.93653595e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.47209386e-04\n",
      "    0.00000000e+00   0.00000000e+00   7.39338839e-06   0.00000000e+00]\n",
      " [  1.29990227e-10   0.00000000e+00   6.29766937e-03   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   9.91436243e-01\n",
      "    0.00000000e+00   0.00000000e+00   2.44638816e-01   0.00000000e+00]] (5, 12)\n"
     ]
    }
   ],
   "source": [
    "testing_target = [6, 8, 2, 1, 3]\n",
    "testing_source = [2, 10, 7, 0]\n",
    "result = emission_model.train_mini_batch(testing_target, testing_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
