{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import theano.tensor as T\n",
    "from theano import function, printing\n",
    "import theano\n",
    "\n",
    "from theano import config\n",
    "# config.device = 'cpu'\n",
    "config.mode = 'DebugMode'\n",
    "config.gcc.cxxflags = \"-D_hypot=hypot\"\n",
    "config.compute_test_value = 'raise'\n",
    "\n",
    "\n",
    "# updates = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evolution:\n",
    "\n",
    "    def calculate_AER(self, S, P, A):\n",
    "        s_a, p_a, len_s, len_a = 0, 0, 0, 0\n",
    "        for s, p, a in zip(S, P, A):\n",
    "            s_a += len(list(set(s).intersection(a)))\n",
    "            p_a += len(list(set(p).intersection(a)))\n",
    "            len_s += len(s)\n",
    "            len_a += len(a)\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a += s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len_s + len_a)\n",
    "        print (\"aer\", 1.-aer)\n",
    "\n",
    "        return 1. - aer \n",
    "\n",
    "\n",
    "    def calculate_one_AER(self, S, P, A):\n",
    "        s_a = len(list(set(S).intersection(A)))\n",
    "        print (\"s_a\", s_a)\n",
    "        p_a = len(list(set(P).intersection(A))) + s_a\n",
    "        print (\"p_a\", p_a)\n",
    "        aer = (s_a + p_a) / (len(S) + len(A))\n",
    "        print (\"aer\", 1.-aer)\n",
    "\n",
    "        return 1. - aer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(9)\n",
    "np.split(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3c9d64aeda2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmissionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_input_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_input_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3c9d64aeda2b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_input_size, layer_size, output_size, epoch, batch, learning_rate, seed)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[1;31m# ReLU layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mz_relu_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# [512, 7] * [7, 5] = [512, 5]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mz_relu_layer_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_relu_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mz_reshaped_relu_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_relu_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mz_relu_layer_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mz_relu_layer_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\tensor\\var.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[1;31m# We should catch the minimum number of exception here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[1;31m# Otherwise this will convert error when Theano flags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[1;31m# compute output value once with test inputs to validate graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 thunk = node.op.make_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 670\u001b[0;31m                                            no_recycling=[])\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mthunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mthunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 935\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    936\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[1;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 839\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0mfill_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cc.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         cthunk, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1189\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cc.py\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                                     \u001b[0moutput_storage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                     \u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                                     keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return (thunk,\n\u001b[1;32m   1133\u001b[0m                 [link.Container(input, storage) for input, storage in\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cc.py\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m             module = get_module_cache().module_from_key(\n\u001b[0;32m-> 1586\u001b[0;31m                 key=key, lnk=self, keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0mvars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morphans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cmodule.py\u001b[0m in \u001b[0;36mmodule_from_key\u001b[0;34m(self, key, lnk, keep_lock)\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                 \u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlimport_workdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlnk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_cmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cc.py\u001b[0m in \u001b[0;36mcompile_cmodule\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mlib_dirs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_dirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 \u001b[0mlibs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlibs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m                 preargs=preargs)\n\u001b[0m\u001b[1;32m   1490\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cmodule.py\u001b[0m in \u001b[0;36mcompile_str\u001b[0;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\u001b[0m\n\u001b[1;32m   2323\u001b[0m             \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__init__.py\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mdlimport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda2\\envs\\python3_5\\lib\\site-packages\\theano\\gof\\cmodule.py\u001b[0m in \u001b[0;36mdlimport\u001b[0;34m(fullpath, suffix)\u001b[0m\n\u001b[1;32m    300\u001b[0m             warnings.filterwarnings(\"ignore\",\n\u001b[1;32m    301\u001b[0m                                     message=\"numpy.ndarray size changed\")\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mimport_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: DLL load failed: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "class EmissionModel:\n",
    "    \"\"\" Simple emission model without CNN\n",
    "    word embedding layer -> ReLU layer -> softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def init_weights_bias(self, vocab_input_size, layer_size, output_size, seed=1402):\n",
    "        random_state = np.random.RandomState(seed)\n",
    "        \n",
    "        size_list = np.concatenate(([vocab_input_size], layer_size, [output_size]), axis=0)\n",
    "        w = []\n",
    "        b = []\n",
    "        \n",
    "        for i in range(len(size_list) - 1):\n",
    "            w.append(\n",
    "                theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], size_list[i])), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True\n",
    "                )\n",
    "            )\n",
    "            b.append(\n",
    "                theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], 1)), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return w, b\n",
    "    \n",
    "    #[7,512]\n",
    "    def __init__(self, vocab_input_size, layer_size, output_size, epoch=1, batch=1, learning_rate = .01, seed=1412):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch = batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.posteriors = []\n",
    "        \n",
    "        x_training_input = T.matrix().astype(config.floatX)\n",
    "        x_training_input.tag.test_value = np.asarray([\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  1.,  0.],\n",
    "            [ 0.,  0.,  1.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  1.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 1.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  1.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.]\n",
    "        ]).astype(x_training_input.dtype)\n",
    "        \n",
    "        self.w, self.b = self.init_weights_bias(vocab_input_size, layer_size, output_size, seed)\n",
    "        \n",
    "        # word embedding layer\n",
    "        word_embedding_layer = T.dot(self.w[0], x_training_input) # [7, 10] * [10, 5] = [7, 5]\n",
    "        \n",
    "        # ReLU layer\n",
    "        z_relu_layer = T.dot(self.w[1], word_embedding_layer) + self.b[1] # [512, 7] * [7, 5] = [512, 5]\n",
    "        z_relu_layer_shape = T.shape(z_relu_layer)\n",
    "        z_reshaped_relu_layer = T.reshape(z_relu_layer, [z_relu_layer_shape[0]*z_relu_layer_shape[1], 1])\n",
    "        relu_layer = T.nnet.relu(z_reshaped_relu_layer)\n",
    "        relu_layer_reshaped = T.reshape(relu_layer, z_relu_layer_shape) # [512, 5]\n",
    "        \n",
    "        # Softmax layer\n",
    "        z_softmax_layer = T.dot(self.w[2], relu_layer_reshaped) + self.b[2] # [9, 512] * [512, 5] = [9, 5]\n",
    "        softmax_layer = T.transpose(T.nnet.softmax(T.transpose(z_softmax_layer))) # [9, 5]\n",
    "        \n",
    "        # calculate new gradient\n",
    "        posteriors = T.matrix().astype(config.floatX)\n",
    "        posteriors.tag.test_value = np.asarray([\n",
    "            [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62],\n",
    "            [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55],\n",
    "            [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05],\n",
    "            [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51],\n",
    "            [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11]\n",
    "        ]).astype(posteriors.dtype)\n",
    "        \n",
    "        cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer))\n",
    "        # TODO: use dw[] and db[] abstractly \n",
    "        dw0,dw1,dw2,db1,db2 = T.grad(\n",
    "            cost=cost, wrt=[self.w[0],self.w[1],self.w[2],self.b[1],self.b[2]]\n",
    "        )\n",
    "\n",
    "        # Update w and b\n",
    "        updates = [\n",
    "            (self.w[0], self.w[0] - self.learning_rate * dw0), \n",
    "            (self.w[1], self.w[1] - self.learning_rate * dw1), \n",
    "            (self.b[1], self.b[1] - self.learning_rate * db1),\n",
    "            (self.w[2], self.w[2] - self.learning_rate * dw2), \n",
    "            (self.b[2], self.b[2] - self.learning_rate * db2)\n",
    "        ]\n",
    "        \n",
    "        # Compile model\n",
    "        self.test = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw1, softmax_layer]\n",
    "        ) \n",
    "        self.train_mini_batch = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=[dw2, self.w[2], softmax_layer], \n",
    "            updates=updates\n",
    "        )\n",
    "        self.test_values = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=[softmax_layer]\n",
    "        )\n",
    "        \n",
    "    def train_model(inputs):\n",
    "        pass\n",
    "#         for i in range(self.epoch):\n",
    "#             for x_input in np.split(inputs, self.batch):\n",
    "#                 self.posteriors = \n",
    "#                 self.train_mini_batch(x_input, posteriors)\n",
    "            # TODO: create train_batch function \n",
    "\n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "    ]).astype(config.floatX)\n",
    "\n",
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "vocab_input_size = np.shape(x)[0]\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "output_size = 9\n",
    "\n",
    "model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, output_size=output_size)\n",
    "\n",
    "result = model.train_mini_batch(x, posteriors)\n",
    "print(np.shape(result[0]))\n",
    "print(np.shape(result[1]))\n",
    "print(np.shape(result[2]))\n",
    "# print(np.shape(model.evaluate_model(x)))\n",
    "# print(model.calculate_gradient(posteriors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.25463021e-01  -1.74731529e+00  -3.40455741e-01 ...,   8.40714455e-01\n",
      "    1.16684544e+00  -1.74363041e+00]\n",
      " [ -1.73396075e+00  -3.52505970e+00  -1.80466461e+00 ...,   1.56047273e+00\n",
      "    2.06868815e+00  -2.42936778e+00]\n",
      " [ -5.50276101e-01   5.80202416e-02  -1.51815784e+00 ...,   9.53709185e-01\n",
      "    2.68967301e-01   3.69657218e-01]\n",
      " ..., \n",
      " [ -9.68661189e-01   5.42034388e-01   1.15126371e+00 ...,  -8.52006793e-01\n",
      "   -6.70304239e-01   4.77848887e-01]\n",
      " [  3.67000699e-01   4.21549737e-01   8.97033513e-03 ...,   2.57458866e-01\n",
      "    1.96427971e-01   7.65285119e-02]\n",
      " [ -4.27518426e-05  -1.21131372e-02   3.77787501e-02 ...,  -8.82423893e-02\n",
      "    4.54101712e-03   5.87681048e-02]]\n",
      "\n",
      "[[ -1.84710197e+01  -7.45869827e+01  -6.41599178e+00  -2.60746651e+01\n",
      "   -1.36473475e+01]\n",
      " [ -3.23260193e+01  -8.30433655e+01  -8.10556221e+00  -3.15460014e+01\n",
      "   -2.47860146e+01]\n",
      " [ -4.28678932e+01  -1.31970825e+02  -5.94393015e+00  -4.57771158e+00\n",
      "   -1.53612356e+01]\n",
      " [ -3.80866966e+01  -1.26840454e+02  -1.99077091e+01  -2.30702705e+01\n",
      "   -1.82781239e+01]\n",
      " [ -7.54507494e+00  -6.50036545e+01  -6.28632164e+00  -4.02389079e-01\n",
      "   -1.08189918e-02]\n",
      " [ -2.83696628e+00   0.00000000e+00  -2.49587297e+00  -1.13631201e+00\n",
      "   -1.17951384e+01]\n",
      " [ -4.67132912e+01  -9.42796783e+01  -1.91201077e+01  -1.46994944e+01\n",
      "   -9.69219494e+00]\n",
      " [ -1.10600939e+01  -9.10002518e+01  -1.04452059e-01  -2.24838829e+01\n",
      "   -7.43489122e+00]\n",
      " [ -6.09690063e-02  -6.65967712e+01  -4.57201195e+00  -2.98846054e+01\n",
      "   -4.59525347e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(model.test(x, posteriors)[0])\n",
    "print(\"\")\n",
    "print(model.test(x, posteriors)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -4.25463021e-01,  -1.74731529e+00,  -3.40455741e-01, ...,\n",
      "          8.40714455e-01,   1.16684544e+00,  -1.74363041e+00],\n",
      "       [ -1.73396075e+00,  -3.52505970e+00,  -1.80466461e+00, ...,\n",
      "          1.56047273e+00,   2.06868815e+00,  -2.42936778e+00],\n",
      "       [ -5.50276101e-01,   5.80202416e-02,  -1.51815784e+00, ...,\n",
      "          9.53709185e-01,   2.68967301e-01,   3.69657218e-01],\n",
      "       ..., \n",
      "       [ -9.68661189e-01,   5.42034388e-01,   1.15126371e+00, ...,\n",
      "         -8.52006793e-01,  -6.70304239e-01,   4.77848887e-01],\n",
      "       [  3.67000699e-01,   4.21549737e-01,   8.97033513e-03, ...,\n",
      "          2.57458866e-01,   1.96427971e-01,   7.65285119e-02],\n",
      "       [ -4.27518426e-05,  -1.21131372e-02,   3.77787501e-02, ...,\n",
      "         -8.82423893e-02,   4.54101712e-03,   5.87681048e-02]], dtype=float32), array([[ -1.84710197e+01,  -7.45869827e+01,  -6.41599178e+00,\n",
      "         -2.60746651e+01,  -1.36473475e+01],\n",
      "       [ -3.23260193e+01,  -8.30433655e+01,  -8.10556221e+00,\n",
      "         -3.15460014e+01,  -2.47860146e+01],\n",
      "       [ -4.28678932e+01,  -1.31970825e+02,  -5.94393015e+00,\n",
      "         -4.57771158e+00,  -1.53612356e+01],\n",
      "       [ -3.80866966e+01,  -1.26840454e+02,  -1.99077091e+01,\n",
      "         -2.30702705e+01,  -1.82781239e+01],\n",
      "       [ -7.54507494e+00,  -6.50036545e+01,  -6.28632164e+00,\n",
      "         -4.02389079e-01,  -1.08189918e-02],\n",
      "       [ -2.83696628e+00,   0.00000000e+00,  -2.49587297e+00,\n",
      "         -1.13631201e+00,  -1.17951384e+01],\n",
      "       [ -4.67132912e+01,  -9.42796783e+01,  -1.91201077e+01,\n",
      "         -1.46994944e+01,  -9.69219494e+00],\n",
      "       [ -1.10600939e+01,  -9.10002518e+01,  -1.04452059e-01,\n",
      "         -2.24838829e+01,  -7.43489122e+00],\n",
      "       [ -6.09690063e-02,  -6.65967712e+01,  -4.57201195e+00,\n",
      "         -2.98846054e+01,  -4.59525347e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "posteriors = np.asarray([\n",
    "    [ 0.65, -0.32,  0.44, -0.04, -0.36, -0.81,  0.38, -0.84, -0.93],\n",
    "    [-0.41, -0.05,  0.96,  0.71,  0.08,  0.85,  0.12,  0.43, -0.08],\n",
    "    [-0.45,  0.04, -0.94,  0.41,  0.04, -0.3 ,  0.89, -0.09, -0.42],\n",
    "    [-0.19,  0.32,  0.  ,  0.02, -0.66, -0.41,  0.11, -0.05,  0.76],\n",
    "    [-0.32,  0.86,  0.09, -0.41, -0.57, -0.55, -0.85, -0.09, -0.27]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "print(model.test(x, posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaumWelchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.  69.  76.  58.   0.   0.   0.   0.]\n",
      " [ 87.  63.  69.  76.   0.   0.   0.   0.]\n",
      " [ 93.  87.  63.  69.   0.   0.   0.   0.]\n",
      " [ 53.  93.  87.  63.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ],\n",
       "       [ 0.21283784,  0.22115385,  0.25762712,  0.21804511,  0.3       ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29391892,  0.20192308,  0.23389831,  0.28571429,  0.        ,\n",
       "         0.3       ,  0.        ,  0.        ],\n",
       "       [ 0.31418919,  0.27884615,  0.21355932,  0.2593985 ,  0.        ,\n",
       "         0.        ,  0.3       ,  0.        ],\n",
       "       [ 0.17905405,  0.29807692,  0.29491525,  0.23684211,  0.        ,\n",
       "         0.        ,  0.        ,  0.3       ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix and \"Baum Welch Algorithm\"\n",
    "\n",
    "Compute forward messages: alpha <br>\n",
    "Compute backward messages: beta <br>\n",
    "Compute posteriors: <br>\n",
    "    p(z|x) = alpha * beta <br>\n",
    "    p(z_i, z_i+1 | x) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaumWelchModel:\n",
    "    \n",
    "    def normalize_matrix(self, x, axis=1, whole_matrix=False):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\n",
    "            axis=1: row\n",
    "            axis=0: column \n",
    "        Input\n",
    "        -----\n",
    "        \n",
    "        Output\n",
    "        ------\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) == 1 or whole_matrix:\n",
    "#             e_x = np.exp(x - np.max(x))\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x)\n",
    "        if axis == 0:\n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[None, :]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[None, :]\n",
    "        else: \n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[:, None]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[:, None]\n",
    "        \n",
    "    def generate_transition_distant_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "        We extend the latent sentence for 2*length in which each word has \n",
    "        an empty word to represent no-alignment state.\n",
    "        where [sentence_length:end] elements are empty words considered as \n",
    "        latent words having no direct aligment.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_distant_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_distant_matrix = np.zeros((2*sentence_length, 2*sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_distant_matrix[i][j] = p_\n",
    "\n",
    "        print(trans_distant_matrix)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            trans_distant_matrix[i+sentence_length][i+sentence_length] = po\n",
    "            trans_distant_matrix[i][i+sentence_length] = po\n",
    "\n",
    "            sum_d = np.sum(trans_distant_matrix[:sentence_length, i])\n",
    "            trans_distant_matrix[:sentence_length, i] = \\\n",
    "                    np.divide(\n",
    "                        trans_distant_matrix[:sentence_length, i], \n",
    "                        sum_d\n",
    "                    )\n",
    "            trans_distant_matrix[sentence_length:, i] = \\\n",
    "                    np.copy(trans_distant_matrix[:sentence_length, i])\n",
    "\n",
    "        return trans_distant_matrix\n",
    "    \n",
    "    def generate_transition_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_matrix = np.zeros((sentence_length, sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_matrix[i][j] = p_\n",
    "        if nomalized:\n",
    "            return self.normalize_matrix(trans_matrix, axis=1)\n",
    "        return trans_matrix\n",
    "        \n",
    "    def __init__(self, max_distance, po=0.3, seed=1402):\n",
    "        np.random.seed(seed)\n",
    "        self.max_distance = max_distance\n",
    "        self.non_negative_set = np.random.randint(\n",
    "                                    low=1, high=100, \n",
    "                                    size=[max_distance + max_distance + 3]\n",
    "        )\n",
    "        self.po = po\n",
    "        \n",
    "    def calc_forward_messages(self, unary_matrix, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the forward messages ~ alpha values.\n",
    "        \n",
    "        \n",
    "        Input\n",
    "        -----\n",
    "        unary_matrix: emission posteriors - marginal probabilities ~ initial matrix.\n",
    "                      size ~ [1, target_len]\n",
    "        transition_matrix: size ~ [target_len, target_len]\n",
    "        emission_matrix: size ~ [target_len, source_len]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        alpha = np.zeros(np.shape(emission_matrix))\n",
    "        alpha[:,0] = np.multiply(emission_matrix[:,0], unary_matrix)\n",
    "        \n",
    "        for t in np.arange(1, source_len):\n",
    "            for i in range(target_len):\n",
    "                sum_al = 0.0;\n",
    "#                 print(\"alpha : \", t, i, \" :: \", emission_matrix[i][t])\n",
    "                for j in range(target_len):\n",
    "                    sum_al += alpha[j][t-1] * transition_matrix[j][i]\n",
    "#                     print(\"   sum_al: \", t, i, j, alpha[j][t-1], transition_matrix[j][i])\n",
    "\n",
    "                alpha[i][t] = emission_matrix[i][t] * sum_al\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def calc_backward_messages(self, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the backward messages ~ beta values.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        beta\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        beta = np.zeros(np.shape(emission_matrix))\n",
    "        beta[:,-1] = [1]*target_len\n",
    "\n",
    "        for t in reversed(range(source_len-1)):\n",
    "            for i in range(target_len):\n",
    "    #             print(\"beta \", t, i)\n",
    "                for j in range(target_len):\n",
    "                    beta[i][t] += beta[j][t+1] * transition_matrix[i][j] * emission_matrix[j][t+1]\n",
    "    #                 print(\"    \", beta[t+1][j], transition_matrix[i][j], emission_matrix[ observation_sentence[t+1] ][j], beta[t][i])\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def calc_posterior_matrix(self, alpha, beta):\n",
    "        \"\"\"Calcualte the gama and epsilon values in order to reproduce \n",
    "        better transition and emission matrix.\n",
    "        \n",
    "        gamma: P(e_aj|f_j)\n",
    "        epsilon: P(e_aj,e_a(j+1)|f_j)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        unary_matrix, posterior_gamma, posterior_epsilon\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(alpha)[1]\n",
    "        target_len = np.shape(alpha)[0]\n",
    "\n",
    "        gamma = np.multiply(alpha, beta)\n",
    "        epsilon = np.zeros((source_len, target_len, target_len))\n",
    "\n",
    "        # Normalization on columns\n",
    "        gamma = self.normalize_matrix(gamma, axis=0)\n",
    "\n",
    "        for t in range(source_len-1):   \n",
    "            for i in range(target_len):\n",
    "                for j in range(target_len):\n",
    "                    epsilon[t][i][j] = alpha[i][t] * transition_matrix[i][j] * \\\n",
    "                                        beta[j][t+1] * emission_matrix[j][t+1]\n",
    "            # Normalization\n",
    "            epsilon[t] = self.normalize_matrix(epsilon[t], whole_matrix=True)\n",
    "\n",
    "        # Update unary matrix\n",
    "        # Normalization unary\n",
    "        new_unary_matrix = np.copy(gamma[:,0])#self.normalize_matrix(np.copy(gamma[:,0]), axis=1)\n",
    "\n",
    "#         new_transition_matrix = np.zeros( (latent_indice_len, latent_indice_len) )\n",
    "#         new_emission_matrix = np.zeros( (observation_len, latent_indice_len) )\n",
    "            \n",
    "#         # Update emission matrix\n",
    "#         sum_gamma = [np.sum(gamma.T[i]) for i in range(latent_indice_len)]   \n",
    "#         for i in range(latent_indice_len):\n",
    "#             new_emission_matrix.T[i] = np.divide(gamma.T[i], sum_gamma[i])\n",
    "\n",
    "        return new_unary_matrix, gamma, epsilon\n",
    "\n",
    "\n",
    "    def calculate_baum_welch_posteriors(self, sentence_length, emission_matrix, unary_matrix=None):\n",
    "        if unary_matrix == None:\n",
    "            unary_matrix = [0.1]*target_length\n",
    "            unary_matrix[0] = 1 - np.sum(unary_matrix) + 0.1\n",
    "        transition_matrix = self.generate_transition_matrix(sentence_length)\n",
    "        alpha = self.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "        beta = self.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "        new_unary_matrix, emission_posterior, transition_posterior = self.calc_posterior_matrix(alpha, beta)\n",
    "        return emission_posterior, transition_posterior # gamma, epsilon\n",
    "    \n",
    "    def update_non_negative_transition_set(self, emission_posteriors, transition_posteriors):\n",
    "        pass\n",
    "        # TODO 1: update non-negative set: s[-1] = \n",
    "        # TODO 1.1: calculate new transition matrix\n",
    "        transition_list = np.array([])\n",
    "        for gamma, epsilon in zip(emission_posteriors, transition_posteriors):\n",
    "            source_len = np.shape(gamma)[1]\n",
    "            target_len = np.shape(gamma)[0]\n",
    "            new_transition_matrix = np.zeros((target_len, target_len))\n",
    "\n",
    "            for i in range(target_len):\n",
    "                sum_gamma = np.sum(gamma[i][:-1])\n",
    "                for j in range(target_len):\n",
    "                    sum_ep = np.sum(epsilon[:-1][i][j])\n",
    "                    new_transition_matrix[i][j] = sum_ep/sum_gamma\n",
    "            # Normalization\n",
    "            new_transition_matrix = self.normalize_matrix(new_transition_matrix, axis=1)\n",
    "            transition_list.append(new_transition_matrix)\n",
    "            \n",
    "        # TODO 1.2: update\n",
    "        new_non_negative_set = np.zeros(max_distance)\n",
    "        \n",
    "        return new_non_negative_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_negative_set [58 48 56 10 59 11 62]\n",
      "transition_matrix [[ 0.05813953  0.3255814   0.27906977  0.3372093 ]\n",
      " [ 0.34104046  0.05780347  0.32369942  0.27745665]\n",
      " [ 0.08088235  0.43382353  0.07352941  0.41176471]\n",
      " [ 0.43661972  0.07746479  0.41549296  0.07042254]]\n",
      "emission_matrix [[ 0.22222222  0.33333333  0.26923077  0.07692308  0.23529412  0.4       ]\n",
      " [ 0.33333333  0.33333333  0.15384615  0.15384615  0.41176471  0.2       ]\n",
      " [ 0.11111111  0.2         0.30769231  0.69230769  0.11764706  0.15      ]\n",
      " [ 0.33333333  0.13333333  0.26923077  0.07692308  0.23529412  0.25      ]]\n",
      "alpha [[  2.15555556e-01   5.07145654e-03   3.69359339e-03   2.08747262e-04\n",
      "    1.19557985e-04   1.18326645e-04]\n",
      " [  3.33333333e-03   2.37045996e-02   1.41985426e-03   5.33144041e-04\n",
      "    4.69317390e-04   2.29307705e-05]\n",
      " [  1.11111111e-03   1.25401425e-02   4.34674510e-03   2.34877976e-03\n",
      "    6.13782685e-05   4.61472491e-05]\n",
      " [  3.33333333e-03   9.90726028e-03   3.80919096e-03   2.84427012e-04\n",
      "    2.83644837e-04   5.39449218e-05]]\n",
      "beta [[ 0.00107805  0.00452427  0.01942338  0.06766259  0.21453488  1.        ]\n",
      " [ 0.00108279  0.00484332  0.02108511  0.0500518   0.26589595  1.        ]\n",
      " [ 0.00112636  0.00492989  0.00929125  0.07976302  0.23308824  1.        ]\n",
      " [ 0.00123278  0.00421653  0.02606408  0.04639017  0.27007042  1.        ]]\n",
      "new_unary_matrix:  [ 0.96283366  0.01495464  0.00518545  0.01702626]\n",
      "gamma:  [[ 0.96283366  0.09506816  0.29725378  0.0585225   0.10627471  0.49027076]\n",
      " [ 0.01495464  0.47569605  0.12404325  0.11056501  0.51704914  0.09501061]\n",
      " [ 0.00518545  0.25614949  0.16733684  0.77624234  0.0592773   0.19120501]\n",
      " [ 0.01702626  0.1730863   0.41136613  0.05467015  0.31739885  0.22351363]]\n",
      "epsilon:  [[[  7.83090416e-02   4.69455540e-01   2.45749714e-01   1.69319363e-01]\n",
      "  [  7.10339212e-03   1.28886761e-03   4.40800096e-03   2.15437717e-03]\n",
      "  [  5.61555135e-04   3.22438032e-03   3.33764078e-04   1.06574786e-03]\n",
      "  [  9.09417329e-03   1.72726413e-03   5.65800626e-03   5.46812281e-04]]\n",
      "\n",
      " [[  6.38862283e-03   2.21926062e-02   1.67644584e-02   4.97224747e-02]\n",
      "  [  1.75162655e-01   1.84163068e-02   9.08905178e-02   1.91226572e-01]\n",
      "  [  2.19765359e-02   7.31192276e-02   1.09221364e-02   1.50131586e-01]\n",
      "  [  9.37259617e-02   1.03151116e-02   4.87597274e-02   2.02854996e-02]]\n",
      "\n",
      " [[  4.63104877e-03   3.83679394e-02   2.35839239e-01   1.84155492e-02]\n",
      "  [  1.04425940e-02   2.61852954e-03   1.05157410e-01   5.82471858e-03]\n",
      "  [  7.58187581e-03   6.01640534e-02   7.31272857e-02   2.64636251e-02]\n",
      "  [  3.58669796e-02   9.41449213e-03   3.62118407e-01   3.96625328e-03]]\n",
      "\n",
      " [[  2.53836909e-03   3.08315001e-02   6.61894005e-03   1.85336889e-02]\n",
      "  [  3.80288242e-02   1.39801794e-02   1.96083636e-02   3.89476473e-02]\n",
      "  [  3.97336538e-02   4.62242302e-01   1.96226771e-02   2.54643708e-01]\n",
      "  [  2.59738671e-02   9.99515349e-03   1.34273211e-02   5.27380453e-03]]\n",
      "\n",
      " [[  1.15202942e-02   3.22568238e-02   2.07365296e-02   4.17610665e-02]\n",
      "  [  2.65268687e-01   2.24803972e-02   9.44176682e-02   1.34882383e-01]\n",
      "  [  8.22776430e-03   2.20653679e-02   2.80491965e-03   2.61792500e-02]\n",
      "  [  2.05254014e-01   1.82080174e-02   7.32458882e-02   2.06909289e-02]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "target_length = 4\n",
    "max_distance = 2\n",
    "\n",
    "baum_welch_model = BaumWelchModel(max_distance)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)\n",
    "unary_matrix = [0.97, .01, .01, .01]\n",
    "# transition_matrix = np.array([\n",
    "#     [.3, .7], \n",
    "#     [.1, .9]\n",
    "# ])\n",
    "transition_matrix = baum_welch_model.generate_transition_matrix(target_length, nomalized=True)\n",
    "print(\"transition_matrix\", transition_matrix)\n",
    "# print(\"unnormalized transition\", baum_welch_model.generate_transition_matrix(target_length, nomalized=False))\n",
    "emission_matrix = baum_welch_model.normalize_matrix(np.array([\n",
    "    [.4, .5, .7, .1, .4, .8],\n",
    "    [.6, .5, .4, .2, .7, .4],\n",
    "    [.2, .3, .8, .9, .2, .3],\n",
    "    [.6, .2, .7, .1, .4, .5]\n",
    "]), axis=0)\n",
    "print(\"emission_matrix\", emission_matrix)\n",
    "\n",
    "alpha = baum_welch_model.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "beta = baum_welch_model.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "print(\"alpha\", alpha)\n",
    "print(\"beta\", beta)\n",
    "\n",
    "new_unary_matrix, emission_posterior, transition_posterior = baum_welch_model.calc_posterior_matrix(alpha, beta)\n",
    "print(\"new_unary_matrix: \", new_unary_matrix)\n",
    "print(\"gamma: \", emission_posterior)\n",
    "print(\"epsilon: \", transition_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission_posterior [[ 0.96283366  0.09506816  0.29725378  0.0585225   0.10627471  0.49027076]\n",
      " [ 0.01495464  0.47569605  0.12404325  0.11056501  0.51704914  0.09501061]\n",
      " [ 0.00518545  0.25614949  0.16733684  0.77624234  0.0592773   0.19120501]\n",
      " [ 0.01702626  0.1730863   0.41136613  0.05467015  0.31739885  0.22351363]]\n",
      "transition_posterior [[[  7.83090416e-02   4.69455540e-01   2.45749714e-01   1.69319363e-01]\n",
      "  [  7.10339212e-03   1.28886761e-03   4.40800096e-03   2.15437717e-03]\n",
      "  [  5.61555135e-04   3.22438032e-03   3.33764078e-04   1.06574786e-03]\n",
      "  [  9.09417329e-03   1.72726413e-03   5.65800626e-03   5.46812281e-04]]\n",
      "\n",
      " [[  6.38862283e-03   2.21926062e-02   1.67644584e-02   4.97224747e-02]\n",
      "  [  1.75162655e-01   1.84163068e-02   9.08905178e-02   1.91226572e-01]\n",
      "  [  2.19765359e-02   7.31192276e-02   1.09221364e-02   1.50131586e-01]\n",
      "  [  9.37259617e-02   1.03151116e-02   4.87597274e-02   2.02854996e-02]]\n",
      "\n",
      " [[  4.63104877e-03   3.83679394e-02   2.35839239e-01   1.84155492e-02]\n",
      "  [  1.04425940e-02   2.61852954e-03   1.05157410e-01   5.82471858e-03]\n",
      "  [  7.58187581e-03   6.01640534e-02   7.31272857e-02   2.64636251e-02]\n",
      "  [  3.58669796e-02   9.41449213e-03   3.62118407e-01   3.96625328e-03]]\n",
      "\n",
      " [[  2.53836909e-03   3.08315001e-02   6.61894005e-03   1.85336889e-02]\n",
      "  [  3.80288242e-02   1.39801794e-02   1.96083636e-02   3.89476473e-02]\n",
      "  [  3.97336538e-02   4.62242302e-01   1.96226771e-02   2.54643708e-01]\n",
      "  [  2.59738671e-02   9.99515349e-03   1.34273211e-02   5.27380453e-03]]\n",
      "\n",
      " [[  1.15202942e-02   3.22568238e-02   2.07365296e-02   4.17610665e-02]\n",
      "  [  2.65268687e-01   2.24803972e-02   9.44176682e-02   1.34882383e-01]\n",
      "  [  8.22776430e-03   2.20653679e-02   2.80491965e-03   2.61792500e-02]\n",
      "  [  2.05254014e-01   1.82080174e-02   7.32458882e-02   2.06909289e-02]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "emission_posterior, transition_posterior = \\\n",
    "    baum_welch_model.calculate_baum_welch_posteriors(target_length, unary_matrix, emission_matrix)\n",
    "print(\"emission_posterior\", emission_posterior)\n",
    "print(\"transition_posterior\", transition_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with Unsupervised neural hidden markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BW model variables\n",
    "max_distance = 8\n",
    "baum_welch_model = BaumWelchModel(max_distance)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)\n",
    "\n",
    "\n",
    "target_length = 4\n",
    "emission_posterior, transition_posterior = \\\n",
    "    baum_welch_model.calculate_baum_welch_posteriors(target_length, emission_matrix)\n",
    "\n",
    "# Emission model variables\n",
    "input_size = np.shape(x)\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "output_size = 9\n",
    "emission_model = EmissionModel(input_size=input_size, layer_size=layer_size, output_size=output_size)\n",
    "\n",
    "result = emission_model.train_mini_batch(x, posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
