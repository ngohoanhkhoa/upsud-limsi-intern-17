{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import theano.tensor as T\n",
    "from theano import function, printing\n",
    "import theano\n",
    "\n",
    "from theano import config\n",
    "# config.device = 'cpu'\n",
    "# config.gcc.cxxflags = \"-D_hypot=hypot\"\n",
    "config.compute_test_value = 'off'\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"exception_verbosity=high,on_opt_error=optimizer_excluding=ShapeOpt:local_lift_transpose_through_dot:scan_opt\"\n",
    "from theano.compile.nanguardmode import NanGuardMode\n",
    "# config.NanGuardMode.action == 'pdb'\n",
    "\n",
    "\n",
    "# updates = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5)\n",
      "(12, 5)\n"
     ]
    }
   ],
   "source": [
    "class EmissionModel:\n",
    "    \"\"\" Simple emission model without CNN\n",
    "    word embedding layer -> ReLU layer -> softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def init_weights_bias(self, vocab_input_size, layer_size, vocab_output_size, seed=1402):\n",
    "        random_state = np.random.RandomState(seed)\n",
    "        \n",
    "        size_list = np.concatenate(([vocab_input_size], layer_size, [vocab_output_size]), axis=0)\n",
    "        w = []\n",
    "        b = []\n",
    "        \n",
    "        for i in range(len(size_list) - 1):\n",
    "            w.append(theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], size_list[i])), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), borrow=True\n",
    "            ))\n",
    "            b.append(theano.shared(\n",
    "                    value=np.asarray(\n",
    "                        random_state.uniform(low=-1.0, high=1.0, size=(size_list[i+1], 1)), \n",
    "                        dtype=theano.config.floatX\n",
    "                    ), \n",
    "                    borrow=True,\n",
    "                    broadcastable=(False,True)\n",
    "            ))\n",
    "        \n",
    "        return w, b\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = T.transpose(x)\n",
    "        e_x = T.exp(x - x.max(axis=1, keepdims=True)) \n",
    "        out = e_x / e_x.sum(axis=1, keepdims=True)\n",
    "        return T.transpose(out)\n",
    "    \n",
    "    #[7,512]\n",
    "    def __init__(self, vocab_input_size, layer_size, vocab_output_size, baum_welch_model, \n",
    "                 epoch=1, batch=1, learning_rate = .01, seed=1412):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch = batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.emission_posteriors = []\n",
    "        self.transition_posteriors = []\n",
    "        self.baum_welch_model = baum_welch_model\n",
    "        \n",
    "        self.vocab_input_size = vocab_input_size\n",
    "        self.d_embedding_size = layer_size[0]\n",
    "        \n",
    "        x_training_input = T.matrix().astype(config.floatX)\n",
    "        \n",
    "        self.w, self.b = self.init_weights_bias(vocab_input_size, layer_size, vocab_output_size, seed)\n",
    "        \n",
    "        # Word embedding layer\n",
    "        word_embedding_layer = T.dot(self.w[0], x_training_input) # [7, 10] * [10, 5] = [7, 5]\n",
    "        \n",
    "        # ReLU layer\n",
    "        z_relu_layer = T.dot(self.w[1], word_embedding_layer) + self.b[1] # [512, 7] * [7, 5] = [512, 5]\n",
    "        z_relu_layer_shape = T.shape(z_relu_layer)\n",
    "        relu_layer = T.nnet.relu(T.flatten(z_relu_layer))\n",
    "        relu_layer_reshaped = T.reshape(relu_layer, z_relu_layer_shape) # [512, 5]\n",
    "        \n",
    "        # Softmax layer\n",
    "        z_softmax_layer = T.dot(self.w[2], relu_layer_reshaped) + self.b[2] # [12, 512] * [512, 5] = [12, 5]\n",
    "#         softmax_layer = T.transpose(T.nnet.softmax(T.transpose(z_softmax_layer))) # Output: [12, 5]\n",
    "        softmax_layer = T.nnet.softmax(z_softmax_layer) # Output: [12, 5]\n",
    "        softmax_layer_clipped = T.clip(softmax_layer, 1e-35, 1.0 - 1e-35)\n",
    "        \n",
    "        # Calculate new gradient\n",
    "        posteriors = T.matrix().astype(config.floatX)\n",
    "        \n",
    "        cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer_clipped))\n",
    "#         cost = T.sum(T.transpose(posteriors) * T.log(softmax_layer))\n",
    "        # TODO: use dw[] and db[] abstractly \n",
    "        dw0,dw1,dw2,db1,db2 = T.grad(\n",
    "            cost=cost, wrt=[self.w[0], self.w[1], self.w[2], self.b[1], self.b[2]]\n",
    "        )\n",
    "\n",
    "        # Update w and b\n",
    "        updates = [\n",
    "            (self.w[0], self.w[0] - self.learning_rate * dw0), \n",
    "            (self.w[1], self.w[1] - self.learning_rate * dw1), \n",
    "            (self.b[1], self.b[1] - self.learning_rate * db1),\n",
    "            (self.w[2], self.w[2] - self.learning_rate * dw2), \n",
    "            (self.b[2], self.b[2] - self.learning_rate * db2)\n",
    "        ]\n",
    "        \n",
    "        # Compile model\n",
    "        self.test = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=[word_embedding_layer, softmax_layer]\n",
    "        ) \n",
    "        self.train_mini_batch_function = theano.function(\n",
    "            inputs=[x_training_input, posteriors], \n",
    "            outputs=softmax_layer, \n",
    "            updates=updates,\n",
    "            mode=NanGuardMode(nan_is_error=True, inf_is_error=True, big_is_error=False)\n",
    "        )\n",
    "        self.test_values = theano.function(\n",
    "            inputs=[x_training_input], \n",
    "            outputs=softmax_layer,\n",
    "            mode=NanGuardMode(nan_is_error=True, inf_is_error=True, big_is_error=False)\n",
    "        )\n",
    "        \n",
    "    def get_params(self):\n",
    "        return np.array(self.w)\n",
    "    \n",
    "    def train_mini_batch(self, testing_target, testing_source):\n",
    "        one_hot_input = np.eye(self.vocab_input_size)[testing_target].T\n",
    "        one_hot_input = np.asarray(one_hot_input).astype(config.floatX)\n",
    "#         print(\"one_hot_input\", one_hot_input, np.shape(one_hot_input))\n",
    "        softmax_matrix = self.test_values(one_hot_input)\n",
    "#         print(\"softmax_matrix\", softmax_matrix, np.shape(softmax_matrix))\n",
    "        \n",
    "#         softmax_matrix = np.clip(softmax_matrix, 1e-35, 1.0 - 1e-35)\n",
    "        \n",
    "        emission_posterior_vout = np.zeros_like(softmax_matrix.T) # [V_f_size, e_size]\n",
    "        emission_matrix = [] # [f_size, e_size]\n",
    "        for indice in testing_source:\n",
    "            emission_matrix.append(softmax_matrix[indice])\n",
    "        emission_matrix = np.array(emission_matrix)\n",
    "#         print(\"emission_matrix\", emission_matrix, np.shape(emission_matrix))\n",
    "        # Normalize emission_matrix\n",
    "#         emission_matrix = self.baum_welch_model.normalize_matrix(emission_matrix, axis=0)\n",
    "#         print(\"emission_matrix nomalized\", emission_matrix, np.shape(emission_matrix))\n",
    "        emission_posterior, transition_posterior = \\\n",
    "            self.baum_welch_model.calculate_baum_welch_posteriors(len(testing_target), np.transpose(emission_matrix))\n",
    "#         print(\"emission_posterior\", emission_posterior, np.shape(emission_posterior))\n",
    "        \n",
    "        # transform emission size to [target_size, v_out]\n",
    "        for i, indice in enumerate(testing_source):\n",
    "            emission_posterior_vout[:, indice] = np.maximum(emission_posterior_vout[:, indice], emission_posterior[:, i])\n",
    "#         print(\"emission_posterior_vout\", emission_posterior_vout, np.shape(emission_posterior_vout))\n",
    "        self.train_mini_batch_function(one_hot_input, np.asarray(emission_posterior_vout).astype(config.floatX))\n",
    "        \n",
    "        return emission_posterior, transition_posterior\n",
    "        \n",
    "    def train_model_epoch(self, target_inputs, source_inputs, input_indice_shift=0):\n",
    "        # TODO: add epoch functionality\n",
    "        for i in range(self.epoch):\n",
    "            self.emission_posteriors = []\n",
    "            self.transition_posteriors = []\n",
    "        #         for target_inputs_batch, source_inputs_batch in zip(np.split(target_inputs, self.batch), np.split(source_inputs, self.batch)):\n",
    "        #             for x_target, x_source in zip(target_inputs_batch, source_inputs_batch):\n",
    "            for i, x_target, x_source in zip(range(len(target_inputs)), target_inputs, source_inputs):\n",
    "                xx_target = [int(x)+input_indice_shift for x in x_target]\n",
    "                xx_source = [int(x)+input_indice_shift for x in x_source]\n",
    "                print(\"\\n+++++++++ The sentence \", i)\n",
    "                print(\"xx_source: \", len(xx_source), \" => \", xx_source)\n",
    "                print(\"xx_target: \", len(xx_target), \" => \", xx_target)\n",
    "                emis_posterior, trans_posterior = self.train_mini_batch(xx_target, xx_source)\n",
    "                self.emission_posteriors.append(emis_posterior)\n",
    "                self.transition_posteriors.append(trans_posterior)\n",
    "            \n",
    "            # Update Non-negative set of BW model\n",
    "            \n",
    "        return posteriors\n",
    "    \n",
    "    def train_model(self, target_inputs, source_inputs, input_indice_shift=0):\n",
    "        # TODO: add epoch functionality\n",
    "#         for i in range(self.epoch):\n",
    "        self.emission_posteriors = []\n",
    "        self.transition_posteriors = []\n",
    "#         for target_inputs_batch, source_inputs_batch in zip(np.split(target_inputs, self.batch), np.split(source_inputs, self.batch)):\n",
    "#             for x_target, x_source in zip(target_inputs_batch, source_inputs_batch):\n",
    "        for i, x_target, x_source in zip(range(len(target_inputs)), target_inputs, source_inputs):\n",
    "            xx_target = [int(x)+input_indice_shift for x in x_target]\n",
    "            xx_source = [int(x)+input_indice_shift for x in x_source]\n",
    "            print(\"\\n+++++++++ The sentence \", i)\n",
    "            print(\"xx_source: \", len(xx_source), \" => \", xx_source)\n",
    "            print(\"xx_target: \", len(xx_target), \" => \", xx_target)\n",
    "            emis_posterior, trans_posterior = self.train_mini_batch(xx_target, xx_source)\n",
    "            self.emission_posteriors.append(emis_posterior)\n",
    "            self.transition_posteriors.append(trans_posterior)\n",
    "        return self.transition_posteriors\n",
    "\n",
    "x = np.asarray([\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  1.,  0.],\n",
    "        [ 0.,  0.,  1.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  1.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 1.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.],\n",
    "        [ 0.,  1.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]\n",
    "    ]).astype(config.floatX)\n",
    "\n",
    "posteriors = np.asarray([\n",
    "    [-0.15,  0.04, -0.26, -0.61, -0.93, -0.72, -0.15, -0.62,  0.62, 0.24, 0.71, 0.81],\n",
    "    [ 0.07,  0.42,  0.11,  0.95, -0.86, -0.17, -0.22, -0.69, -0.55, 0.11, 0.37, 0.18],\n",
    "    [-0.79,  0.3 ,  0.06, -0.79,  0.71,  0.86, -0.58,  0.38,  0.05, 0.62, 0.17, 0.29],\n",
    "    [ 0.92, -0.33, -0.63,  0.99,  0.67, -0.79, -0.08,  0.64, -0.51, 0.19, 0.67, 0.52],\n",
    "    [-0.08, -0.29,  0.87,  0.6 ,  0.31,  0.75,  0.38, -0.42,  0.11, 0.44, 0.37, 0.14]\n",
    "]).astype(config.floatX)\n",
    "\n",
    "vocab_input_size = np.shape(x)[0]\n",
    "d_embedding = 7\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = 12\n",
    "\n",
    "model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                      vocab_output_size=vocab_output_size, baum_welch_model=None)\n",
    "\n",
    "result = model.test(x)\n",
    "print(np.shape(result[0]))\n",
    "print(np.shape(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaumWelchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaumWelchModel:\n",
    "    \n",
    "    def normalize_matrix(self, x, axis=1, whole_matrix=False):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\n",
    "            axis=1: row\n",
    "            axis=0: column \n",
    "        Input\n",
    "        -----\n",
    "        \n",
    "        Output\n",
    "        ------\n",
    "        \"\"\"\n",
    "        if len(np.shape(x)) == 1 or whole_matrix:\n",
    "#             e_x = np.exp(x - np.max(x))\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x)\n",
    "        if axis == 0:\n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[None, :]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[None, :]\n",
    "        else: \n",
    "#             e_x = np.exp( np.subtract(x, np.max(x, axis=axis)[:, None]) )\n",
    "            e_x = x\n",
    "            return e_x / np.sum(e_x, axis=axis)[:, None]\n",
    "        \n",
    "    def generate_transition_distant_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "        We extend the latent sentence for 2*length in which each word has \n",
    "        an empty word to represent no-alignment state.\n",
    "        where [sentence_length:end] elements are empty words considered as \n",
    "        latent words having no direct aligment.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_distant_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_distant_matrix = np.zeros((2*sentence_length, 2*sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_distant_matrix[i][j] = p_\n",
    "\n",
    "        print(trans_distant_matrix)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            trans_distant_matrix[i+sentence_length][i+sentence_length] = po\n",
    "            trans_distant_matrix[i][i+sentence_length] = po\n",
    "\n",
    "            sum_d = np.sum(trans_distant_matrix[:sentence_length, i])\n",
    "            trans_distant_matrix[:sentence_length, i] = \\\n",
    "                    np.divide(\n",
    "                        trans_distant_matrix[:sentence_length, i], \n",
    "                        sum_d\n",
    "                    )\n",
    "            trans_distant_matrix[sentence_length:, i] = \\\n",
    "                    np.copy(trans_distant_matrix[:sentence_length, i])\n",
    "\n",
    "        return trans_distant_matrix\n",
    "    \n",
    "    def generate_transition_matrix(self, sentence_length, po=0., nomalized=True):\n",
    "        \"\"\" Generate a transition matrix based on jump distance in the latent sentence.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        sentence_length: the length of latent sentence\n",
    "                      int value\n",
    "        non_negative_set: random non-negative set as max_distance size\n",
    "        po: default value for A->A_empty_word\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        trans_matrix\n",
    "        \"\"\"\n",
    "        if po==0.:\n",
    "            po = self.po\n",
    "        trans_matrix = np.zeros((sentence_length, sentence_length))\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            for j in range(sentence_length):\n",
    "                indice = i - j + self.max_distance + 1\n",
    "                if indice < 0:\n",
    "                    p_ = self.non_negative_set[0]\n",
    "                elif (indice > 2*self.max_distance + 2):\n",
    "                    p_ = self.non_negative_set[-1]\n",
    "                else:\n",
    "                    p_ = self.non_negative_set[indice]\n",
    "                trans_matrix[i][j] = p_\n",
    "        if nomalized:\n",
    "            return self.normalize_matrix(trans_matrix, axis=1)\n",
    "        return trans_matrix\n",
    "        \n",
    "    def __init__(self, max_distance, po=0.3, seed=1402):\n",
    "        np.random.seed(seed)\n",
    "        self.max_distance = max_distance\n",
    "        self.non_negative_set = np.random.randint(\n",
    "                                    low=1, high=100, \n",
    "                                    size=[max_distance + max_distance + 3]\n",
    "        )\n",
    "        self.po = po\n",
    "        \n",
    "    def calc_forward_messages(self, unary_matrix, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the forward messages ~ alpha values.\n",
    "        \n",
    "        \n",
    "        Input\n",
    "        -----\n",
    "        unary_matrix: emission posteriors - marginal probabilities ~ initial matrix.\n",
    "                      size ~ [1, target_len]\n",
    "        transition_matrix: size ~ [target_len, target_len]\n",
    "        emission_matrix: size ~ [target_len, source_len]\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        alpha\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        alpha = np.zeros(np.shape(emission_matrix))\n",
    "#         print(\"emission_matrix[:,0]\", emission_matrix[:, 0])\n",
    "        print(\"unary_matrix\", unary_matrix)\n",
    "        alpha.T[0] = np.multiply(emission_matrix[:,0], unary_matrix)\n",
    "#         print(\"alpha.T[0]\", alpha.T[0])\n",
    "        \n",
    "        for t in np.arange(1, source_len):\n",
    "            for i in range(target_len):\n",
    "                sum_al = 0.0;\n",
    "#                 print(\"alpha : \", t, i, \" :: \", emission_matrix[i][t])\n",
    "                for j in range(target_len):\n",
    "                    sum_al += alpha[j][t-1] * transition_matrix[j][i]\n",
    "#                     print(\"   sum_al: \", t, i, j, alpha[j][t-1], transition_matrix[j][i])\n",
    "\n",
    "                alpha[i][t] = emission_matrix[i][t] * sum_al\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def calc_backward_messages(self, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the backward messages ~ beta values.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        beta\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(emission_matrix)[1]\n",
    "        target_len = np.shape(emission_matrix)[0]\n",
    "\n",
    "        beta = np.zeros(np.shape(emission_matrix))\n",
    "        beta[:,-1] = [1]*target_len\n",
    "\n",
    "        for t in reversed(range(source_len-1)):\n",
    "            for i in range(target_len):\n",
    "    #             print(\"beta \", t, i)\n",
    "                for j in range(target_len):\n",
    "                    beta[i][t] += beta[j][t+1] * transition_matrix[i][j] * emission_matrix[j][t+1]\n",
    "    #                 print(\"    \", beta[t+1][j], transition_matrix[i][j], emission_matrix[ observation_sentence[t+1] ][j], beta[t][i])\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def calc_posterior_matrix(self, alpha, beta, transition_matrix, emission_matrix):\n",
    "        \"\"\"Calcualte the gama and epsilon values in order to reproduce \n",
    "        better transition and emission matrix.\n",
    "        \n",
    "        gamma: P(e_aj|f_j)\n",
    "        epsilon: P(e_aj,e_a(j+1)|f_j)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        unary_matrix, posterior_gamma, posterior_epsilon\n",
    "        \"\"\"\n",
    "        # TODO: verify matrix length\n",
    "        source_len = np.shape(alpha)[1]\n",
    "        target_len = np.shape(alpha)[0]\n",
    "\n",
    "        gamma = np.multiply(alpha, beta)\n",
    "        epsilon = np.zeros((source_len-1, target_len, target_len))\n",
    "\n",
    "        # Normalization on columns\n",
    "        gamma = self.normalize_matrix(gamma, axis=0)\n",
    "\n",
    "        for t in range(source_len-1):   \n",
    "            for i in range(target_len):\n",
    "                for j in range(target_len):\n",
    "                    epsilon[t][i][j] = alpha[i][t] * transition_matrix[i][j] * \\\n",
    "                                        beta[j][t+1] * emission_matrix[j][t+1]\n",
    "            # Normalization\n",
    "            epsilon[t] = self.normalize_matrix(epsilon[t], whole_matrix=True)\n",
    "\n",
    "        # Update unary matrix\n",
    "        # Normalization unary\n",
    "        new_unary_matrix = np.copy(gamma[:,0])#self.normalize_matrix(np.copy(gamma[:,0]), axis=1)\n",
    "\n",
    "#         new_transition_matrix = np.zeros( (latent_indice_len, latent_indice_len) )\n",
    "#         new_emission_matrix = np.zeros( (observation_len, latent_indice_len) )\n",
    "            \n",
    "#         # Update emission matrix\n",
    "#         sum_gamma = [np.sum(gamma.T[i]) for i in range(latent_indice_len)]   \n",
    "#         for i in range(latent_indice_len):\n",
    "#             new_emission_matrix.T[i] = np.divide(gamma.T[i], sum_gamma[i])\n",
    "\n",
    "        return new_unary_matrix, gamma, epsilon\n",
    "\n",
    "\n",
    "    def calculate_baum_welch_posteriors(self, sentence_length, emission_matrix, unary_matrix=None):\n",
    "        if unary_matrix == None:\n",
    "            unary_matrix = [0.01]*sentence_length\n",
    "            unary_matrix[0] = 1 - np.sum(unary_matrix) + 0.01\n",
    "        transition_matrix = self.generate_transition_matrix(sentence_length)\n",
    "#         emission_matrix = self.normalize_matrix(emission_matrix, axis=0)\n",
    "        \n",
    "        alpha = self.calc_forward_messages(unary_matrix, transition_matrix, emission_matrix)\n",
    "        beta = self.calc_backward_messages(transition_matrix, emission_matrix)\n",
    "\n",
    "        new_unary_matrix, emission_posterior, transition_posterior = self.calc_posterior_matrix(alpha, beta, transition_matrix, emission_matrix)\n",
    "        return emission_posterior, transition_posterior # gamma, epsilon\n",
    "    \n",
    "    def update_non_negative_transition_set(self, emission_posteriors, transition_posteriors):\n",
    "        pass\n",
    "        # TODO 1: update non-negative set: s[-1] = \n",
    "        # TODO 1.1: calculate new transition matrix\n",
    "        transition_list = np.array([])\n",
    "        for gamma, epsilon in zip(emission_posteriors, transition_posteriors):\n",
    "            source_len = np.shape(gamma)[1]\n",
    "            target_len = np.shape(gamma)[0]\n",
    "            new_transition_matrix = np.zeros((target_len, target_len))\n",
    "\n",
    "            for i in range(target_len):\n",
    "                sum_gamma = np.sum(gamma[i][:-1])\n",
    "                for j in range(target_len):\n",
    "                    sum_ep = np.sum(epsilon[:-1][i][j])\n",
    "                    new_transition_matrix[i][j] = sum_ep/sum_gamma\n",
    "            # Normalization\n",
    "            new_transition_matrix = self.normalize_matrix(new_transition_matrix, axis=1)\n",
    "            transition_list.append(new_transition_matrix)\n",
    "            \n",
    "        # TODO 1.2: update\n",
    "        new_non_negative_set = np.zeros(max_distance)\n",
    "        \n",
    "        return new_non_negative_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Read training file\n",
    "# Read vocab en - source\n",
    "#\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.en\"\n",
    "#\"E:/Working/Intership2017/data/en-cz/formatted/testing/testing.en-cz.en\"\n",
    "en = []\n",
    "with open(\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.en\", encoding=\"utf8\") as en_file:\n",
    "    for line in en_file:\n",
    "        en.append(line.strip())\n",
    "en_tokenizer = Tokenizer(lower=False, filters='\\t\\n')\n",
    "en_tokenizer.fit_on_texts(en)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en)\n",
    "en_source_indices = en_tokenizer.word_index\n",
    "\n",
    "# Read vocab cz - target\n",
    "#\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.cz\"\n",
    "#\"E:/Working/Intership2017/data/en-cz/formatted/testing/testing.en-cz.cz\"\n",
    "cz = []\n",
    "with open(\"/vol/work2/2017-NeuralAlignments/data/en-cz/formatted/testing/testing.en-cz.cz\", encoding='utf8') as cz_file:\n",
    "    for line in cz_file:\n",
    "        cz.append(line.strip())\n",
    "cz_tokenizer = Tokenizer(lower=False, filters='\\t\\n')\n",
    "cz_tokenizer.fit_on_texts(cz)\n",
    "cz_sequences = cz_tokenizer.texts_to_sequences(cz)\n",
    "cz_target_indices = cz_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9048\n",
      "15002\n"
     ]
    }
   ],
   "source": [
    "print(len(en_source_indices))\n",
    "print(len(cz_target_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_negative_set [29 56 82 13 35 53 25 23 21 12 15  9 13 87  9 63 62 52 43 77 95 79 77  5 78\n",
      " 41 10 10 88 19  1 37  9 70 22 82 46 51 97 46 12 32 56 30 86 45 99 89  1 88\n",
      " 94 48 26 65 53 54 76 48 98 63 67 74 40 41 76  8 10 78 18 79 92 23  3 64 22\n",
      " 43 24 75  8 36 78  6 98 33 49 31 15 21 30 44 14 75 55 22 71 95 94  9 68 65\n",
      "  3 58 90]\n",
      "\n",
      "+++++++++ The sentence  0\n",
      "xx_source:  6  =>  [29, 712, 4, 2093, 1403, 2]\n",
      "xx_target:  7  =>  [2977, 49, 4, 1156, 4, 2978, 1]\n",
      "unary_matrix [0.93999999999999995, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  1\n",
      "xx_source:  5  =>  [29, 1009, 99, 2094, 2]\n",
      "xx_target:  5  =>  [5158, 49, 26, 2979, 1]\n",
      "unary_matrix [0.95999999999999996, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  2\n",
      "xx_source:  11  =>  [29, 110, 24, 25, 141, 396, 68, 2094, 25, 61, 2]\n",
      "xx_target:  7  =>  [2980, 49, 182, 354, 2007, 1467, 1]\n",
      "unary_matrix [0.93999999999999995, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  3\n",
      "xx_source:  13  =>  [1010, 1, 61, 3, 7, 2095, 1, 22, 1162, 4, 1163, 644, 2]\n",
      "xx_target:  4  =>  [2981, 5159, 5160, 1]\n",
      "unary_matrix [0.96999999999999997, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  4\n",
      "xx_source:  9  =>  [29, 4233, 60, 99, 2803, 5, 4234, 22, 2]\n",
      "xx_target:  6  =>  [5161, 49, 5162, 2, 5163, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  5\n",
      "xx_source:  13  =>  [29, 151, 22, 7, 2096, 5, 4235, 10, 1164, 4, 0, 4236, 2]\n",
      "xx_target:  8  =>  [403, 5164, 49, 5165, 5166, 5167, 1157, 1]\n",
      "unary_matrix [0.93000000000000005, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  6\n",
      "xx_source:  7  =>  [29, 712, 4, 396, 99, 1011, 2]\n",
      "xx_target:  6  =>  [5168, 49, 4, 2008, 2982, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  7\n",
      "xx_source:  7  =>  [29, 110, 24, 396, 99, 1011, 2]\n",
      "xx_target:  5  =>  [5169, 49, 8, 1467, 1]\n",
      "unary_matrix [0.95999999999999996, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  8\n",
      "xx_source:  5  =>  [79, 39, 2097, 2098, 2]\n",
      "xx_target:  3  =>  [5170, 5171, 1]\n",
      "unary_matrix [0.97999999999999998, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  9\n",
      "xx_source:  33  =>  [29, 32, 315, 56, 4237, 4, 369, 4, 0, 370, 12, 22, 89, 32, 56, 156, 61, 0, 71, 1, 5, 89, 32, 56, 2098, 1, 57, 29, 471, 227, 11, 22, 2]\n",
      "xx_target:  22  =>  [5172, 49, 1158, 2009, 0, 7, 355, 29, 1468, 1159, 282, 56, 2, 7, 5173, 0, 32, 5174, 508, 19, 2007, 1]\n",
      "unary_matrix [0.78999999999999992, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  10\n",
      "xx_source:  36  =>  [29, 4238, 1403, 61, 102, 99, 1404, 1, 26, 98, 29, 519, 99, 2804, 67, 4, 99, 420, 1, 5, 29, 571, 7, 1670, 1405, 183, 338, 1, 5, 7, 371, 207, 67, 0, 275, 2]\n",
      "xx_target:  27  =>  [5175, 49, 4, 29, 356, 2983, 357, 38, 1160, 0, 22, 5176, 2984, 0, 90, 13, 5177, 0, 2985, 29, 573, 357, 2, 5178, 5, 5179, 1]\n",
      "unary_matrix [0.73999999999999999, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  11\n",
      "xx_source:  9  =>  [65, 29, 110, 24, 396, 20, 1165, 572, 2]\n",
      "xx_target:  9  =>  [242, 509, 49, 121, 5180, 0, 121, 5181, 1]\n",
      "unary_matrix [0.92000000000000004, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  12\n",
      "xx_source:  8  =>  [29, 712, 4, 1406, 25, 99, 2099, 2]\n",
      "xx_target:  8  =>  [2977, 49, 4, 5182, 4, 5, 2986, 1]\n",
      "unary_matrix [0.93000000000000005, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  13\n",
      "xx_source:  27  =>  [29, 4239, 22, 60, 18, 293, 18, 1012, 22, 47, 397, 1, 5, 29, 1013, 62, 4240, 1, 5, 712, 4, 2093, 22, 16, 0, 76, 2]\n",
      "xx_target:  21  =>  [5183, 49, 182, 42, 1469, 0, 63, 19, 52, 2987, 0, 5184, 1161, 2988, 2, 2989, 4, 5185, 182, 2978, 1]\n",
      "unary_matrix [0.79999999999999993, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  14\n",
      "xx_source:  30  =>  [29, 110, 86, 520, 0, 2805, 1, 5, 0, 86, 1407, 12, 29, 110, 645, 26, 12, 23, 4, 396, 40, 573, 43, 140, 12, 29, 39, 4241, 4242, 2]\n",
      "xx_target:  29  =>  [5186, 49, 52, 48, 5187, 2, 795, 0, 57, 49, 14, 97, 1162, 0, 56, 0, 7, 49, 44, 134, 5188, 5189, 25, 939, 0, 7, 2010, 5190, 1]\n",
      "unary_matrix [0.71999999999999997, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  15\n",
      "xx_source:  14  =>  [29, 39, 2806, 81, 12, 2807, 10, 1671, 7, 1408, 1, 4243, 803, 2]\n",
      "xx_target:  9  =>  [796, 5191, 49, 5192, 23, 2990, 5193, 5194, 1]\n",
      "unary_matrix [0.92000000000000004, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  16\n",
      "xx_source:  7  =>  [29, 4244, 60, 7, 4245, 4246, 2]\n",
      "xx_target:  8  =>  [5195, 49, 4, 5196, 23, 5197, 5198, 1]\n",
      "unary_matrix [0.93000000000000005, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  17\n",
      "xx_source:  7  =>  [29, 571, 4, 99, 2100, 803, 2]\n",
      "xx_target:  6  =>  [2991, 49, 103, 1163, 2992, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  18\n",
      "xx_source:  37  =>  [157, 8, 37, 294, 4247, 3, 4248, 1, 5, 2808, 99, 2094, 1, 5, 2101, 25, 99, 2099, 1, 5, 1409, 66, 0, 4249, 1, 61, 11, 574, 1, 75, 29, 2102, 29, 105, 713, 4250, 41]\n",
      "xx_target:  35  =>  [110, 19, 2011, 1470, 5199, 0, 2, 61, 2010, 2012, 0, 7, 187, 510, 8, 0, 5200, 187, 2979, 0, 5201, 4, 5, 2986, 0, 5202, 9, 5203, 10, 2, 509, 21, 19, 5204, 1]\n",
      "unary_matrix [0.65999999999999992, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  19\n",
      "xx_source:  16  =>  [104, 29, 575, 29, 47, 96, 276, 7, 295, 1166, 17, 398, 4, 276, 138, 2]\n",
      "xx_target:  14  =>  [5205, 677, 0, 7, 18, 5206, 0, 5207, 26, 404, 13, 2013, 5208, 1]\n",
      "unary_matrix [0.87, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  20\n",
      "xx_source:  14  =>  [14, 421, 7, 4251, 1410, 1, 14, 29, 42, 1, 14, 8, 1167, 2]\n",
      "xx_target:  10  =>  [6, 5209, 1471, 2993, 0, 6, 73, 49, 26, 1]\n",
      "unary_matrix [0.91000000000000003, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  21\n",
      "xx_source:  5  =>  [157, 21, 32, 296, 2]\n",
      "xx_target:  4  =>  [2014, 106, 1164, 1]\n",
      "unary_matrix [0.96999999999999997, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  22\n",
      "xx_source:  29  =>  [157, 35, 316, 40, 1167, 60, 3, 296, 43, 60, 3, 4252, 2096, 3, 258, 1411, 1, 4253, 2103, 1, 16, 86, 62, 20, 100, 422, 183, 2, 14]\n",
      "xx_target:  24  =>  [5210, 1164, 5211, 259, 2993, 25, 50, 5212, 5213, 2015, 2994, 0, 14, 213, 260, 91, 29, 940, 17, 29, 261, 5214, 1, 6]\n",
      "unary_matrix [0.77000000000000002, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  23\n",
      "xx_source:  24  =>  [250, 29, 571, 4254, 67, 5, 2104, 276, 1, 5, 51, 42, 46, 14, 1672, 1, 98, 9, 0, 228, 16, 106, 117, 14]\n",
      "xx_target:  17  =>  [2991, 49, 98, 5215, 13, 2013, 2, 2016, 5216, 28, 6, 941, 57, 511, 8, 76, 6]\n",
      "unary_matrix [0.83999999999999997, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  24\n",
      "xx_source:  25  =>  [29, 42, 46, 14, 29, 35, 24, 152, 67, 258, 71, 1, 1412, 2809, 1, 16, 2810, 106, 98, 8, 0, 228, 16, 296, 2]\n",
      "xx_target:  20  =>  [2995, 49, 28, 6, 5217, 5218, 5219, 9, 678, 0, 5220, 5221, 0, 5222, 97, 0, 57, 187, 8, 1]\n",
      "unary_matrix [0.80999999999999994, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  25\n",
      "xx_source:  14  =>  [4255, 8, 2811, 1, 5, 106, 229, 2812, 804, 140, 29, 39, 2813, 2]\n",
      "xx_target:  14  =>  [5223, 8, 5224, 2, 325, 5225, 243, 5226, 0, 5227, 25, 508, 2996, 1]\n",
      "unary_matrix [0.87, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  26\n",
      "xx_source:  13  =>  [65, 29, 35, 1413, 106, 98, 8, 4256, 0, 228, 16, 296, 2]\n",
      "xx_target:  8  =>  [242, 2997, 511, 0, 57, 187, 104, 1]\n",
      "unary_matrix [0.93000000000000005, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  27\n",
      "xx_source:  8  =>  [29, 32, 24, 646, 2814, 9, 2105, 2]\n",
      "xx_target:  4  =>  [5228, 2017, 2998, 1]\n",
      "unary_matrix [0.96999999999999997, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  28\n",
      "xx_source:  25  =>  [1414, 29, 32, 24, 646, 2814, 9, 2105, 1, 29, 471, 1413, 106, 41, 57, 0, 714, 647, 12, 29, 32, 24, 646, 22, 2]\n",
      "xx_target:  17  =>  [1165, 2018, 2017, 2998, 0, 19, 511, 5229, 0, 32, 8, 2019, 0, 7, 128, 2018, 1]\n",
      "unary_matrix [0.83999999999999997, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  29\n",
      "xx_source:  10  =>  [4257, 906, 1, 241, 1, 29, 4258, 646, 2, 14]\n",
      "xx_target:  6  =>  [2999, 679, 64, 2010, 1, 6]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  30\n",
      "xx_source:  12  =>  [339, 29, 805, 276, 340, 29, 1014, 4, 2815, 22, 61, 2]\n",
      "xx_target:  11  =>  [86, 5230, 49, 214, 0, 63, 49, 5, 19, 2020, 1]\n",
      "unary_matrix [0.90000000000000002, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  31\n",
      "xx_source:  53  =>  [1010, 51, 1168, 296, 5, 2106, 73, 296, 1, 5, 4259, 576, 3, 99, 4260, 1, 5, 208, 51, 1415, 296, 102, 0, 4261, 75, 29, 23, 90, 2816, 22, 10, 7, 4262, 1407, 4, 96, 1, 29, 519, 22, 10, 5, 1673, 4263, 4264, 296, 16, 0, 338, 3, 49, 420, 2]\n",
      "xx_target:  40  =>  [5231, 187, 5232, 0, 5233, 4, 15, 1164, 0, 5234, 677, 21, 5235, 2, 1472, 677, 5236, 15, 5237, 0, 61, 49, 19, 354, 5238, 10, 145, 3000, 5239, 10, 2, 942, 2021, 15, 1164, 5240, 680, 1473, 2022, 1]\n",
      "unary_matrix [0.60999999999999999, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  32\n",
      "xx_source:  32  =>  [715, 12, 1, 51, 1169, 73, 5, 1674, 60, 7, 2817, 1, 5, 4265, 22, 67, 5, 1170, 22, 296, 1, 5, 29, 317, 22, 6, 99, 2818, 5, 571, 60, 2]\n",
      "xx_target:  23  =>  [2981, 4, 5241, 2, 2023, 1166, 0, 5242, 182, 2, 3001, 187, 182, 1, 5243, 49, 1166, 15, 5244, 2, 5245, 49, 1]\n",
      "unary_matrix [0.77999999999999992, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  33\n",
      "xx_source:  6  =>  [29, 153, 24, 521, 22, 2]\n",
      "xx_target:  6  =>  [119, 1166, 49, 4, 5246, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  34\n",
      "xx_source:  14  =>  [29, 577, 22, 4, 0, 4266, 1675, 9, 1, 5, 2819, 22, 6, 2]\n",
      "xx_target:  10  =>  [5247, 49, 182, 15, 5248, 5249, 2, 3001, 5250, 1]\n",
      "unary_matrix [0.91000000000000003, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  35\n",
      "xx_source:  11  =>  [15, 803, 1171, 22, 1, 5, 208, 2819, 22, 275, 2]\n",
      "xx_target:  8  =>  [943, 182, 5251, 2, 1474, 187, 182, 1]\n",
      "unary_matrix [0.93000000000000005, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  36\n",
      "xx_source:  8  =>  [157, 42, 51, 153, 90, 648, 22, 2]\n",
      "xx_target:  6  =>  [1475, 0, 7, 19, 797, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  37\n",
      "xx_source:  10  =>  [29, 42, 46, 14, 806, 31, 7, 1675, 117, 14]\n",
      "xx_target:  10  =>  [5252, 49, 4, 28, 6, 5253, 944, 2024, 76, 6]\n",
      "unary_matrix [0.91000000000000003, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  38\n",
      "xx_source:  9  =>  [157, 42, 46, 14, 29, 1416, 7, 1675, 2]\n",
      "xx_target:  6  =>  [2995, 28, 6, 5254, 2024, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  39\n",
      "xx_source:  21  =>  [122, 29, 23, 7, 2107, 10, 4267, 4268, 5, 423, 2820, 807, 1, 29, 229, 13, 472, 4, 4269, 106, 2]\n",
      "xx_target:  15  =>  [5255, 44, 3002, 5256, 2, 5257, 5258, 3, 945, 0, 243, 508, 681, 5259, 1]\n",
      "unary_matrix [0.85999999999999999, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  40\n",
      "xx_source:  8  =>  [4270, 86, 7, 1675, 4271, 296, 2, 14]\n",
      "xx_target:  11  =>  [2025, 49, 52, 5260, 2024, 0, 3003, 681, 5261, 1, 6]\n",
      "unary_matrix [0.90000000000000002, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  41\n",
      "xx_source:  5  =>  [29, 1171, 0, 2817, 2]\n",
      "xx_target:  5  =>  [5262, 49, 26, 1166, 1]\n",
      "unary_matrix [0.95999999999999996, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  42\n",
      "xx_source:  19  =>  [79, 1676, 46, 14, 72, 4272, 2, 4273, 1, 16, 72, 4274, 2, 1417, 4275, 578, 716, 808, 2]\n",
      "xx_target:  16  =>  [5263, 355, 28, 6, 45, 5264, 5265, 11, 45, 5266, 5267, 10, 5268, 1167, 682, 1]\n",
      "unary_matrix [0.84999999999999998, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  43\n",
      "xx_source:  8  =>  [72, 522, 10, 4276, 1418, 578, 809, 2]\n",
      "xx_target:  6  =>  [45, 5269, 5270, 573, 946, 1]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  44\n",
      "xx_source:  8  =>  [72, 2108, 25, 649, 4277, 578, 473, 2]\n",
      "xx_target:  9  =>  [45, 5271, 1168, 3, 574, 682, 3004, 1476, 1]\n",
      "unary_matrix [0.92000000000000004, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  45\n",
      "xx_source:  15  =>  [339, 96, 90, 2821, 67, 258, 420, 16, 717, 106, 96, 90, 1419, 2, 14]\n",
      "xx_target:  10  =>  [86, 5272, 26, 1477, 3005, 0, 798, 5273, 1, 6]\n",
      "unary_matrix [0.91000000000000003, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  46\n",
      "xx_source:  26  =>  [29, 579, 0, 4278, 1, 16, 0, 1408, 424, 10, 2109, 11, 1403, 10, 12, 99, 474, 23, 4279, 1, 5, 8, 230, 398, 19, 2]\n",
      "xx_target:  26  =>  [5274, 49, 4, 129, 5275, 0, 107, 447, 683, 2990, 2026, 10, 5276, 9, 799, 10, 7, 2011, 405, 44, 5277, 2, 3006, 134, 5278, 1]\n",
      "unary_matrix [0.75, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  47\n",
      "xx_source:  39  =>  [48, 0, 1172, 2822, 1, 398, 275, 4, 0, 4280, 10, 4281, 4282, 1, 29, 39, 0, 2110, 1, 810, 61, 2823, 1, 0, 523, 718, 139, 179, 14, 7, 650, 4283, 4, 216, 3, 68, 1420, 2, 14]\n",
      "xx_target:  31  =>  [86, 10, 1169, 4, 1474, 13, 5279, 5280, 5281, 10, 404, 49, 135, 2027, 5282, 3007, 3008, 0, 14, 213, 947, 44, 6, 5283, 3009, 13, 575, 5284, 1478, 6, 1]\n",
      "unary_matrix [0.69999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  48\n",
      "xx_source:  11  =>  [421, 29, 1677, 6, 12, 207, 118, 2099, 82, 1413, 2]\n",
      "xx_target:  10  =>  [168, 3, 197, 1170, 5285, 0, 5286, 4, 5287, 1]\n",
      "unary_matrix [0.91000000000000003, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  49\n",
      "xx_source:  12  =>  [1173, 99, 2824, 2825, 29, 32, 56, 7, 4284, 4, 22, 2]\n",
      "xx_target:  11  =>  [358, 5288, 3010, 49, 44, 3, 197, 1170, 5289, 5290, 1]\n",
      "unary_matrix [0.90000000000000002, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  50\n",
      "xx_source:  8  =>  [4285, 3, 1421, 5, 2826, 16, 0, 251]\n",
      "xx_target:  12  =>  [5291, 800, 1479, 12, 326, 2, 800, 5292, 12, 326, 13, 359]\n",
      "unary_matrix [0.89000000000000001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  51\n",
      "xx_source:  41  =>  [15, 1174, 3, 0, 251, 9, 141, 227, 21, 13, 252, 86, 17, 1421, 907, 4, 2111, 0, 129, 3, 2827, 2112, 11, 341, 242, 1678, 811, 20, 4286, 2828, 205, 811, 1, 5, 30, 32, 908, 37, 719, 253, 2]\n",
      "xx_target:  40  =>  [3011, 1480, 948, 359, 30, 3012, 136, 3013, 5293, 12, 326, 0, 22, 30, 5294, 21, 360, 406, 800, 3014, 2, 1171, 9, 1481, 5295, 169, 3015, 17, 3016, 3017, 407, 3018, 0, 2, 22, 1172, 1173, 9, 684, 1]\n",
      "unary_matrix [0.60999999999999999, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  52\n",
      "xx_source:  27  =>  [15, 719, 253, 21, 909, 16, 0, 425, 3, 33, 2113, 5, 3, 68, 277, 5, 1679, 720, 4287, 4, 13, 812, 17, 0, 1015, 1175, 2]\n",
      "xx_target:  24  =>  [1482, 9, 684, 59, 35, 3, 153, 11, 512, 5296, 2, 11, 5297, 0, 39, 5298, 53, 1483, 2, 39, 576, 1484, 949, 1]\n",
      "unary_matrix [0.77000000000000002, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  53\n",
      "xx_source:  58  =>  [1016, 813, 907, 4, 2111, 0, 129, 3, 37, 2829, 30, 1680, 11, 242, 1678, 811, 52, 426, 580, 16, 0, 251, 11, 0, 524, 3, 0, 141, 227, 1, 160, 12, 0, 2829, 8, 1176, 20, 4288, 19, 1, 5, 4289, 4, 1, 0, 1681, 3, 427, 1678, 475, 1422, 17, 910, 5, 4290, 1682, 4291, 2]\n",
      "xx_target:  60  =>  [950, 262, 12, 326, 5299, 21, 360, 406, 169, 202, 17, 1485, 1171, 9, 1481, 0, 122, 3019, 169, 3015, 17, 3016, 3017, 407, 3018, 0, 4, 43, 3020, 13, 359, 12, 303, 1480, 948, 21, 3021, 0, 7, 169, 801, 17, 1171, 9, 1481, 8, 5300, 3, 153, 4, 5301, 1486, 685, 5302, 1487, 5303, 951, 2, 5304, 1]\n",
      "unary_matrix [0.40999999999999992, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  54\n",
      "xx_source:  20  =>  [180, 0, 524, 3, 33, 2113, 1, 4292, 10, 2830, 2827, 21, 13, 2831, 11, 719, 16, 0, 251, 2]\n",
      "xx_target:  19  =>  [263, 303, 53, 202, 4, 92, 169, 202, 17, 1171, 9, 1481, 3022, 14, 2028, 3020, 13, 359, 1]\n",
      "unary_matrix [0.81999999999999995, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  55\n",
      "xx_source:  35  =>  [91, 814, 719, 253, 21, 13, 2114, 17, 0, 4293, 911, 4, 0, 476, 651, 5, 1, 16, 0, 4294, 3, 0, 651, 1, 4, 0, 1015, 1175, 11, 525, 17, 7, 1177, 342, 2]\n",
      "xx_target:  20  =>  [5305, 202, 9, 684, 361, 5306, 362, 577, 3023, 2, 11, 3024, 3025, 3023, 949, 103, 578, 952, 363, 1]\n",
      "unary_matrix [0.80999999999999994, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  56\n",
      "xx_source:  26  =>  [48, 4295, 60, 0, 129, 3, 0, 719, 253, 111, 0, 251, 5, 37, 580, 813, 183, 2115, 21, 428, 0, 4296, 3, 0, 76, 2]\n",
      "xx_target:  20  =>  [264, 406, 202, 9, 684, 51, 2029, 2, 2030, 169, 1479, 12, 326, 5307, 953, 1174, 5308, 117, 3026, 1]\n",
      "unary_matrix [0.80999999999999994, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  57\n",
      "xx_source:  29  =>  [15, 719, 253, 21, 2832, 0, 2833, 119, 5, 1423, 3, 0, 251, 5, 0, 580, 813, 1, 6, 425, 720, 16, 0, 1424, 129, 3, 33, 253, 2]\n",
      "xx_target:  27  =>  [1482, 9, 684, 3027, 579, 5309, 154, 2, 686, 359, 2, 5310, 169, 262, 12, 326, 0, 2, 19, 3, 153, 11, 5311, 3028, 53, 202, 1]\n",
      "unary_matrix [0.73999999999999999, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  58\n",
      "xx_source:  48  =>  [1016, 580, 813, 21, 13, 2834, 4, 2835, 26, 0, 251, 209, 34, 141, 227, 164, 1425, 4, 34, 4297, 4, 2116, 1422, 26, 0, 372, 227, 1, 160, 12, 0, 580, 813, 5, 34, 2836, 32, 4298, 5, 31, 2837, 2117, 36, 1423, 4, 0, 251, 2]\n",
      "xx_target:  42  =>  [5312, 169, 262, 12, 326, 8, 5313, 5314, 26, 687, 14, 1480, 948, 359, 36, 129, 104, 2031, 802, 5315, 685, 14, 1488, 948, 21, 283, 0, 7, 123, 262, 2, 111, 5316, 5317, 2, 5318, 5319, 58, 3029, 188, 359, 1]\n",
      "unary_matrix [0.58999999999999997, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  59\n",
      "xx_source:  25  =>  [1016, 719, 253, 21, 581, 11, 7, 4299, 3, 1426, 111, 0, 580, 813, 5, 0, 251, 140, 68, 4300, 3, 0, 719, 253, 2]\n",
      "xx_target:  19  =>  [1482, 9, 684, 5320, 5321, 5322, 51, 2030, 169, 1479, 12, 326, 2, 2029, 80, 5323, 2032, 202, 1]\n",
      "unary_matrix [0.81999999999999995, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  60\n",
      "xx_source:  37  =>  [1016, 580, 813, 52, 1, 94, 0, 719, 253, 104, 1680, 5, 16, 0, 4301, 3, 0, 1427, 580, 813, 4302, 0, 193, 1683, 1, 2838, 4, 0, 119, 5, 1423, 3, 0, 1427, 580, 813, 2]\n",
      "xx_target:  36  =>  [688, 19, 801, 9, 684, 284, 2, 954, 1489, 169, 262, 12, 326, 3030, 12, 2033, 5324, 11, 129, 5325, 1490, 0, 43, 3030, 169, 262, 12, 326, 2034, 15, 183, 2, 5326, 1489, 262, 1]\n",
      "unary_matrix [0.64999999999999991, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  61\n",
      "xx_source:  10  =>  [15, 251, 21, 24, 2118, 582, 6, 1683, 210, 2]\n",
      "xx_target:  7  =>  [2035, 5327, 304, 5, 689, 3031, 1]\n",
      "unary_matrix [0.93999999999999995, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  62\n",
      "xx_source:  22  =>  [652, 1, 0, 251, 52, 4303, 3, 1683, 811, 86, 653, 4, 55, 815, 28, 343, 27, 4, 28, 815, 27, 2]\n",
      "xx_target:  17  =>  [5328, 5329, 4, 5330, 3031, 136, 5, 327, 155, 1, 513, 125, 1, 215, 90, 513, 1]\n",
      "unary_matrix [0.83999999999999997, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  63\n",
      "xx_source:  47  =>  [180, 0, 721, 3, 0, 372, 227, 1, 0, 476, 651, 21, 26, 71, 4, 71, 2839, 654, 1683, 4304, 1, 297, 1421, 1, 231, 20, 24, 50, 31, 580, 1421, 1, 18, 2826, 1, 160, 12, 50, 2119, 0, 912, 165, 60, 6, 1684, 655, 2]\n",
      "xx_target:  37  =>  [263, 303, 1488, 948, 577, 1484, 2036, 5331, 3032, 3033, 12, 326, 203, 800, 1479, 12, 326, 0, 5332, 146, 580, 0, 3013, 3033, 12, 326, 21, 283, 0, 7, 5333, 803, 804, 3, 66, 581, 1]\n",
      "unary_matrix [0.6399999999999999, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  64\n",
      "xx_source:  5  =>  [4305, 3, 227, 5, 1178]\n",
      "xx_target:  4  =>  [3034, 1491, 2, 1492]\n",
      "unary_matrix [0.96999999999999997, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  65\n",
      "xx_source:  15  =>  [15, 429, 3, 227, 3, 0, 251, 21, 13, 18, 1179, 6, 1684, 4306, 2]\n",
      "xx_target:  9  =>  [3034, 1491, 359, 8, 5334, 3, 66, 5335, 1]\n",
      "unary_matrix [0.92000000000000004, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  66\n",
      "xx_source:  15  =>  [15, 251, 21, 576, 1, 5, 2120, 34, 656, 2121, 6, 1, 1428, 1178, 2]\n",
      "xx_target:  11  =>  [2035, 3035, 2037, 1492, 2, 1493, 3, 174, 514, 5336, 1]\n",
      "unary_matrix [0.90000000000000002, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  67\n",
      "xx_source:  36  =>  [2840, 18, 160, 6, 55, 1685, 28, 430, 27, 28, 259, 27, 1, 118, 44, 21, 1180, 20, 2841, 1017, 19, 0, 816, 1, 173, 20, 583, 17, 0, 251, 3, 1428, 1178, 2842, 26, 46]\n",
      "xx_target:  32  =>  [184, 3036, 155, 1, 3037, 125, 1, 265, 448, 1, 189, 20, 1175, 515, 5337, 121, 5338, 805, 97, 0, 27, 1176, 5339, 0, 5340, 17, 5341, 2037, 1492, 3038, 14, 28]\n",
      "unary_matrix [0.68999999999999995, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  68\n",
      "xx_source:  13  =>  [28, 7, 27, 1429, 3, 4307, 3, 184, 3, 582, 526, 217, 41]\n",
      "xx_target:  10  =>  [2, 20, 1177, 21, 3039, 124, 304, 690, 364, 36]\n",
      "unary_matrix [0.91000000000000003, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  69\n",
      "xx_source:  29  =>  [28, 259, 27, 1429, 3, 1686, 217, 1, 1018, 6, 4308, 3, 1686, 217, 1, 2122, 20, 1018, 2123, 1687, 26, 0, 719, 3, 1421, 16, 0, 251, 41]\n",
      "xx_target:  26  =>  [189, 20, 1177, 2038, 364, 0, 1494, 2039, 2038, 364, 0, 5342, 17, 3040, 3, 1494, 5343, 14, 684, 169, 262, 12, 326, 13, 359, 36]\n",
      "unary_matrix [0.75, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  70\n",
      "xx_source:  8  =>  [28, 722, 27, 1429, 3, 2843, 2124, 41]\n",
      "xx_target:  6  =>  [516, 20, 1177, 3041, 2040, 36]\n",
      "unary_matrix [0.94999999999999996, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "\n",
      "+++++++++ The sentence  71\n",
      "xx_source:  5  =>  [28, 373, 27, 4309, 41]\n",
      "xx_target:  4  =>  [365, 20, 691, 36]"
     ]
    }
   ],
   "source": [
    "# BW model variables\n",
    "max_distance = 50\n",
    "baum_welch_model = BaumWelchModel(max_distance, seed=1111)\n",
    "print(\"non_negative_set\", baum_welch_model.non_negative_set)\n",
    "\n",
    "# Emission model variables\n",
    "vocab_input_size = len(cz_target_indices)\n",
    "d_embedding = 128\n",
    "layer_size = [d_embedding, 512]\n",
    "vocab_output_size = len(en_source_indices)\n",
    "emission_model = EmissionModel(vocab_input_size=vocab_input_size, layer_size=layer_size, \n",
    "                               vocab_output_size=vocab_output_size, baum_welch_model=baum_welch_model)\n",
    "\n",
    "trans_posteriors = emission_model.train_model(target_inputs=np.array(cz_sequences), \n",
    "                                              source_inputs=np.array(en_sequences), \n",
    "                                              input_indice_shift=-1)\n",
    "# print(\"trans_posteriors\", np.shape(trans_posteriors), trans_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
